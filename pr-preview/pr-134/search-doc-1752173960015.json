{"searchDocs":[{"title":"o4-mini is now available!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/blog/2025-05-02-o4-mini/","content":"We are pleased to announce that o4-mini is now available via Pythia. Here is an overview of the model: https://platform.openai.com/docs/models/o4-mini When compared to o3-mini, o4-mini adds support for images as inputs. It costs the same as o3-mini for regular usage, though the price is lower if you use cached inputs. Thus, this model will now be the default reasoning model we provide and the use of o3-mini is now deprecated.","keywords":"","version":null},{"title":"Dataproc","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/intro/","content":"","keywords":"","version":"Next"},{"title":"What is Hadoop?​","type":1,"pageTitle":"Dataproc","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/intro/#what-is-hadoop","content":" Hadoop is an open-source software framework for storing and processing big data in a distributed/parallel fashion on large clusters of commodity hardware. At its core, Hadoop strives to increase processing speed by increasing data locality (i.e., it moves computation to servers where the data is located). There are three components to Hadoop: HDFS (the Hadoop Distributed File System), the Hadoop implementation of MapReduce, and YARN (Yet Another Resource Negotiator; a scheduler).  ","version":"Next","tagName":"h3"},{"title":"Autoscaling​","type":1,"pageTitle":"Dataproc","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/intro/#autoscaling","content":" NYU Dataproc is configured to be as cloud-agnostic as possible, and still uses HDFS and non-proprietary HBase components. It does, however, leverage autoscaling. This means that if the cluster hasn't been used for a while, it might take a while for resources to become available (typically 3-5 minutes) as nodes are turned on and NodeManagers register with YARN. During this time, the following warning message will appear and indicate that the cluster is at capacity and resources are not available:  WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources   This warning will go away after new nodes have been added by autoscaling and more resources are available for YARN applications. If it does not go away after more than 10 minutes, please contact the HPC team. Autoscaling is actively monitored, but a duration of more than 10 minutes may indicate a failure of Dataproc's monitoring infrastructure.  Currently, NYU Dataproc's autoscaling is configured so that the cluster will have between 3 and 43 nodes depending upon demand. The number of nodes that are currently active can be seen in the YARN web UI. Additionally, percentage of cluster capacity that is used can be seen on the Scheduler page in the lefthand menu in the YARN web UI.  ","version":"Next","tagName":"h3"},{"title":"Accessing the NYU Dataproc Hadoop Cluster​","type":1,"pageTitle":"Dataproc","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/intro/#accessing-the-nyu-dataproc-hadoop-cluster","content":" Access to the NYU Dataproc cluster is granted via your NYU Google account. If you are in a class that uses Dataproc, your instructor or TA will request Dataproc access for your NetID.  Once this is granted you can log in by navigating to https://dataproc.hpc.nyu.edu/ssh in your web browser. After you’ve reached this page you will have access to a browser-based terminal interface where you can run Hadoop commands.  If you are having difficulty connecting to a terminal, please make sure that you are not logged in to a non-NYU Google account by clicking the icon displayed in the upper right corner in the Gmail web interface (see here). If you are using Google Chrome, you may also need to switch to your NYU account profile using the instructions here.  If you continue to have difficulties connecting via https://dataproc.hpc.nyu.edu/ssh, you can also log in by navigating to https://shell.cloud.google.com/ and running the following command in the terminal that appears:  gcloud compute ssh nyu-dataproc-m --project=hpc-dataproc-19b8 --zone=us-central1-f   You may need to authorize Google to log in to Dataproc after running the above command.  Once logged in, your username will be of the form NetID_nyu_edu rather than just your NetID. ","version":"Next","tagName":"h2"},{"title":"Data management","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/","content":"","keywords":"","version":"Next"},{"title":"File Permissions and Access Control Lists​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#file-permissions-and-access-control-lists","content":" You can share files with others using access control lists (ACLs). An ACL gives you per-file, per-directory and per-user control over who has permission to access files. You can see the ACL for a file or directory with the getfacl command:  $ hdfs dfs -getfacl /user/&lt;net_id&gt;_nyu_edu/testdir   To modify permissions for files or directories, use setfacl  $ hdfs dfs -setfacl -R -m user:&lt;net_id&gt;_nyu_edu:rwx /user/&lt;net_id&gt;_nyu_edu/testdir $ hdfs dfs -setfacl -R -m default:user:&lt;net_id&gt;_nyu_edu:rwx /user/&lt;net_id&gt;_nyu_edu/testdir   To open the subdirectory permission to others, you need to open each higher level directory's navigation permission too:  $ hdfs dfs -setfacl -m user:&lt;net_id&gt;_nyu_edu:--x /user/&lt;net_id&gt;_nyu_edu   ","version":"Next","tagName":"h3"},{"title":"Uploading Data to HDFS from Your Computer​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#uploading-data-to-hdfs-from-your-computer","content":" ","version":"Next","tagName":"h2"},{"title":"Small Transfers​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#small-transfers","content":" You can add smaller files to HDFS by copying them to your local filesystem / non-HDFS home directory and then copying them from there to HDFS. Note that there is a limit on the size of your local filesystem home directory, so you should only perform these steps for smaller amounts of data- for larger datasets you should use the method described in the Large Transfers section.  Navigate to the command line interface by going to http://dataproc.hpc.nyu.edu/ssh.In the upper right portion of the header banner, select Upload File. Use the web browser dialog to select your file.Once the file is uploaded to your Unix directory, run the following command to copy it into HDFS:  hdfs dfs -put /home/&lt;your_netid&gt;_nyu_edu/&lt;path_to_file&gt; &lt;hdfs_path&gt;   To retrieve data from HDFS and copy it to your local filesystem home directory, you can use one of the following commands:  hdfs dfs -get &lt;hdfs_path&gt; /home/&lt;your_netid&gt;_nyu_edu/&lt;path_to_file&gt; hdfs dfs -copyToLocal &lt;hdfs_path&gt; /home/&lt;your_netid&gt;_nyu_edu/&lt;path_to_file&gt;   You can then download data by going to the upper right corner of the window in the command line interface and selecting the Download File option and entering a file path (i.e., /home/&lt;your_netid&gt;_nyu_edu/&lt;path_to_file&gt;).  To list files in HDFS, use the following command:  hdfs dfs -ls   ","version":"Next","tagName":"h3"},{"title":"Large Transfers​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#large-transfers","content":" To upload large datasets to HDFS, first navigate to the data ingest website at https://dataproc.hpc.nyu.edu/ingest. The data ingest website provides a web interface for temporary cloud-based bucket storage. Any datasets that are uploaded to the data ingest website will remain there for 2 days. Before these 2 days have elapsed, you will need to upload your datasets to your HDFS home directory. To do that you can use the following command after logging in:  hadoop distcp gs://nyu-dataproc-hdfs-ingest/&lt;file_or_folder_name&gt; /user/&lt;your_net_id&gt;_nyu_edu   You can find the full path to your file/folder in the ingest storage by clicking on it in the web interface and then scrolling down to the gsutil URI field in the Live Object tab.  ::: warning Data uploaded into the ingest website will be visible to all members of the cluster temporarily. If you are uploading files that cannot be shared with all cluster users (e.g., code) please use the alternate method described below. :::  ","version":"Next","tagName":"h3"},{"title":"Uploading Data to HDFS from Greene​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#uploading-data-to-hdfs-from-greene","content":" First download gcloud on a desktop computer with a browser by following the instructions here. Ensure that you run the install command described as optional in step 4. Log into Greene and activate the Google Cloud command line interface module:  ml load google-cloud-sdk/379.0.0   Then log into Google Cloud by typing the following:  gcloud auth login   Copy and paste the command that you are given into a terminal application on your desktop and run it. When prompted, type y to proceed. If you are signed into multiple Google accounts, you will then be presented with a browser window where you can choose your account. Select your NYU account. Google will then present a message saying that “Google Cloud SDK wants to access your Google Account”. Click Allow.  Copy the URL that you are given in the terminal window, and paste it into your Greene session on the line where gcloud asks for it.  Type gcloud auth list to verify that you are logged in:  [NetID@hlog-1 ~]$ gcloud auth list Credentialed Accounts ACTIVE ACCOUNT * NetID@nyu.edu To set the active account, run: $ gcloud config set account `ACCOUNT`   Now that you are logged in, use the instructions under the Small Transfers or Large Transfers sections below to upload your data.  ","version":"Next","tagName":"h2"},{"title":"Small Transfers​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#small-transfers-1","content":" Run the following commands on Greene to ensure that gcloud knows that you are using it with Dataproc (rather than a different Google Cloud application):  gcloud config set project hpc-dataproc-19b8 gcloud config set compute/zone us-central1-f   Run the following command on Greene to upload your file to your filesystem home directory on Dataproc:  gcloud compute scp MYFILE nyu-dataproc-m:~   If you are prompted to give a passphrase while generating an SSH key, hit enter twice. The above commands are the command line equivalent to the upload dialogue at i```[http://dataproc.hpc.nyu.edu/ssh](http://dataproc.hpc.nyu.edu/ssh) that is mentioned earlier.  Your file should now be available within your filesystem home directory on Dataproc. You can then run the following command to get it into HDFS:  hdfs dfs -put /home/&lt;your_netid&gt;_nyu_edu/&lt;path_to_file&gt; &lt;hdfs_path&gt;   ","version":"Next","tagName":"h3"},{"title":"Large Transfers​","type":1,"pageTitle":"Data management","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/data_management/#large-transfers-1","content":" On Greene, run the following to upload a single file to the staging bucket:  gsutil cp FILE gs://nyu-dataproc-hdfs-ingest   Or run the following to copy a directory:  gsutil rsync -r DIRECTORY gs://nyu-dataproc-hdfs-ingest   The above commands are the command line equivalents to the data ingest website described earlier.  As with the earlier example, run the following from within Dataproc to ingest the dataset into your HDFS home directory:  hadoop distcp gs://nyu-dataproc-hdfs-ingest/&lt;file_or_folder_name&gt; /user/&lt;your_net_id&gt;_nyu_edu  ","version":"Next","tagName":"h3"},{"title":"NIH Strides","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/nih_strides/","content":"","keywords":"","version":"Next"},{"title":"Enrolling to the NIH Strides initiative​","type":1,"pageTitle":"NIH Strides","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/nih_strides/#enrolling-to-the-nih-strides-initiative","content":" NYU has enrolled in the NIH Strides initiative in December 2020 by signing an agreement with Carahsoft, GCP's billing and administrative partner. Thus NIH-funded NYU researchers with an active NIH award may take advantage of the STRIDES Initiative for their NIH-funded research projects. The NYU RTS team works closely with Burwood Group, a GCP reseller, to provide access to GCP resources for NYU researchers who are approved to participate in the NIH STRIDES initiative. NYU researchers who wish to participate must follow the steps outlined below.  ","version":"Next","tagName":"h2"},{"title":"Contacts​","type":1,"pageTitle":"NIH Strides","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/nih_strides/#contacts","content":" For general questions about Research Cloud/GCP please email the NYU HPC Research Cloud team: hpc-cloud@nyu.eduTo learn more about the NIH Strides Initiative, email the team at strides@nih.gov ","version":"Next","tagName":"h2"},{"title":"Google Cloud Platform for Research","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/","content":"","keywords":"","version":"Next"},{"title":"Why work with the NYU Research Cloud team to deploy your research project on GCP?​","type":1,"pageTitle":"Google Cloud Platform for Research","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/#why-work-with-the-nyu-research-cloud-team-to-deploy-your-research-project-on-gcp","content":" NYU researchers who work with the Research Cloud team to deploy projects on GCP may benefit from the following:  Discounted rates, lower GCP project cost: Through NYU's participation in the Internet2 Net+ agreement, as well as the 3-year commitment NYU made to using GCP, GCP projects enjoy discounted rates, lowering the cost of the project. The exact discounts depend on the GCP service used in the research project and can vary between 5% and 25%. Free data egress is usually included.GCP Expertise and project setup: The NYU research cloud team can work closely with researchers to Identity and Access Management, groups access, establish billing, etc.GCP Support cases: Given proper permissions the NYU Research Cloud team can open support cases on behalf of the researchers or directly discuss GCP project issues with GCP expertsCost monitoring, billing alerts, and spending reports: The Research Cloud team has access to additional tools that can provide cost monitoring, switch between billing options and provide spending reports.Research project funding options: The NYU research cloud team can help with providing invoices, internal fund transfers, establishing GCP Billing ids, etc. that can enable NYU researchers to pay for GCP services.  ","version":"Next","tagName":"h2"},{"title":"Getting Started with GCP​","type":1,"pageTitle":"Google Cloud Platform for Research","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/#getting-started-with-gcp","content":" There are a number of ways you can get started with using GCP in research projects:  To start using GCP resources in a research project, request a consultation with the Research Cloud team (via email research-cloud-support@nyu.edu). The Research Cloud team can advise on ways to set up your research project, available discounts, etc.If a project involves Data Science and Machine Learning, consult with the DS3 team before starting your project on GCP. Please note:Creating a GCP project using your NYU account (NetID@nyu.edu) will place the project under the nyu.edu organization, an environment managed by the NYU Research Cloud team. However, your project doesn't automatically qualify for the price discounts and benefits that NYU has negotiated with GCP.For non-NYU work on GCP, please use your personal, non-NYU, Google email when creating GCP projects.  tip For non-NYU work on GCP, please use your personal, non-NYU, Google email when creating GCP projects.  The NYU Research Cloud team does not currently offer training on how to deploy and utilize resources on GCP in research projects or teaching. If you are new to GCP and you want to learn the GCP fundamentals or if you want to learn how to perform specific tasks on GCP (obtain skill badges), please consider the following resources:  Through the Google For Education program, GCP offers training credits and discounts to Students, Faculty, and IT Admins. To apply for training credits and discounts, please click here.Getting started with Google Cloud Platform offers quick starts and sample projects on GCP.  ","version":"Next","tagName":"h2"},{"title":"How can I fund my research project on GCP?​","type":1,"pageTitle":"Google Cloud Platform for Research","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/#how-can-i-fund-my-research-project-on-gcp","content":" ","version":"Next","tagName":"h2"},{"title":"GCP Free Tier​","type":1,"pageTitle":"Google Cloud Platform for Research","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/#gcp-free-tier","content":" Apply for credits using your NYU account (https://cloud.google.com/free/). After credits expire, if you would like to switch to another type of funding and are approved to do so, we will modify your project so it can use other funds:  https://edu.google.com/programs/credits/research/?modal_active=nonehttps://edu.google.com/programs/?modal_active=none  ","version":"Next","tagName":"h3"},{"title":"Sources of funding for GCP project​","type":1,"pageTitle":"Google Cloud Platform for Research","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/gcp_self_managed/intro/#sources-of-funding-for-gcp-project","content":" Please consider options below and explore other options which may exist for your specific field.  Google Cloud research creditsNIH STRIDESNSF CloudBankGCP seed grant through NYU HPCDepartmental fundsApply using this form GCP project request form ","version":"Next","tagName":"h3"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/getting_started/intro/","content":"Start here! We facilitate access to cloud computing resources (GCP) for research, HPC Bursting, visualization, data analysis (hadoop) and host an OpenShift Kubernetes cluster on-prem. Please proceed to the relevant section to know more about the offerings and how you could harness them.","keywords":"","version":"Next"},{"title":"Computation","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/","content":"","keywords":"","version":"Next"},{"title":"MapReduce​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#mapreduce","content":" MapReduce is a programming model and an associated implementation for processing and generating large datasets with a parallel, distributed algorithm on a cluster.  A MapReduce job splits a large dataset into independent chunks and organizes them into key-value pairs for parallel processing. The mapping and reducing functions receive not just values, but (key, value) pairs.  Every MapReduce job consists of at least two parts:  The MapperThe Reducer  Mapping Phase: Takes input as &lt;key,value&gt; pairs, processes them, and produces another set of intermediate &lt;key,value&gt; pairs as output.  Reducing Phase: Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value.  ","version":"Next","tagName":"h2"},{"title":"MapReduce Word Count Example​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#mapreduce-word-count-example","content":" Ingest a text file into HDFS. In the below example, the hdfs -put command is combined with another Unix command (curl) to send a copy of a Sherlock Holmes book (located at the URL https://www.gutenberg.org/files/1661/1661-0.txt) directly into HDFS:  curl -o input.txt https://www.gutenberg.org/files/1661/1661-0.txt hdfs dfs -put input.txt   The following command will run an example Word Count job (described in more detail here) with the Sherlock Holmes book as its input.  hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount -D mapreduce.job.maps=6 -D mapreduce.job.reduces=2 /user/&lt;netid&gt;_nyu_edu/input.txt /user/&lt;netid&gt;_nyu_edu/output   ","version":"Next","tagName":"h3"},{"title":"Spark​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#spark","content":" Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. (source: Wikipedia)  ","version":"Next","tagName":"h2"},{"title":"Launching an Interactive Spark Shell​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#launching-an-interactive-spark-shell","content":" Spark provides an interactive shell that you can use to learn the Spark API and analyze datasets interactively. To connect to Spark Shell from the command line, execute the following command:  spark-shell --deploy-mode client --num-executors=1 --driver-memory=1G --executor-memory=1G   Note: NYU Dataproc deploys Spark applications in cluster mode by default. The following error indicates that you are trying to deploy an interactive shell, which must use client mode:  Exception in thread &quot;main&quot; org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.   To resolve this error, either use the command line flag indicated above (--deploy-mode client) or set the spark.submit.deployMode property in your Spark configuration to client. More details about the difference between cluster and client mode can be found here.  ","version":"Next","tagName":"h3"},{"title":"YARN Scheduler​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#yarn-scheduler","content":" YARN is the resource manager and job scheduler in the Hadoop cluster. YARN allows you to use various data processing engines for batch, interactive, and real-time stream processing of data stored in HDFS.  Application status and logs  Please find the list of current running apps using 'Yarn' script. Running the yarn script without any arguments prints the description for all commands  yarn application -list   To kill a currently running app because the submitted app started malfunctioning or in worst case scenario, it's stuck in an infinite loop. Get the app ID and then kill it as given below  yarn application -kill &lt;application_ID&gt;   To download application logs for examination on the command line  yarn logs -applicationId &lt;application_ID&gt;   ","version":"Next","tagName":"h2"},{"title":"Using Hive​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#using-hive","content":" Apache Hive is a data warehouse software package that facilitates querying and managing large datasets residing in distributed storage (i.e., HDFS). Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called Hive Query Language (HiveQL or HQL).  You can access Hive with the following command:  beeline -u jdbc:hive2://localhost:10000 0: jdbc:hive2://localhost:10000&gt; use &lt;netid&gt;_nyu_edu; 0: jdbc:hive2://localhost:10000&gt; show tables; 0: jdbc:hive2://localhost:10000&gt; !quit Closing: 0: jdbc:hive2://localhost:10000   It is important to note that in order to exit properly from a beeline session, you type !quit.  If you are planning on using SerDe to query/work with JSON files, you will need to run the following code at the Beeline prompt first in order to ensure that the JsonSerDe class is loaded:  ADD JAR /usr/lib/hive/lib/hive-hcatalog-core.jar   See here for more information.  Access to Hive databases on NYU Dataproc is derived from HDFS permissions because we use Storage-Based Authorization. To grant read-only access to a Hive database to someone other than yourself, you can run the following command:  hdfs dfs -setfacl -R -m user:&lt;OTHER_PERSON_NETID&gt;_nyu_edu:r-x /user/hive/warehouse/NetID_nyu_edu.db   Outside of NYU, other Hadoop installations may use a different mechanism to share databases with other colleagues– it is common for Hadoop installations to use a SQL style grant/revoke mechanism for sharing databases (SQL Standards Based Authorization). This mechanism is not used at NYU and it is important to bear in mind that external documentation referring to grant/revoke statements is not applicable to NYU Dataproc.  ","version":"Next","tagName":"h2"},{"title":"Using Trino​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#using-trino","content":" Trino is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. To access Trino, you can type the following command:  trino   Once you are inside, you can reference multiple data sources through catalogs (see here). For instance, you may want to query Hive using Trino. You can select a database to use through a fully-qualified database or table name as shown in the code below:  trino&gt; show catalogs; Catalog ---------------------- bigquery bigquery_public_data hive mastersql memory system tpcds tpch (8 rows) Query 20240827_132732_00001_ygn47, FINISHED, 2 nodes Splits: 20 total, 20 done (100.00%) 0.17 [0 rows, 0B] [0 rows/s, 0B/s] trino&gt; use hive.&lt;netid&gt;_nyu_edu; USE   You can also specify a catalog / data source that you want to use on the command line when you start Trino:  trino --catalog=hive   ","version":"Next","tagName":"h2"},{"title":"Using Conda​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#using-conda","content":" NYU Dataproc comes with miniconda3 by default. This can be used to manage Python packages within your filesystem home directory. See here or here for more information on the conda command.  ","version":"Next","tagName":"h2"},{"title":"Using Jupyter Notebooks​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#using-jupyter-notebooks","content":" Log into the Dataproc cluster and run jupyter-notebook. Do not close the command line interface where jupyter-notebook is running until you're done using Jupyter.From the output produced by jupyter-notebook, obtain the port number that the notebook is running on. In the example below, for instance, the notebook is running on port 8888:   To access the notebook, open this file in a browser: file:///home/jp6546_nyu_edu/.local/share/jupyter/runtime/nbserver-7866-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1 or http://127.0.0.1:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1   On an individual workstation that has the gcloud command installed (installation instructions for gcloud can be found here), run the following command (with PORT replaced with the port number from step 2):  gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L PORT:localhost:PORT   In our example, from the output in step 2 this command would be as follows:  gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L 8888:localhost:8888   You can then use the URLs from the jupyter-notebook output in step 2 (e.g., http://localhost:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1) to access your notebook from the workstation.When you are done, you can exit the terminals where the jupyter-notebook and the gcloud commands are running.  ","version":"Next","tagName":"h2"},{"title":"Using Zeppelin Notebooks​","type":1,"pageTitle":"Computation","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/dataproc/computation/#using-zeppelin-notebooks","content":" Log into the Dataproc cluster and run zeppelin start. The terminal will output three pieces of information that you will need later: username, password, and port number. The output should look something like this:  NetID_nyu_edu@nyu-dataproc-m:~$ zeppelin start Zeppelin is starting with the following configuration: ------------------------------------------------------- Username: jp6546_nyu_edu Password: REDACTED Port: 64739 Zeppelin start [ OK ]   On an individual workstation that has the gcloud command installed (installation instructions for gcloud can be found here), run the following command (with PORT replaced with the port number from step 1):  gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L PORT:localhost:PORT   In our example this command would be as follows using the output from step 1 :  gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L 64739:localhost:64739   In a web browser, navigate to localhost:PORT.Log in by clicking the Login button in the upper right corner. Use your credentials from step 1.If you forget the password or the port number at any time, you can run the following commands to retrieve this information:  zeppelin get-port zeppelin get-pass   When you are finished, run zeppelin stop to turn off the Zeppelin server. ","version":"Next","tagName":"h2"},{"title":"Visualization Workstations","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/visualization/","content":"Visualization Workstations The burst cluster includes a partition (nvgrid) that can be used to run graphical applications on NVIDIA GPUs for visualization purposes. You can use this partition by following the instructions below. Add the following to your SSH config file (~/.ssh/config) on your local workstation so that you can log into the burst login node directly: Host burst HostName burst.hpc.nyu.edu User &lt;NetID&gt; ProxyJump &lt;NetID&gt;@greene.hpc.nyu.edu ProxyJump &lt;NetID&gt;@burst.hpc.nyu.edu StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR Log into the burst login node by running ssh &lt;NetID&gt;@burst while on-campus or connected to the VPN. Run the following command on the login node to request an interactive command line session: srun --account=hpc --partition=nvgrid --gres=gpu:p100:1 --pty /bin/bash When your interaction session is active, run the following command to start the VNC (remote desktop) server. If this is the first time you’ve used a visualization node, you will be prompted to set a password to use when you access your remote session: /opt/TurboVNC/bin/vncserver Note the hostname of the node that you are running on. This hostname is displayed in the NODELIST column of the output from the squeue command: [NetID@b-23-1 ~]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) … 92727 nvgrid bash jp6546 R 2:55 1 b-23-1 In another terminal on your local machine, run the following command: ssh -N -L 5901:&lt;Hostname&gt;:5901 &lt;NetID&gt;@burst This command will ensure that you can connect to the remote desktop service from your local computer. If you do not already have a VNC remote desktop client installed on your computer, you will need to install one. A list of VNC clients available for various platforms can be found here. Note that Mac OS X comes with a built-in VNC client, which is accessible from the Finder by navigating to Go → Connect to Server and then typing vnc:// at the beginning of the server field. Within your VNC client, connect to localhost:5901 (vnc://localhost:5901 on Mac OS). You should now be presented with a desktop environment. If you are using any OpenGL-based applications that are started from a terminal, be sure to type vglrun before the command name in order to ensure that the application uses the GPU. After your first time using the nvgrid partition, you can start the remote desktop server non-interactively using the following batch script (although you will need to remember the password that you set in step 3). Note that the sleep command should have the length of time that you want the server to run (in seconds) after it (3600 seconds for 1 hour in the example below). #!/bin/bash #SBATCH --gres=gpu:p100:1 #SBATCH --partition=nvgridk #SBATCH --account=hpc #SBATCH --job-name=vnc #SBATCH --time=1:00 #SBATCH --output=slurm_%j.out /opt/TurboVNC/bin/vncserver sleep 3600 ","keywords":"","version":"Next"},{"title":"HPC Bursting","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/intro/","content":"","keywords":"","version":"Next"},{"title":"Running a Bursting Job​","type":1,"pageTitle":"HPC Bursting","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/intro/#running-a-bursting-job","content":" Note: this is not public, only per request of eligible classes or researchers  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ssh to the class on GCP (burst login node) - anyone can login but you can only submit jobs if you have approval  ssh burst   Start an interactive job  srun --account=hpc --partition=interactive --pty /bin/bash   If you got an error &quot;Invalid account or account/partition combination specified&quot; it means your account is not approved to use cloud bursting.  Once your files are copied to the bursting instance you can run a batch job from the interactive session.  ","version":"Next","tagName":"h2"},{"title":"Access to Slurm Partitions​","type":1,"pageTitle":"HPC Bursting","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/intro/#access-to-slurm-partitions","content":" In the example above the partition &quot;interactive&quot; is used. You can list current partitions by running command  sinfo   However, approval is required to submit jobs to the partitions. Partitions are set up by the resources available to a job, such as the number of CPU, amount of memory, and number of GPUs. Please email hpc@nyu.edu to request access to a specific partition or create a new partition (e.g. 10 CPUs and 64 GB Memory) for more optimal cost/performance of your job.  ","version":"Next","tagName":"h2"},{"title":"Storage​","type":1,"pageTitle":"HPC Bursting","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/intro/#storage","content":" Greene's /home and /scratch are mounted (available) at login node of bursting setup.  Compute node however, do have independent /home and /scratch. These /home and /scratch mounts are persistent, are available from any compute node and independent from /home and /scratch at Greene.  User may need to copy data from Greene's /home or /scratch to GCP mounted /home or /scratch  When you run a bursting job the compute nodes will not see those file mounts. This means that you need to copy data to the burst instance.  The file systems are independent, so you must copy data to the GCP location.  To copy data, you must first start an interactive job. Once started, you can copy your data using scp from the HPC Data Transfer Nodes (greene-dtn). Below is the basic setup to copy files from Greene to your home directory while you are in an interactive bursting job:  scp &lt;NetID&gt;@greene-dtn.hpc.nyu.edu:/path/to/files /home/&lt;NetID&gt;/   ","version":"Next","tagName":"h3"},{"title":"Current Limits​","type":1,"pageTitle":"HPC Bursting","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/hpc_bursting_to_cloud/intro/#current-limits","content":" 20,000 CPUs available at any given time for all active bursting users ","version":"Next","tagName":"h3"},{"title":"LLM Catalogue","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/catalogue/","content":"","keywords":"","version":"Next"},{"title":"OpenAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/catalogue/#openai","content":" gpt-4o-minigpt-4oo4-minio3-mini (deprecated)text-embedding-3-small  ","version":"Next","tagName":"h2"},{"title":"VertexAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/catalogue/#vertexai","content":" gemini-2.5-flash-preview-05-20Gemini-2.0 models (flash, flash-lite)Gemini-1.5 models (flash, pro) (deprecated)  For a comprehensive list, please refer to the VertexAI documentation. ","version":"Next","tagName":"h2"},{"title":"OpenWebUI","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/playground/","content":"OpenWebUI We are working on providing an NYU hosted instance of OpenWebUI. More details about this will be provided soon.","keywords":"","version":"Next"},{"title":"Portkey","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/llm_access/","content":"","keywords":"","version":"Next"},{"title":"Onboarding​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/llm_access/#onboarding","content":" Send an email to genai-support@nyu.edu to start the onboarding process.  ","version":"Next","tagName":"h2"},{"title":"Getting started with Portkey​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/external_llms/llm_access/#getting-started-with-portkey","content":" As part of the onboarding process, you would have received an invite which gives you access to a workspace. We will also add virtual keys for LLMs to your workspace as part of the onboarding process. Once you've accepted it, head over to https://app.portkey.ai/ and select the sign-in with Single Sign-On option and proceed with your NYU email address.  Access to Portkey is only permitted via NYU VPN You need to be connected to the NYU VPN to access the Portkey LLM gateway. If you are not, your requests will timeout and result in connection errors.  You will now be able to create an API key for yourself by access the API Keys item on the left sidebar. With an API key and a virtual key at your disposal, you can now run the following script:  from portkey_ai import Portkey portkey = Portkey( base_url=&quot;https://ai-gateway.apps.cloud.rt.nyu.edu/v1/&quot;, api_key=&quot;&quot;, # Replace with your Portkey API key virtual_key=&quot;&quot;, # Replace with your virtual key ) completion = portkey.chat.completions.create( messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are not a helpful assistant&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}, ], model=&quot;gemini-2.0-flash-lite&quot;, ) print(completion)   Once the script is executed, you can head back to app.portkey.ai to view the logs for the call! ","version":"Next","tagName":"h2"},{"title":"Research Technology Cloud","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/cloud/on_prem_cloud/intro/","content":"Research Technology Cloud We host an OpenShift kubernetes cluster on-prem that is in the process of being made available to researchers. Stay tuned for further updates.","keywords":"","version":"Next"},{"title":"Pythia","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/getting_started/intro/","content":"Pythia Non-research workflows If you're looking to harness Generative AI for administrative or classroom use, please reach out to genai-support@nyu.edu Welcome to Pythia, the generative AI platform for research workflows. As part of the Pythia platform, the following capabilities are offered: Access to externally hosted LLMsHPC resources for fine tuning LLMs Personal use If you want to access NYU provided LLMs for personal use, proceed to https://gemini.google.com/app with your NYU credentials.","keywords":"","version":"Next"},{"title":"Retrieval-augmented generation","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/how_to_guides/retrieval_augmented_generation/","content":"Retrieval-augmented generation Large Language Models only know about the data they were trained upon and do not have the context needed to be effective at answering questions based on: private datasetsnewer knowledge past the cutoff date (i.e. the date at which data collection was frozen) To get around this issue, one of the most popular techniques is Retrieval-augmented generation, the most basic version of which is outlined below: It starts with the &quot;Ingestion&quot; phase where a document to be used as context is parsed and broken into chunks. These chunks are then converted to embeddings and stored in a vector database (which specializes in storing and retrieving vectors). This setup allows us now &quot;retrieve&quot; the required context for an incoming prompt before it is sent to an LLM. The &quot;retrieval&quot; phase consists of converting the prompt to an embedding and looking up embeddings for chunks of the document that are similar to it. The text chunks associated with the embeddings similar to the embedding for the query are then added as additional context to the prompt before passing it to an LLM. The LLM now has the associated context it needs to generate an relevant response to the prompt. Here's a link to a script to test this out yourself, once you have API access to an embedding model and an LLM: https://github.com/NYU-RTS/rts-docs-examples/tree/main/genai/rag . You can run it to ask a question about a recent event that occurred after the knowledge cutoff for the dataset used to train the LLM: ss19980@ITS-JQKQGQQMTX ~/D/p/r/g/rag (main)&gt; uv run rag_basic.py \\ https://en.wikipedia.org/wiki/2025_London_Marathon \\ &quot;Which athletes won the 2025 London Marathon?&quot; ... Processing chunks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:13&lt;00:00, 3.22it/s] ---------------------------------------------------------------- Query embedding vector (first 5 dims) is: [-0.013783585280179977, 0.022411219775676727, -0.018617955967783928, -0.04355597868561745, -0.009368936531245708] ---------------------------------------------------------------- Retrieved chunks and similarity scores: ('The 2025 London Marathon was the 45th running of the London Marathon; it took place on 27 April 2025.[1][2]', 0.8441829085350037) ('1. ^ Martin, Andy (26 April 2025). &quot;London Marathon 2025: route, runners and everything else you need to know&quot;. The Guardian. Retrieved 26 April 2025.\\n2. ^ Poole, Harry (26 April 2025). &quot;Will \\'greatest\\' London Marathon line-ups break records?&quot;. BBC News. Retrieved 26 April 2025.\\n3. ^ a b c d &quot;2025 London Marathon results&quot;. NBC Sports. 27 April 2025. Retrieved 27 April 2025.', 0.837587296962738) ('Venue, 45th London Marathon = London, England. Date, 45th London Marathon = 27 April 2025. Champions, 45th London Marathon = Champions. Men, 45th London Marathon = Sabastian Sawe (2:02:27). Women, 45th London Marathon = Tigst Assefa (2:15:50). Wheelchair men, 45th London Marathon = Marcel Hug (1:25:25). Wheelchair women, 45th London Marathon = Catherine Debrunner (1:34:18). ←\\xa020242026\\xa0→, 45th London Marathon = ←\\xa020242026\\xa0→', 0.8032743334770203) ---------------------------------------------------------------- Generated response from LLM without additional context is: The 2025 London Marathon has not happened yet! The event typically takes place in **April** each year. We will only know the winners after the race takes place in April 2025. --------------------------------------------------------------- Generated response from LLM with additional context is: The athletes who won the 2025 London Marathon are: * **Men:** Sabastian Sawe * **Women:** Tigst Assefa * **Wheelchair men:** Marcel Hug * **Wheelchair women:** Catherine Debrunner E20250611 14:32:54.595919 362220 server.cpp:47] [SERVER][BlockLock][] Process exit ss19980@ITS-JQKQGQQMTX ~/D/p/r/g/rag (main)&gt; ","keywords":"","version":"Next"},{"title":"Generating embeddings","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/how_to_guides/embeddings/","content":"","keywords":"","version":"Next"},{"title":"Applications of embeddings​","type":1,"pageTitle":"Generating embeddings","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/how_to_guides/embeddings/#applications-of-embeddings","content":" Embeddings are typically used for:  retrieval-augmented generationsearchclassification  info Embeddings are typically stored in a vector database which is designed for efficient storage and fast retrieval of vectors. ","version":"Next","tagName":"h2"},{"title":"Fine tuning","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/llm_fine_tuning/intro/","content":"Fine tuning You can use HPC for this. But for most cases, RAG might suffice. Please look into harnessing RAG before attempting to fine-tune a model.","keywords":"","version":"Next"},{"title":"Effect of Temperature","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/genai/how_to_guides/temperature/","content":"Effect of Temperature Generating text (or images) from LLMs is inherently probabilistic. However, as an end user you have many parameters at your disposal to tweak the behavior of LLMs. Of these, temperature is the most commonly used. Broadly, it controls the randomness of the generated text. A lower temperature produces more deterministic outputs, while a higher temperature produces more random &quot;creative&quot; output. For a more comprehensive explanation on this topic, refer to the following: How to generate text: using different decoding methods for language generation with TransformersWhat is LLM Temperature? tip The effect of temperature is probabilistic, so you might need to run the script repeatedly to obtain a representative sample of generated text from the LLM. Here's a script to test the effect of temperature: from portkey_ai import Portkey portkey = Portkey( base_url=&quot;https://ai-gateway.apps.cloud.rt.nyu.edu/v1/&quot;, api_key=&quot;&quot;, # Replace with your Portkey API key virtual_key=&quot;&quot;, # Replace with your virtual key ) completion = portkey.chat.completions.create( messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are not a helpful assistant&quot;}, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Complete the following sentence:The sun is shining and the sky is&quot;, }, ], model=&quot;gemini-2.5-flash-preview-04-17&quot;, temperature=2.0, #tweak this parameter! ) print(completion) At the temperature of 2.0, you might get an output along along the lines of: &quot;listening to old radio static.&quot;&quot;... a really peculiar shade of chartreuse today.&quot;&quot;The sun is shining and the sky is, I assume, still above.&quot;&quot;...containing numerous molecules of gas.&quot; alongside the more common response of blue which is likely the only response you'd get a lower temperature (like 0.1). note Reasoning models do not support the temperature parameter. Instead, you should look into tweaking the thinking budget parameter for reasoning models.","keywords":"","version":"Next"},{"title":"Connecting to the HPC Cluster","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/","content":"","keywords":"","version":"Next"},{"title":"Command Line Interface (with a Terminal)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#command-line-interface-with-a-terminal","content":" ","version":"Next","tagName":"h2"},{"title":"Mac & Linux Access​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#mac--linux-access","content":" To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Windows CMD​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-cmd","content":" Windows 11 users have several options. First, the CMD program should contain an ssh client, allowing you to log into Greene or Hudson the same way as with a Linux terminal.  ","version":"Next","tagName":"h3"},{"title":"Windows WSL2​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-wsl2","content":" If you run Windows 10, you can install WSL, and then install Ubuntu or other Linux distribution (for example, from Microsoft Store). You will have a fully functional Ubuntu with terminal and can connect to cluster using instructions provided above for Linux/Mac users.  Instructions on WSL installation can be found here: https://docs.microsoft.com/en-us/windows/wsl/install-win10  tip One of many options to get terminal that support tabs, etc. is to install 'Windows Terminal' from Microsoft Store.If you are using WSL 2 (Windows subsystem for Linux), you may not be able to access internet when Cisco AnyConnect VPN, installed from exe file, is activated. A potential solution: uninstall Cisco AnyConnect and install AnyConnect using Microsoft Store, and then setup new VPN connection using settings described on IT webpage.  ","version":"Next","tagName":"h3"},{"title":"Setting up SSH Keys​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#setting-up-ssh-keys","content":" Instead of typing your password every time you need to log in, you can also specify an ssh key.  Only do that on the computer you trust Generate ssh key pair (terminal in Linux/Mac or cmd/WSL in Windows):https://www.ssh.com/ssh/keygen/ Note the path to ssh key files. Don't share key files with anybody - anybody with this key file can login to your account Log into cluster using regular login/password and then add the content of generated public key file (the one with .pub) to $HOME/.ssh/authorized_keys on cluster Next time you will log into cluster no password will be required  For additional recommendations on how to configure your SSH sessions, see the [ssh configuring and x11 forwarding page].  ","version":"Next","tagName":"h2"},{"title":"PuTTY (Only for Windows)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/connecting_to_hpc/#putty-only-for-windows","content":" There are many SSH clients for Windows OS, but we recommend using PuTTY SSH if you have not already. Once it is installed, launch PuTTY and configure new session &quot;Session&quot; category as in the screenshot below:    Here we are instructing PuTTY to connect to host gw.hpc.nyu.edu on port 22 using SSH protocol (note, that this interface allows you to save this connection configuration for future). Just like for Linux and Mac users, if you are connecting from the outside of NYU network, you need to go through the gateway servers.  Once you click &quot;Open&quot;, a terminal window with prompt for password will pop up. Enter your NetID password and you should be authorized on the gateway server. Gateways are designed to support only a very minimal set of commands and their only purpose it to let users access HPC systems. Once you are there type in an ssh command that will let you connect to Greene cluster :  # Greene Login ssh greene.hpc.nyu.edu   A new command line interface window will open up that prompts you for your password on the gateway server, from there you can connect to Greene by entering the following:  ssh greene.hpc.nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Open OnDemand (Web-based Graphical User Interface)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ood/","content":"","keywords":"","version":"Next"},{"title":"Access the Shell​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ood/#access-the-shell","content":" Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required.    Interactive Applications  GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options.    ","version":"Next","tagName":"h2"},{"title":"Troubleshooting Connections to Open OnDemand​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ood/#troubleshooting-connections-to-open-ondemand","content":" A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue.  In Chrome, this can be done by navigating to this page in your settings:  chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&amp;search   The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache.    Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu. ","version":"Next","tagName":"h2"},{"title":"Using Containers on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/containers/","content":"Using Containers on HPC","keywords":"","version":"Next"},{"title":"SSH Tunneling and X11 Forwarding","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/","content":"","keywords":"","version":"Next"},{"title":"Avoiding Man in the Middle Warning.​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#avoiding-man-in-the-middle-warning","content":" If you see this warning:  warning @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed.   Do not be alarmed - this is an issue that occurs because the cluster has multiple login nodes (log-1, log-2, and log-3) that greene.hpc.nyu.edu resolves to.  To avoid this warning, you can add these lines to your SSH configuration file. Open ~/.ssh/config and place the following lines in it:  tip Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR   The above will also fix SSH timeout errors by extending the ServerAliveInterval argument.  ","version":"Next","tagName":"h2"},{"title":"SSH Tunneling (Mac, Linux)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#ssh-tunneling-mac-linux","content":" Setting up your workstation for SSH tunneling will make logging in and transferring files significantly easier, and installing and running an X server will allow you to use graphical software on the HPC clusters. X server is a software package that draws on your local screen windows created on a remote computer such as on the remote HPC.  Linux users have X set up already. Mac users can download and install XQuartz.  ","version":"Next","tagName":"h2"},{"title":"Set up a reusable tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#set-up-a-reusable-tunnel","content":" To avoid repeatedly setting up a tunnel, you can write the details of the tunnel into your SSH configuration file. Using your favorite editor, open the file ~/.ssh/config and place the following lines in it:  # first we create the tunnel, with instructions to pass incoming # packets on ports 8027 and 8028 through it and to specific locations Host hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User &lt;Your NetID&gt; # next we create an alias for incoming packets on the port # The alias corresponds to where the tunnel forwards these packets Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User &lt;Your NetID&gt;   Create this file/directory In case you don't have it. Make sure that &quot;.ssh&quot; directory has correct permissions (it should be &quot;700&quot; or &quot;drwx------&quot;). If needed, set permissions with:  chmod 700 ~/.ssh   You may also need to setup permissions on your local computer:  chmod 700 $HOME chmod 700 $HOME/.ssh ## to be safe, all files inside ~/.ssh should be set 600 chmod 600 ~/.ssh/*   ","version":"Next","tagName":"h3"},{"title":"Start the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#start-the-tunnel","content":" To create the tunnel, ssh to it with the following command:  ssh hpcgwtunnel   tip You must leave this window open for the tunnel to remain open. It is best to start a new terminal window for subsequent logins.  ","version":"Next","tagName":"h3"},{"title":"Log in via the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#log-in-via-the-tunnel","content":" Open a new terminal window and use ssh to log in to the cluster, as shown below.  ssh greene   Note that you must use the short name defined above in your .ssh/config file, not the fully qualified domain name:  Creating a once-off tunnel.  Alternatively, you can set up a once-off tunnel without editing .ssh/config by running the following command:  ssh -L 8027:greene:22 NetID@gw.hpc.nyu.edu # to set up a tunnel ssh -Y -p 8027 NetID@localhost   This is the equivalent to running &quot;ssh hpcgwtunnel&quot; in the reusable tunnel instructions, but the port forwarding is specified on the command line.  ","version":"Next","tagName":"h3"},{"title":"Tunneling (Windows)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#tunneling-windows","content":" ","version":"Next","tagName":"h2"},{"title":"Creating the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#creating-the-tunnel","content":" First open Putty and prepare to log in to gw.hpc.nyu.edu. If you saved your session during that process, you can load it by selecting from the &quot;Saved Sessions&quot; box and hitting &quot;Load&quot;. Don't hit &quot;Open&quot; yet! Under &quot;Connection&quot; -&gt; &quot;SSH&quot;, just below &quot;X11&quot;, select &quot;Tunnels Write &quot;8026&quot; (the port number) in the &quot;Source port&quot; box, and &quot;greene.hpc.nyu.edu:22&quot; (the machine you wish to tunnel to - 22 is the port that ssh listens on) in the &quot;Destination&quot; box Click &quot;Add&quot;. You can repeat step 3 with a different port number and a different destination. If you do this you will create multiple tunnels, one to each destination Before hitting &quot;Open&quot;, go back to the &quot;Sessions&quot; page, give the session a name (&quot;hpcgw_tunnel&quot;) and hit &quot;Save&quot;. Then next time you need not do all this again, just load the saved session Hit &quot;Open&quot; to login in to gw.hpc.nyu.edu and create the tunnel. A terminal window will appear, asking for your login name (NYU NetID) and password (NYU password). Windows may also ask you to allow certain connections through its firewall - this is so you can ssh to port 8026 on your workstation - the entrance to the tunnel  note You can add other NYU hosts to the tunnel by adding a new source port and destination and clicking &quot;Add&quot;. For example, you could add &quot;Source port = 8025&quot; and &quot;Destination = EXAMPLE.hpc.nyu.edu:22&quot;, then press &quot;Add&quot;. You would then perform Step 2 (below) twice - once for greene on port 8026 and once for an example server on port 8025.  Using your SSH tunnel: To log in via the tunnel, first the tunnel must be open. If you've just completed Step 1, it will be open and you can jump down to &quot;Step 2: Logging in via your SSH tunnel&quot;. If you completed Step 1 yesterday, and now want to re-use the tunnel you created, first start the tunnel:  Starting the tunnel: During a session, you need only do this once - as long as the tunnel is open, new connections will go over it.  Start Putty.exe (again, if necessary), and load the session you saved in settings during procedure above Hit &quot;Open&quot;, and log in to the bastion host with your NYU NetID and password. This will create the tunnel.  ","version":"Next","tagName":"h3"},{"title":"Logging in via your SSH tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#logging-in-via-your-ssh-tunnel","content":" Start the second Putty.exe. In the &quot;Host Name&quot; box, write &quot;localhost&quot; and in the &quot;Port&quot; box, write &quot;8026&quot; (or whichever port number you specified when you set up the tunnel in the procedure above). We use &quot;localhost&quot; because the entrance of the tunnel is actually on this workstation, at port 8026 Go to &quot;Connections&quot; -&gt; &quot;SSH&quot; -&gt; &quot;X11&quot; and check &quot;Enable X11 forwarding&quot; Optionally, give this session a name (in &quot;Saved Sessions&quot;) and hit &quot;Save&quot; to save it. Then next time instead of steps 1 and 2 you can simply load this saved session Hit &quot;Open&quot;. You will again get a terminal window asking for your login (NYU NetID) and password (NYU password). You are now logged in to the HPC cluster!  ","version":"Next","tagName":"h3"},{"title":"X11 Forwarding​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#x11-forwarding","content":" In rare cases when you need to interact with GUI applications on HPC clusters, you need to enable X11 forwarding for your SSH connection. Mac and Linux users will need to run the ssh commands described above with an additional flag:  ssh -Y &lt;NYU_NetID&gt;@greene.hpc.nyu.edu   However, Mac users need to install XQuartz, since X-server is no longer shipped with the macOS.  Windows users will also need to install X server software. We recommend two options out there. We recommend installing Xming. Start Xming application and configure PuTTY to support X11 forwarding: ","version":"Next","tagName":"h2"},{"title":"Custom Applications with Containers","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#what-is-singularity","content":" Singularity is a container based Linux kernel workspace that works just like Docker. You can run pre-built programs in containers without having to worry about the pre-install environment. You can even run Docker containers with Singularity. Please see the Singularity and Docker documentation by Syslabs for details about all the ways Singularity supports Docker. For a detailed introduction on Singularity, visit their official site.  ","version":"Next","tagName":"h2"},{"title":"Why do we use Singularity​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#why-do-we-use-singularity","content":" There are multiple reasons to use Singularity on the HPC clusters:  Security: Singularity provides a layer of security as it does not require any root access on our clusters. This makes it safer against malware and bad scripts that might jeopardize the outer system. Thus we only support Singularity on our clusters(there are not other options such as Kubernetes or Docker on our clusters right now).Containerization: Singularity will run all your images(packaged and pre-built programs) inside of its containers, each container works like a small vm. They contain all the required environment and files of a single Linux kernel and you don't have to worry about any pre-installation nonsense.Inter-connectivity: Containers are able to talk to each other, as well as the home system, so while each container has its own small space, they are still a part of a big interconnected structure. Thus enabling you to connect your programs.Accessibility: Probably the most important feature of all, Singularity allows you to run your program in 2 to 3 simple steps, as shown below.  ","version":"Next","tagName":"h2"},{"title":"How to run a singularity container​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#how-to-run-a-singularity-container","content":" There are 3 steps to run a Singularity container on our clusters:  ","version":"Next","tagName":"h2"},{"title":"1. pull a image from Singularity hub or Docker hub​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#1-pull-a-image-from-singularity-hub-or-docker-hub","content":" $ singularity pull &lt;image name&gt; # image name can be for example shub://vsoch/hello-world or docker://godlovedc/lolcow     ","version":"Next","tagName":"h3"},{"title":"2. build the image​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#2-build-the-image","content":" $ singularity build &lt;a name of your choosing&gt;.simg:rw &lt;image name&gt; # the image name can be a local image or an image from a hub   We add the :rw tag at the end of the .simg to explicitly give it &quot;read and write&quot; permissions while building.    You can now run your container using the built image.  ","version":"Next","tagName":"h3"},{"title":"3. run container​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#3-run-container","content":" # this is one way of running a container $ singularity run &lt;image name&gt;.simg:ro # this is another way to run a container $ ./&lt;image name&gt;.simg:ro   Unlike in the build phase, we add the :ro tag which means &quot;read only&quot; - as we are now just executing the image, not building it, and thus do not need it to be written.  warning Writing access causes the Singularity image to be locked and it can become inaccessible while it is in read/write mode, so read only mode is best for executing commands.  running this would yield a menu for output:    Enter Container​  singularity shell &lt;image name&gt;.simg:ro # after this step, you will be going into the container and start your programming   :::  Run commands outside the container​  You can run commands for the container using exec arguments without actually going into the container  $ singularity exec &lt;image name&gt;.simg:ro &lt;commands&gt; # adding commands to the back will return the display result of these commands in the container without actually going into the container   Example:    That's it! Now you're good to go and can just use these simple steps to run singularity images and run your programs.  For full information and documentation please visit Singularity.  ","version":"Next","tagName":"h3"},{"title":"How to Create a Singularity Container​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#how-to-create-a-singularity-container","content":" So what if you want to create an image from your container and save it for a rainy day?  The instructions are here for your convenience, read through them to create your own Singularity container and package it into an image!  For those that know how docker containers are built, you can build docker containers using the information here and upload them onto docker hub and pulling them using Singularity. Singularity supports all docker images!  ","version":"Next","tagName":"h2"},{"title":"Singularity vs Docker​","type":1,"pageTitle":"Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/intro/#singularity-vs-docker","content":" Why are there so many mentions of Docker? The reason is that Singularity is essentially the same as Docker and you don't need to relearn Singularity if you already have experience with Docker. Now let's get into some pros and cons between the two programs.  Docker is more accepted commercially than Singularity. You can download and run Docker on your own computer with any operating system and build containers with ease while Singularity is used in a more academic setting. Singularity only supports Linux operating systems and cannot run on a Windows Linux Kernel(your Windows Ubuntu), so it is much more limited.However, Docker requires root or admin access for the operating system it deploys on, and our clusters do not offer that access to any software that requires this criteria. Thus Docker is not available on the clusters and Singularity is.A silver lining in all of this is that Singularity fully supports Docker images and you can do everything in Docker and push your image to Docker Hub and pull them on the clusters. Thus making sure that you don't need to relearn Singularity all over again and can just use it through the simplest of commands in this wiki.  Good luck with Singularity, and have fun! ","version":"Next","tagName":"h2"},{"title":"Squash File System and Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/squash_file_system_and_singularity/","content":"","keywords":"","version":"Next"},{"title":"Working with Datasets​","type":1,"pageTitle":"Squash File System and Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/squash_file_system_and_singularity/#working-with-datasets","content":" Writable ext3 overlay images have conda environments installed inside, Singularity can work with squashFS for fixed datasets, such as the coco datasets. Here's an example using the my_pytorch.ext3 we created in the last tutorial Singularity with Conda:  [NetID@log-1 ~]$ singularity exec \\ --overlay /scratch/&lt;NetID&gt;/pytorch-example/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash   If you have many tiny files as fixed datasets, please make squashFS files to work with Singularity. Here is an example:  Make a temporary folder in /state/partition1, it is a folder in local hard drive on each computer node  [NetID@log-3 NetID]$ srun --pty /bin/bash srun: job 62890341 queued and waiting for resources srun: job 62890341 has been allocated resources [NetID@cm002 NetID]$ mkdir -p /state/partition1/&lt;NetID&gt; [NetID@cm002 NetID]$ cd /state/partition1/&lt;NetID&gt;   Unzip files there, for example  [NetID@cm002 NetID]$ tar -vxzf /scratch/work/public/examples/squashfs/imagenet-example.tar.gz   Change access permissions in case we'll share files with others  [NetID@cm002 NetID]$ find imagenet-example -type d -exec chmod 755 {} \\; [NetID@cm002 NetID]$ find imagenet-example -type f -exec chmod 644 {} \\;   Convert to a single squashFS file on host  [NetID@cm002 NetID]$ mksquashfs imagenet-example imagenet-example.sqf -keep-as-directory   For more details on working with squashFS, please see details from SquashFS documentation.  Copy this file to /scratch  [NetID@cm002 NetID]$ cp -rp /state/partition1/&lt;NetID&gt;/imagenet-example.sqf /scratch/&lt;NetID&gt;/.   To test, files are in /imagenet-example inside Singularity container:  [NetID@cm002 NetID]$ singularity exec --overlay /scratch/&lt;NetID&gt;/imagenet-example.sqf:ro /scratch/work/public/singularity/ubuntu-20.04.1.sif /bin/bash Singularity&gt; find /imagenet-example | wc -l 1303 Singularity&gt; find /state/partition1/&lt;NetID&gt;/imagenet-example | wc -l 1303   To delete the tempoary folder on host  [NetID@cm002 NetID]$ rm -rf /state/partition1/&lt;NetID&gt;  ","version":"Next","tagName":"h2"},{"title":"Working with Datasets","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/working_with_datasets/","content":"Working with Datasets Please see the Squash File System and Singularity page in the Containers section for details about working with datasets on the clusters.","keywords":"","version":"Next"},{"title":"Datasets Available","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/","content":"","keywords":"","version":"Next"},{"title":"General​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#general","content":" The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under  /scratch/work/public/ml-datasets//vast/work/public/ml-datasets/  We recommend to use version stored at /vast (when available) to have better read performance  note For some of the datasets users must provide a signed usage agreement before accessing  ","version":"Next","tagName":"h2"},{"title":"Format​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#format","content":" Many datasets are available in the form of '.sqf' file, which can be used with Singularity. For example, in order to use coco dataset, one can run the following commands  $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif find /coco | wc -l 532896   ","version":"Next","tagName":"h2"},{"title":"Data Sets​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#data-sets","content":" ","version":"Next","tagName":"h2"},{"title":"COCO Dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#coco-dataset","content":" About data set: https://cocodataset.org/  Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset.  Dataset is available under/scratch  /scratch/work/public/ml-datasets/coco/coco-2014.sqf/scratch/work/public/ml-datasets/coco/coco-2015.sqf/scratch/work/public/ml-datasets/coco/coco-2017.sqf  /vast  /vast/work/public/ml-datasets/coco/coco-2014.sqf/vast/work/public/ml-datasets/coco/coco-2015.sqf/vast/work/public/ml-datasets/coco/coco-2017.sqf  ","version":"Next","tagName":"h3"},{"title":"ImageNet and ILSVRC​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#imagenet-and-ilsvrc","content":" About data set: ImageNet (image-net.org)  ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995). Each concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). ImageNet is larger in scale and diversity than the other image classification datasets (https://arxiv.org/abs/1409.0575).  note WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (https://wordnet.princeton.edu/)  ILSVRC (subset of ImageNet)​  ILSVRC uses a subset of ImageNet images for training the algorithms and some of ImageNet’s image collection protocols for annotating additional images for testing the algorithms (https://arxiv.org/abs/1409.0575). The name comes from 'ImageNet Large Scale Visual Recognition Challenge (ILSVRC)'. Competition was moved to Kaggle (http://image-net.org/challenges/LSVRC/2017/)  What is included (https://arxiv.org/abs/1409.0575).  1000 object classesapproximately 1.2 million training images50 thousand validation images100 thousand test imagesSize of data is about 150 GB (for train and validation)  Dataset is available under  /scratch/work/public/ml-datasets/imagenet/vast/work/public/ml-datasets/imagenet  Get access to Data​  New York University does not own this dataset.  Please open the ImageNet site, find the terms of use (http://image-net.org/download), copy them, replace the needed parts with your name, send us an email including the terms with your name - thereby confirming you agree to the these terms. Once you do this, we can grant you access to the copy of the dataset on the cluster.  ","version":"Next","tagName":"h3"},{"title":"Millions Songs​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#millions-songs","content":" About data set: https://labrosa.ee.columbia.edu/millionsong/  Dataset is available under  /scratch/work/public/MillionSongDataset/vast/work/public/ml-datasets/millionsongdataset/  ","version":"Next","tagName":"h3"},{"title":"Twitter Decahose​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#twitter-decahose","content":" About data set: https://developer.twitter.com/en/docs/twitter-api/enterprise/decahose-api/overview/decahose  NYU has a subscription to Twitter Decahose - 10% random sample of the realtime Twitter Firehose through a streaming connection  Data are stored in GCP cloud (BigQuery) and on HPC clusters Greene and Peel (Parquet format).  Please contact Megan Brown at The Center for Social Media &amp; Politics to get access to data and learn the tools available to work with it.  On cluster dataset is available under (given that you have permissions)  /scratch/work/twitter_decahose/  ","version":"Next","tagName":"h3"},{"title":"ProQuest Congressional Record​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#proquest-congressional-record","content":" About data set: ProQuest Congressional Record  The ProQuest Congressional Record text-as-data collection consists of machine-readable files capturing the full text and a small number of metadata fields for a full run of the Congressional Record between 1789 and 2005. Metadata fields include the date of publication, subjects (for issues for which such information exists in the ProQuest system), and URLs linking the full text to the canonical online record for that issue on the ProQuest Congressional platform. A total of 31,952 issues are available.  Dataset is available under:  /scratch/work/public/proquest/  ","version":"Next","tagName":"h3"},{"title":"C4​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#c4","content":" About data set: c4 | TensorFlow Datasets  A colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: https://commoncrawl.org  Dataset is available under  /scratch/work/public/ml-datasets/c4/vast/work/public/ml-datasets/c4  ","version":"Next","tagName":"h3"},{"title":"GQA​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#gqa","content":" About data set: GQA: Visual Reasoning in the Real World (stanford.edu)  Question Answering on Image Scene Graphs  Dataset is available under  /scratch/work/public/ml-datasets/gqa/vast/work/public/ml-datasets/gqa  ","version":"Next","tagName":"h3"},{"title":"MJSynth​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#mjsynth","content":" About data set: Visual Geometry Group - University of Oxford  This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images  This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in the author's work (archived dataset is about 10 GB)  Dataset is available under  /vast/work/public/ml-datasets/mjsynth  ","version":"Next","tagName":"h3"},{"title":"open-images-dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#open-images-dataset","content":" About data set: Open Images Dataset – opensource.google  A dataset of ~9 million varied images with rich annotations  The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). It contains image-level labels annotations, object bounding boxes, object segmentations, visual relationships, localized narratives, and more  Dataset is available under  /scratch/work/public/ml-datasets/open-images-dataset/vast/work/public/ml-datasets/open-images-dataset  ","version":"Next","tagName":"h3"},{"title":"Pile​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#pile","content":" About data set: The Pile (eleuther.ai)  The Pile is a 825 GiB diverse, open source language modeling data set that consists of 22 smaller, high-quality datasets combined together.  Dataset is available under  /scratch/work/public/ml-datasets/pile/vast/work/public/ml-datasets/pile  ","version":"Next","tagName":"h3"},{"title":"Waymo open dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/datasets/intro/#waymo-open-dataset","content":" About data set: Open Dataset – Waymo  The field of machine learning is changing rapidly. Waymo is in a unique position to contribute to the research community with some of the largest and most diverse autonomous driving datasets ever released.  Dataset is available under  /vast/work/public/ml-datasets/waymo_open_dataset_scene_flow/vast/work/public/ml-datasets/waymo_open_dataset_v_1_2_0_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_3_2_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_4_1_individual_files ","version":"Next","tagName":"h3"},{"title":"How to create a project on Coldfront","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/coldfront_creating_a_project/","content":"How to create a project on Coldfront Login to coldfront at coldfront.hpc.nyu.edu either on-campus or on the VPN. Navigate to the &quot;Project&quot; item in the navigation bar on the top of the page and select &quot;Projects&quot; from the drop down menu. If you are a PI, you will be able to see a button &quot;+ Add a Project&quot; button that will let you create a new project. Click on it to head to the project creation page where you can add the project title, description and the school it will be associated with. The school a project is associated with determines the resources it will have access to. The school approver for the associated school has the capacity to approve allocation requests for resources associated with the school. ","keywords":"","version":"Next"},{"title":"How to approve an allocation request","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/approving_an_allocation_request/","content":"How to approve an allocation request If you are an approver for your school, first login to coldfront page and you'll see &quot;staff&quot; navigation bar. Click the &quot;staff&quot; navigation bar and click &quot;Allocation Requests&quot; to see a list of requests waiting for your approval. Now, you'll see a list of allocation requests. Click &quot;Details&quot; button for the request that you would like to approve. You should click &quot;Details&quot; instead of just &quot;Approve&quot; button if you'd like to set more details. After clicking &quot;Details&quot;, now you'll see Allocation information part to fill in with your approval. Here, you could set start date, end date, description, etc. If you're done with adding the information, you could click &quot;Update&quot; button to update the allocation request with the information you put. After clicking &quot;Update&quot;, the allocation request is approved, and the allocation is activated now! Slurm account name is also available on the allocation attributes if you need it. If you have any difficulties or questions, please contact us at hpc@nyu.edu.","keywords":"","version":"Next"},{"title":"Getting and Renewing an Account","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/","content":"","keywords":"","version":"Next"},{"title":"Who is eligible for an NYU HPC account ?​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#who-is-eligible-for-an-nyu-hpc-account-","content":" NYU HPC clusters and related resources are available to full-time NYU faculty and to all NYU staff and, students with sponsorship from a full-time NYU faculty.  ","version":"Next","tagName":"h2"},{"title":"Getting a new account on the NYU HPC clusters​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-a-new-account-on-the-nyu-hpc-clusters","content":" To request an NYU HPC account please log in to NYU Identity Management service and follow the link to &quot;Request HPC account&quot;. We have a walkthrough of how to request an account through IIQ. If you are a student, alumni or an external collaborator you need an NYU faculty sponsor.  ","version":"Next","tagName":"h2"},{"title":"Renewing HPC account​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#renewing-hpc-account","content":" Each year, non-faculty users must renew their HPC account by filling in the account renewal form from the NYU Identity Management service. See Renewing your HPC account for a walkthrough of the process.  ","version":"Next","tagName":"h2"},{"title":"Information for faculty who sponsor HPC users​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#information-for-faculty-who-sponsor-hpc-users","content":" All full-time NYU faculty members (other than NYU Med School) are eligible to become sponsors and in turn can sponsor:  NYU Degree program students Scholars visiting NYU NYU Research staff NYU School of Medicine faculty, staff and students Other NYU staff/affiliates with a NetID Non-NYU researchers with whom they are actively collaborating  If you need to sponsor an HPC account for an external collaborator (for example, for an NYU alumnus), please, request a &quot;research affiliate&quot; affiliation for your collaborator. You can find the instructions at https://start.nyu.edu/.  You can request a NetID for your student(s) or collaborator(s) at https://start.nyu.edu/pwm/public/. The request form has additional information about affiliates.  HPC faculty sponsors are expected to:  Approve/disapprove sponsored users' association with you Approve/disapprove the purpose for which user is requesting an account on NYU HPC resources Agree to supervise the sponsored individual, to the extent necessary, to ensure proper use of the NYU HPC resource and compliance with all applicable policies Respond promptly to account-related requests from HPC staff  Each year, your sponsored users must renew their account. You will need to approve the renewal by logging into the NYU Identity Management service. We have a walkthrogh of the approval process here  ","version":"Next","tagName":"h2"},{"title":"Bulk HPC Accounts for Courses​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#bulk-hpc-accounts-for-courses","content":" HPC bulk accounts request is disabled for HPC sponsors.  If you would like to use JupyterHub for your classes, please don't submit the form below, read [Jupyter Hub page]TODO Add a page for JHub in the Cloud section instead (the link to an intake form is also there) Please fill out this request form for the course, we'll create HPC accounts for the class per request Note that accounts created for courses last until the end of the semester, rather than a full year.  ","version":"Next","tagName":"h2"},{"title":"Getting an account with one of NYU partners​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-an-account-with-one-of-nyu-partners","content":" NYU partners ([look for the list here]) with many state and national facilities with a variety of HPC systems and expertise. [Contact us] for assistance setting up a collaboration with any of these.  ","version":"Next","tagName":"h2"},{"title":"Non-NYU Researchers​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#non-nyu-researchers","content":" If you are part of collaboration with NYU researcher you need to obtain an affiliate status before applying for an NYU HPC account. A full-time NYU faculty member must sponsor a non-NYU collaborator for an affiliate status.  Please see instructions for affiliate management (NYU NetID login is required to follow the link). Please read instructions about sponsoring external collaborators here.  ","version":"Next","tagName":"h2"},{"title":"Access to cluster after Graduation​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#access-to-cluster-after-graduation","content":" If you will still work on a project with an NYU researchers after graduation - refer to the section above for &quot;Non-NYU Researchers&quot;  If you are not part of a collaboration, your access to cluster will end together with NetID becoming non-active. Please copy all your data cluster (if you need any) before that time.  ","version":"Next","tagName":"h2"},{"title":"VPN on a Linux machine​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/getting_and_renewing_an_account/#vpn-on-a-linux-machine","content":" note In order to request a new HPC account or renew an expired one, you need to be connected to the NYU VPN if you are working remotely, Please see instructions on how to install and use the NYU VPN. Linux clients are not officially supported, however we were able to successfully use openVPN client. Here are installation and connection instructions for a debian linux distribution with apt package manager: anyconnectopenconnect Download the VPN package from here and unzip the file;Type the following command in the Terminal sudo sh anyconnect-core-linux64-4.10.07061.sh  ","version":"Next","tagName":"h2"},{"title":"Managing allocations for your project","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/coldfront_requesting_an_allocation_request/","content":"","keywords":"","version":"Next"},{"title":"Current allocations​","type":1,"pageTitle":"Managing allocations for your project","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/coldfront_requesting_an_allocation_request/#current-allocations","content":" Scroll down and you'll reach the Allocations section. This section lists all the allocations associated with this project. All allocations have SLURM accounts associated with them. Allocations that are &quot;Active&quot; will allow you to submit jobs using the SLURM account associated with it.  ","version":"Next","tagName":"h2"},{"title":"Requesting new allocations​","type":1,"pageTitle":"Managing allocations for your project","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/coldfront_requesting_an_allocation_request/#requesting-new-allocations","content":" If you scroll down, you'll see &quot;+Request Resource Allocation&quot; button.  After clicking &quot;+Request Resource Allocation&quot;, you'll see a list of resources you can request for. All projects can request access to Torch, which provides access to the cluster. However, depending on the school the project is affiliated with, high priority resources are available.  Available resource for each school: TandonArts &amp; Sciences tandon_advanced: includes both A100 and H100 GPUstandon_priority: includes A100 GPUs only  Please select a resource and fill in justification to complete the allocation request process.  Now your allocation request is created! You'll see an allocation request with &quot;New&quot; status.  Once your school approver approves the request, the status will change to &quot;Active&quot; (and you will receive an email notifying you of the change). ","version":"Next","tagName":"h2"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/intro/","content":"Start here! Welcome to the Torch HPC documentation! If you do not have an HPC account, please proceed to the next section that explains how you may be able to get one. If you are an active user, you can proceed to one of the categories on the left.","keywords":"","version":"Next"},{"title":"How to approve an HPC Account Request","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/walkthrough_approve_hpc_account_request/","content":"How to approve an HPC Account Request When someone nominates you as their HPC sponsor, you should be notified by email. VPN Needed You need to be on the NYU VPN to perform this task! You can also log into IIQ at any time, and if you have a request awaiting your approval, it will appear in your &quot;Actions Items&quot; box, as per the following screenshot: Another way to get to pending approvals is to click on the line item in the “Latest Approvals” section which will lead directly to the approval page. For new HPC Account Requests, the page will look like this: Here, the Approve or Deny button should be clicked, then confirmed, in order to complete the request. For HPC Account Renewals, the page will look like this: Here, all systems should be selected by clicking the check box in the menu bar, and choosing “Select Everything”. Then, the “Select Bulk Action” menu is used to Approve or Reject all items selected. Please note that the line items may span multiple pages and all items must be acted upon in order to complete the request. Clicking “Complete” will complete the request.","keywords":"","version":"Next"},{"title":"HPC Accounts for Sponsored External Collaborators","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/hpc_accounts_external_collaborators/","content":"HPC Accounts for Sponsored External Collaborators External (non-NYU) collaborators can access, with proper sponsorship, the NYU HPC Environment. The first step is to sponsor a collaborator for an NYU netid (if they do not have one already). A department administrator or the faculty sponsor must submit the Affiliate Management Form (the link is only accessible over VPN, or within NYU-Net).Once a netid for the external collaborator is created, the collaborator must submit the Request for an NYU HPC account. tip The collaborator must setup VPN access to be able to access the HPC account request form.The collaborator must enter in the account request form the Netid of the sponsoring NYU Full time facultyThe collaborator should select &quot;External Collaborator&quot; as Affiliation, when filing the HPC account request form. Once the HPC request is submitted, the sponsoring faculty will receive an email with a link to approve (or deny) the HPC account request for the external collaborator. The account approval link can only be accessed over VPN. note Once the sponsoring faculty approves the account request, the HPC account is created within one hour. Once the HPC account is created, the external collaborator can access HPC resources as described here. note As with all sponsored accounts, HPC accounts for external collaborators are valid for a period of 12 months, at which point a renewal process is required to continue access to the NYU HPC environment.","keywords":"","version":"Next"},{"title":"How to request an HPC account","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/walkthrough_request_hpc_account/","content":"How to request an HPC account First, check if you already have an account. You can check this by attempting to log in to the cluster, according to the instructions at Connecting to the HPC Cluster VPN Needed You need to be on the NYU VPN to perform this task! Login to the URL given below, using your NetID/password, to create or manage HPC Account Requests:https://identity.it.nyu.edu/ Researcher/End-UserFaculty/Sponsor/Approver Upon logging in, an end user’s landing page will look like this: Note that if the menu does not appear, select the &quot;burger&quot; menu on the top left hand corner: Navigate to Manage Accounts &gt; Request HPC Account: You will be prompted with a form. Continue to fill it out and Submit.","keywords":"","version":"Next"},{"title":"Renewing your HPC Account","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/walkthrough_renew_hpc_account/","content":"Renewing your HPC Account VPN Needed You need to be on the NYU VPN to perform this task! Login to the URL given below, using your NetID/password, to create or manage HPC Account Requests: https://identity.it.nyu.edu/ Upon logging in, an end user’s landing page will look like this If the menu does not appear, select the &quot;burger&quot; menu on the top left hand corner: The burger menu will show an &quot;Update/Renew HPC Account&quot; option - select this. Next complete the form as instructed. Please note that all accounts require the sponsorship of a full-time NYU faculty member. The user’s name will be pre-populated, and the forms required fields must be completed (sponsor, reason for request, consent to terms of use). After clicking “Submit” the chosen sponsor will be notified of the request and provisioning will only occur after approval. NOTE: If your HPC Account is due for renewal you will get an update on your dashboard which will suggest you to fill out a form given in the &quot;Latest form&quot; widget for renewing your account If you are not a full-time NYU faculty member, you will need an NYU faculty member to sponsor your application. This is probably your thesis supervisor, or NYU collaborator. Hit submit, and the request will go to your sponsor to approve (if applicable), and your account will be created, usually within a day of being approved. You will be returned to the dashboard, and now you should see your request in the &quot;Pending Approvals&quot; tables. If after a few days you still do not have an account, check with your sponsor - they may have missed a step in the approval process. If you are still stuck, contact us at hpc@nyu.edu for assistance.","keywords":"","version":"Next"},{"title":"Machine Learning on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/intro/","content":"Machine Learning on HPC","keywords":"","version":"Next"},{"title":"HPC project management portal","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/getting_started/what_is_coldfront/","content":"HPC project management portal Coldfront is the HPC project and allocation management portal for Torch at NYU. You can access it by navigating to coldfront.hpc.nyu.edu. With it you can: PIsResearchersSchool Approvoers Create a projectRequest allocations for your projectsAdd/Delete members associated with your projectModify access to allocations for the members of your project VPN Needed You need to be connected to the NYU VPN or be present on campus to access the Coldfront HPC project management portal. How do I: Create a project: refer to creating a projectRequest an allocation: refer to submitting an allocation requestApprove an allocation request: refer to approving an allocation requestView resources that are available to me: refer to current allocations sub-section here A coldfront allocation is needed to perform any work on Torch. The next sections describe the project and allocation request process in detail.","keywords":"","version":"Next"},{"title":"LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_on_hpc/","content":"LLMs on HPC How to run LLMs on HPC.","keywords":"","version":"Next"},{"title":"PyTorch on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/pytorch_on_hpc/","content":"PyTorch on HPC Distribtued training, inference, etc","keywords":"","version":"Next"},{"title":"Singularity with Conda","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity?​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#what-is-singularity","content":" Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments. Containers allow for the support of highly specific environments and further increase scientific reproducibility and portability. Using Singularity containers, researchers can work in the reproducible containerized environments of their choice can easily tailor them to their needs.  ","version":"Next","tagName":"h2"},{"title":"Using Singularity Overlays for Miniforge (Python & Julia)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#using-singularity-overlays-for-miniforge-python--julia","content":" ","version":"Next","tagName":"h2"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NetID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Fine tune LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/","content":"","keywords":"","version":"Next"},{"title":"Model and Dataset Selection Rationale​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#model-and-dataset-selection-rationale","content":" Component\tConfigurationBase Model\tgoogle/gemma-3-4b-pt (pretrained) Comparison Model\tgoogle/gemma-3-4b-it (instruction-tuned) Dataset\ttimdettmers/openassistant-guanaco Justification\tUsing Gemma-3 allows direct comparison between base pretrained, our LoRA fine-tuned, and official instruction-tuned variants. The OpenAssistant Guanaco dataset provides high-quality instruction-following examples.  ","version":"Next","tagName":"h2"},{"title":"Dataset Overview​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#dataset-overview","content":" The timdettmers/openassistant-guanaco dataset is a high-quality instruction-following dataset containing conversational exchanges between humans and AI assistants. It includes diverse question-answer pairs covering topics like creative writing, problem-solving, factual queries, and technical explanations. The dataset is specifically designed to train models to follow instructions and provide helpful, harmless, and honest responses.  ","version":"Next","tagName":"h3"},{"title":"Fine-tuning Benefits​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#fine-tuning-benefits","content":" Fine-tuning Gemma-3-4B-PT on this dataset significantly improves the model's ability to:  Follow complex instructions: Better understanding of multi-step requests and nuanced promptsMaintain conversational context: Improved coherence in longer dialoguesProvide structured responses: Enhanced ability to format answers appropriatelyReduce harmful outputs: Training on carefully curated data reduces problematic responses  By using Gemma-3, we can compare three variants:  Base pretrained (google/gemma-3-4b-pt) - Raw language modeling capabilitiesOur LoRA fine-tuned - Custom instruction following on Guanaco datasetOfficial instruction-tuned (google/gemma-3-4b-it) - Google's instruction tuning baseline  This provides a comprehensive evaluation of fine-tuning effectiveness.    ","version":"Next","tagName":"h3"},{"title":"Three-Way Model Comparison​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#three-way-model-comparison","content":" ","version":"Next","tagName":"h2"},{"title":"Example Prompt: \"Explain quantum computing in simple terms for a beginner\"​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#example-prompt-explain-quantum-computing-in-simple-terms-for-a-beginner","content":" Base Pretrained Model (google/gemma-3-4b-pt):​  Quantum computing uses quantum bits or qubits instead of classical bits. Qubits can be in superposition states and can be entangled. This allows quantum computers to perform certain calculations exponentially faster than classical computers for specific problems like factoring large numbers and simulating quantum systems.   Our LoRA Fine-tuned Model (Gemma-3-4B-PT + LoRA on Guanaco):​  Great question! Let me break down quantum computing in simple terms: **Think of it like this**: Regular computers use bits that are either 0 or 1 (like light switches that are either ON or OFF). Quantum computers use &quot;quantum bits&quot; or &quot;qubits&quot; that can be 0, 1, or both at the same time - imagine a coin that's spinning in the air, being both heads and tails until it lands. **Why this matters**: This allows quantum computers to: - Try many solutions to a problem simultaneously - Solve certain types of problems much faster than regular computers - Handle complex calculations that would take classical computers thousands of years **Real applications**: Password breaking, drug discovery, weather prediction, and financial modeling.   The fine-tuned model provides clearer explanations with better structure, analogies, and practical context!  Official Instruction-tuned Model (google/gemma-3-4b-it):​  Quantum computing is a revolutionary approach to computation that leverages quantum mechanics principles. Here's a beginner-friendly explanation: **Basic Concept**: While classical computers use bits (0 or 1), quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously through &quot;superposition.&quot; **Key Advantages**: - Parallel processing of multiple possibilities - Exponential speedup for specific problem types - Superior performance in cryptography, optimization, and simulation **Current Applications**: Drug discovery, financial modeling, cryptography, and artificial intelligence research.   This comparison demonstrates how our custom fine-tuning can achieve similar or better instruction-following capabilities compared to the official instruction-tuned variant.    tip Complete scripts used are available here: https://github.com/NYU-RTS/rts-docs-examples/tree/main/hpc/llm_fine_tuning  ","version":"Next","tagName":"h3"},{"title":"System Environment Setup​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#system-environment-setup","content":" ","version":"Next","tagName":"h2"},{"title":"Singularity Container & Overlay Configuration​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#singularity-container--overlay-configuration","content":" Component\tConfigurationSingularity Image\t/scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif Overlay\tCreated using singularity overlay create --size 25000 overlay-25GB-conda.ext3 Conda Path\t/ext3/miniconda3 within overlay Singularity Shell Command\tSee below  singularity shell --nv \\ --overlay /scratch/&lt;NetID&gt;/fine-tune/overlay-25GB-conda.ext3:rw \\ /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif   ","version":"Next","tagName":"h3"},{"title":"Python Environment and Dependency Installation​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#python-environment-and-dependency-installation","content":" bash Miniconda3-latest-Linux-x86_64.sh -b -p /ext3/miniconda3 source /ext3/miniconda3/bin/activate pip install torch transformers datasets accelerate peft trl   ","version":"Next","tagName":"h3"},{"title":"Model Cache Configuration for Hugging Face​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#model-cache-configuration-for-hugging-face","content":" To avoid exceeding home directory quotas during large model downloads:  export HF_HOME=/scratch/&lt;NetID&gt;/.cache/huggingface   Ensure this is set both interactively and within sbatch scripts.    ","version":"Next","tagName":"h3"},{"title":"Operational Troubleshooting: Common Errors and Recommended Fixes​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#operational-troubleshooting-common-errors-and-recommended-fixes","content":" This section provides a comprehensive overview of all environment-related issues encountered during the fine-tuning of google/gemma-3-4b-pt on the NYU Greene HPC cluster. Each entry includes the error symptom, root cause, and resolution strategy, categorized for clarity.  ","version":"Next","tagName":"h2"},{"title":"1. Filesystem and Path Setup Issues​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#1-filesystem-and-path-setup-issues","content":" Problem\tSymptom\tCause\tResolutionIncorrect overlay filename\tNo such file: overlay-50GB-500K.ext3.gz\tThe filename was incorrectly assumed\tUse ls /scratch/work/public/overlay-fs-ext3/ to verify the correct file: overlay-50G-10M.ext3.gz Compressed overlay used directly\tFATAL: while loading overlay images...\tAttempted to use .gz file directly with Singularity\tRun gunzip overlay-50G-10M.ext3.gz before using the file Overlay missing in working directory\tsbatch cannot find the overlay file\tOverlay not copied to the training directory\tEnsure the overlay file is placed in /scratch/&lt;NetID&gt;/fine-tune/ where sbatch accesses it Invalid overlay structure\tFATAL: could not create upper dir\tOverlay created via fallocate + mkfs.ext3, missing necessary internal structure\tAlways use singularity overlay create --size 25000 to create overlays  ","version":"Next","tagName":"h3"},{"title":"2. Container Runtime and Overlay Mounting Errors​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#2-container-runtime-and-overlay-mounting-errors","content":" Problem\tSymptom\tCause\tResolutionGPU warning on login node\tWARNING: Could not find any nv files\t--nv flag used outside GPU-enabled session\tIgnore the warning, or only use --nv within a srun --gres=gpu:1 session Overlay locked by another process\toverlay in use by another process\tAn interactive container shell using the overlay was still active\tRun lsof or ps aux and terminate blocking process  ","version":"Next","tagName":"h3"},{"title":"3. Python Package Installation and Environment Setup Errors​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#3-python-package-installation-and-environment-setup-errors","content":" Problem\tSymptom\tCause\tResolutionwhich pip returns Illegal option --\tUnexpected error when checking pip\tUses /usr/bin/which instead of Bash built-in\tUse command -v pip or simply run pip --version xformers install fails due to missing torch\tNo module named torch during install\tPyTorch not installed before building xformers\tInstall torch first: pip install torch, then pip install xformers Missing transformers in sbatch\tImportError: No module named transformers\tConda not activated in job script\tAdd source /ext3/miniconda3/bin/activate before executing the training script Installed pip packages not found\tTraining job fails to locate modules\tpip used outside overlay context\tOnly install packages while the overlay is mounted with :rw in an active container session  ","version":"Next","tagName":"h3"},{"title":"4. Disk Quota and Cache Management Issues​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#4-disk-quota-and-cache-management-issues","content":" Problem\tSymptom\tCause\tResolutionQuota exceeded on home\tOSError: [Errno 122] Disk quota exceeded: ~/.cache/huggingface\tDefault HuggingFace cache path inside /home\tSet HF_HOME=/scratch/$USER/.cache/huggingface Cache redownloading on each sbatch\tHugging Face cache not shared\tHF_HOME not consistently defined\tPersist and reuse the same HF_HOME path across runs  ","version":"Next","tagName":"h3"},{"title":"5. Slurm Job Submission and Runtime Failures​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#5-slurm-job-submission-and-runtime-failures","content":" Problem\tSymptom\tCause\tResolutionInvalid Slurm account\tsbatch: Invalid account\t--account flag not set or invalid\tUse --account=pr_100_tandon_priority Conda environment not recognized\tNo module named transformers\tActivation missing in sbatch\tAdd source /ext3/miniconda3/bin/activate in sbatch Overlay not found during job\tsbatch fails to locate file\tOverlay not placed in expected directory\tEnsure all relevant files are in /scratch/&lt;NetID&gt;/fine-tune/ or update paths accordingly    ","version":"Next","tagName":"h3"},{"title":"Recommended Best Practices for Stable Execution​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#recommended-best-practices-for-stable-execution","content":" Recommendation\tRationaleUse singularity overlay create for overlay creation\tEnsures upper/ and work/ directories are properly set up Install pip packages only after mounting overlay\tEnsures packages persist and are isolated inside the overlay Activate Conda explicitly in sbatch\tSlurm jobs do not inherit interactive shell environments Set HF_HOME to /scratch\tPrevents hitting disk quota limits in home directories Avoid return_tensors=&quot;pt&quot; in tokenizer mapping\tLeads to shape mismatch errors in batched training Use subset sampling (e.g., train[:1%]) for testing\tMinimizes resource consumption and enables fast debugging    ","version":"Next","tagName":"h2"},{"title":"LoRA Configuration Parameters​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#lora-configuration-parameters","content":" LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning large models with reduced computational cost. It adapts the model's layers by adding low-rank matrices while maintaining the original model's parameters. This enables efficient training with fewer resources.  Learn more about LoRA here.  Here are the configuration parameters used for LoRA in this fine-tuning setup:  peft_config = LoraConfig( r=8, lora_alpha=16, target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;], lora_dropout=0.05, bias=&quot;none&quot;, task_type=TaskType.CAUSAL_LM )     ","version":"Next","tagName":"h2"},{"title":"sbatch Job Script for Model Training​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#sbatch-job-script-for-model-training","content":" ","version":"Next","tagName":"h2"},{"title":"Training Script: train_gemma3.py​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#training-script-train_gemma3py","content":" The complete training script is available here. Below are the key configuration snippets:  Model and Dataset Configuration:  # Model and dataset configuration model_name = &quot;google/gemma-3-4b-pt&quot; # Base pretrained model dataset_name = &quot;timdettmers/openassistant-guanaco&quot; output_dir = &quot;./gemma3_output&quot;   LoRA Configuration:  # LoRA configuration peft_config = LoraConfig( r=8, lora_alpha=16, target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;], lora_dropout=0.05, bias=&quot;none&quot;, task_type=TaskType.CAUSAL_LM )   Training Arguments:  # Training arguments training_args = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=4, gradient_accumulation_steps=4, num_train_epochs=1, learning_rate=2e-4, fp16=True, logging_steps=10, save_steps=50, save_total_limit=2, remove_unused_columns=False, dataloader_pin_memory=False, )   ","version":"Next","tagName":"h3"},{"title":"sbatch Script​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#sbatch-script","content":" #!/bin/bash #SBATCH --job-name=gemma3-finetune #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=40GB #SBATCH --gres=gpu:1 #SBATCH --time=12:00:00 #SBATCH --output=/scratch/&lt;NetID&gt;/fine-tune/gemma3_train_%j.out #SBATCH --error=/scratch/&lt;NetID&gt;/fine-tune/gemma3_train_%j.err #SBATCH --mail-type=END,FAIL #SBATCH --mail-user=&lt;NetID&gt;@nyu.edu export HF_HOME=/scratch/&lt;NetID&gt;/.cache/huggingface singularity exec --nv \\ --overlay /scratch/&lt;NetID&gt;/fine-tune/overlay-25GB-conda.ext3:rw \\ /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif \\ /bin/bash -c &quot; source /ext3/miniconda3/bin/activate cd /scratch/&lt;NetID&gt;/fine-tune python train_gemma3.py &quot;     ","version":"Next","tagName":"h3"},{"title":"Generated Output Artifacts​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#generated-output-artifacts","content":" File\tDescriptionadapter_model.safetensors\tLoRA adapter weights adapter_config.json\tAdapter architecture definition trainer_state.json\tTraining metadata training_args.bin\tSaved training configuration tokenizer_config.json, tokenizer.json\tTokenizer data  Location: /scratch/&lt;NetID&gt;/fine-tune/gemma3_output/checkpoint-13/    ","version":"Next","tagName":"h2"},{"title":"Training Completion Summary​","type":1,"pageTitle":"Fine tune LLMs on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ml_ai_hpc/llm_fine_tuning/#training-completion-summary","content":" Epochs\tSteps\tStatus1\t13\tCompleted successfully ","version":"Next","tagName":"h2"},{"title":"Open OnDemand (OOD) with Conda/Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/","content":"","keywords":"","version":"Next"},{"title":"OOD + Singularity + conda​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#ood--singularity--conda","content":" This page describes how to use your Singularity with conda environment in Open OnDemand (OOD) GUI at Greene.  ","version":"Next","tagName":"h2"},{"title":"Log Into Greene via the Terminal​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#log-into-greene-via-the-terminal","content":" The following commands must be run from the terminal. Information on accessing via the terminal can be found at Connecting to the HPC.  ","version":"Next","tagName":"h3"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NetID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Prepare Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#prepare-overlay-file","content":" [NetID@log-1 ~]$ mkdir /scratch/$USER/my_env [NetID@log-1 ~]$ cd /scratch/$USER/my_env [NetID@log-1 my_env]$ cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . [NetID@log-1 my_env]$ gunzip overlay-15GB-500K.ext3.gz   Above we used the overlay file overlay-15GB-500K.ext3.gz which will contain all of the installed packages. There are more optional overlay files. You can find instructions on the following pages: Singularity with Conda, Squash File System and Singularity.  ","version":"Next","tagName":"h3"},{"title":"Launch Singularity Environment for Installation​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#launch-singularity-environment-for-installation","content":" [NetID@log-1 ~]$ singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash   Above we used the Singularity OS image cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif which provides the base operating system environment for the conda environment. There are other Singularity OS images available at /scratch/work/public/singularity  Launching Singularity with the --overlay flag mounts the overlay file to a new directory: /ext3 - you will notice that when not using Singularity /ext3 is not available.  warning Be sure that you have the Singularity prompt (Singularity&gt;) and that /ext3 is available before the next step. Singularity&gt; ls -lah /ext3 total 8.5K drwxrwxr-x. 2 root root 4.0K Oct 19 10:01 . drwx------. 29 root root 8.0K Oct 19 10:01 ..   ","version":"Next","tagName":"h3"},{"title":"Install Miniforge to Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#install-miniforge-to-overlay-file","content":" Singularity&gt; wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh Singularity&gt; sh Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3   Next, create a wrapper script at /ext3/env.sh:  Singularity&gt; touch /ext3/env.sh Singularity&gt; echo '#!/bin/bash' &gt;&gt; /ext3/env.sh Singularity&gt; echo 'unset -f which' &gt;&gt; /ext3/env.sh Singularity&gt; echo 'source /ext3/miniforge3/etc/profile.d/conda.sh' &gt;&gt; /ext3/env.sh Singularity&gt; echo 'export PATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh Singularity&gt; echo 'export PYTHONPATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh   Your /ext3/env.sh file should now contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies.  Next, activate your conda environment with the following:  Singularity&gt; source /ext3/env.sh   ","version":"Next","tagName":"h3"},{"title":"Install Packages to Miniforge Environment​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#install-packages-to-miniforge-environment","content":" Now that your environment is activated, you can update and install packages:  Singularity&gt; conda config --remove channels defaults Singularity&gt; conda update -n base conda -y Singularity&gt; conda clean --all --yes Singularity&gt; conda install pip --yes Singularity&gt; conda install ipykernel --yes # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  Singularity&gt; unset which Singularity&gt; which conda # output: /ext3/miniforge3/bin/conda Singularity&gt; which python # output: /ext3/miniforge3/bin/python Singularity&gt; python --version # output: Python 3.12.9 Singularity&gt; which pip # output: /ext3/miniforge3/bin/pip   Now use either conda or pip to install your required python packages to the Miniforge environment.  To install larger packages, like Tensorflow, you must first start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  Singularity&gt; exit [NetID@log-1 my_env]$ srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node [NetID@cm001 my_env]$ singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash Singularity&gt; source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  ","version":"Next","tagName":"h3"},{"title":"Configure iPython Kernels​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#configure-ipython-kernels","content":" To create a kernel named my_env copy the template files to your home directory.  Singularity&gt; mkdir -p ~/.local/share/jupyter/kernels Singularity&gt; cd ~/.local/share/jupyter/kernels Singularity&gt; cp -R /share/apps/mypy/src/kernel_template ./my_env # this should be the name of your Singularity env Singularity&gt; cd ./my_env Singularity&gt; ls #kernel.json logo-32x32.png logo-64x64.png python # files in the ~/.local/share/jupyter/kernels directory   To set the conda environment, edit the file named python in /.local/share/jupyter/kernels/my_env/.  The python file is a wrapper script that the Jupyter notebook will use to launch your Singularity container and attach it to the notebook.  At the bottom of the file we have the template singularity command.  singularity exec $nv \\ --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:ro \\ /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif \\ /bin/bash -c &quot;source /ext3/env.sh; $cmd $args&quot;   warning If you used a different overlay (/scratch/$USER/my_env/overlay-15GB-500K.ext3 shown above) or .sif file (/scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif shown above), you MUST change those lines in the command above to the files you used.  Edit the default kernel.json file by setting PYTHON_LOCATION and KERNEL_DISPLAY_NAME using a text editor like nano/vim.  { &quot;argv&quot;: [ &quot;PYTHON_LOCATION&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;KERNEL_DISPLAY_NAME&quot;, &quot;language&quot;: &quot;python&quot; }   to  { &quot;argv&quot;: [ &quot;/home/&lt;Your NetID&gt;/.local/share/jupyter/kernels/my_env/python&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;my_env&quot;, &quot;language&quot;: &quot;python&quot; }   Update the &quot;&lt;Your NetID&gt;&quot; to your own NetID without the &quot;&lt;&gt;&quot; symbols.  ","version":"Next","tagName":"h3"},{"title":"Launch an Open OnDemand Jupyter Notebook​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#launch-an-open-ondemand-jupyter-notebook","content":" https://ood.hpc.nyu.edu    ","version":"Next","tagName":"h3"},{"title":"Configure and Launch your Notebook​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#configure-and-launch-your-notebook","content":"   ","version":"Next","tagName":"h3"},{"title":"Select kernel​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/ood/open_on_demand/#select-kernel","content":" Once configured and launched, kernels can be selected in the &quot;New&quot; dropdown or within the notebook under the kernel menu. Please note that your notebook view may look slightly different depending on available directories and environments, as well as if you choose the lab or traditional notebook view.   ","version":"Next","tagName":"h3"},{"title":"Greene Spec Sheet","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/","content":"","keywords":"","version":"Next"},{"title":"Hardware Specs​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#hardware-specs","content":" Please find Greene's hardware specification in detail at the google sheets here:  tip Hover a mouse over a cell with a black triangle to see more details_    ","version":"Next","tagName":"h2"},{"title":"Mounted Storage Systems​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#mounted-storage-systems","content":" Please find the details on Greene's available storage offerings at the google sheets here:    ","version":"Next","tagName":"h2"},{"title":"General Parallel File System (GPFS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#general-parallel-file-system-gpfs","content":" The NYU HPC Clusters are served by a General Parallel File System (GPFS) storage cluster. GPFS is a high-performance clustered file system software developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters.  The cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464 TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1 PB raw storage  ","version":"Next","tagName":"h2"},{"title":"GPFS Performance​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#gpfs-performance","content":" \tRead Bandwidth\t78 GB per second reads Write Bandwidth\t42 GB per second writes I/O Performance\t~650k Input/Output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, if you have jobs to run with huge number of tiny files, VAST may be a good candidate.  Please contact the team hpc@nyu.edu for more information.  NVMe Interface778 TB Total StorageAvailable to all users as read onlyWrite access available to approved users only  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and work amongst project or lab members for long term research needs.RPS directories are available on the Greene HPC cluster.RPS is backed up. There is no file purging policy on RPS.There is a cost per TB per year, and inodes per year for RPS volumes.  Please find more information at [Research Project Space page].  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Nodes (gDTN)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/spec_sheet/#data-transfer-nodes-gdtn","content":" \tNode Type\tLenovo SR630 Number of Nodes\t2 CPUs\t2x Intel Xeon Gold 6244 8C 150W 3.6 GHz Processor. Memory\t192 GB (total) - 12x 16GB DDR4, 2933 MHz Local Disk\t1x 1.92 TB SSD Infiniband Interconnect\t1x Mellanox ConnectX-6 HDR100/100GbE VPI 1-Port x16 PCIe 3.0 HCA Ethernet Connectivity to the NYU High-Speed Research Network (HSRN)\t200 Gbit - 1x Mellanox ConnectX-5 EDR IB/100GbE VPI Dual-Port x16 PCIe 3.0 HCA ","version":"Next","tagName":"h2"},{"title":"Miniforge Environment PyTorch Example​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#miniforge-environment-pytorch-example","content":" Conda environments allow users to create customizable, portable work environments and dependencies to support specific packages or versions of software for research. Common conda distributions include Anaconda, Miniconda and Miniforge. Packages are available via &quot;channels&quot;. Popular channels include &quot;conda-forge&quot; and &quot;bioconda&quot;. In this tutorial we shall use Miniforge which sets &quot;conda-forge&quot; as the package channel. Traditional conda environments, however, also create a large number of files that can cut into quotas. To help reduce this issue, we suggest using Singularity, a container technology that is popular on HPC systems. Below is an example of how to create a pytorch environment using Singularity and Miniforge.  Create a directory for the environment  [NetID@log-1 ~]$ mkdir /scratch/&lt;NetID&gt;/pytorch-example [NetID@log-1 ~]$ cd /scratch/&lt;NetID&gt;/pytorch-example   Copy an appropriate gzipped overlay images from the overlay directory. You can browse available images to see available options  [NetID@log-1 pytorch-example]$ ls /scratch/work/public/overlay-fs-ext3   In this example we use overlay-15GB-500K.ext3.gz as it has enough available storage for most conda environments. It has 15GB free space inside and is able to hold 500K files You can use another size as needed.  [NetID@log-1 pytorch-example]$ cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . [NetID@log-1 pytorch-example]$ gunzip overlay-15GB-500K.ext3.gz   Choose a corresponding Singularity image. For this example we will use the following image  /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif   For Singularity image available on nyu HPC greene, please check the singularity images folder  [NetID@log-1 pytorch-example]$ ls /scratch/work/public/singularity/   For the most recent supported versions of PyTorch, please check the PyTorch website.  Launch the appropriate Singularity container in read/write mode (with the :rw flag)  singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash   The above starts a bash shell inside the referenced Singularity Container overlaid with the 15GB 500K you set up earlier. This creates the functional illusion of having a writable filesystem inside the typically read-only Singularity container.  Now, inside the container, download and install miniforge to /ext3/miniforge3  Singularity&gt; wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh Singularity&gt; bash Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3 # rm Miniforge3-Linux-x86_64.sh # if you don't need this file any longer   Next, create a wrapper script /ext3/env.sh using a text editor, like nano.  Singularity&gt; touch /ext3/env.sh Singularity&gt; nano /ext3/env.sh   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies. The script should contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   Activate your conda environment with the following:  Singularity&gt; source /ext3/env.sh   If you have the &quot;defaults&quot; channel enabled, please disable it with  Singularity&gt; conda config --remove channels defaults   Now that your environment is activated, you can update and install packages:  Singularity&gt; conda update -n base conda -y Singularity&gt; conda clean --all --yes Singularity&gt; conda install pip -y Singularity&gt; conda install ipykernel -y # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  Singularity&gt; unset -f which Singularity&gt; which conda # output: /ext3/miniforge3/bin/conda Singularity&gt; which python # output: /ext3/miniforge3/bin/python Singularity&gt; python --version # output: Python 3.12.10 Singularity&gt; which pip # output: /ext3/miniforge3/bin/pip Singularity&gt; exit # exit Singularity   Install packages​  You may now install packages into the environment with either the pip install or conda install commands.  First, start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  [NetID@log-1 pytorch-example]$ srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node [NetID@cm001 pytorch-example]$ singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash Singularity&gt; source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  We will install PyTorch as an example:  Singularity&gt; pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 Singularity&gt; pip3 install jupyter jupyterhub pandas matplotlib scipy scikit-learn scikit-image Pillow   For the latest versions of PyTorch please check the PyTorch website.  You can see the available space left on your image with the following commands:  find /ext3 | wc -l # output: should be something like: 77674 du -sh /ext3 # output should be something like: 6.5G /ext3   Now, exit the Singularity container and then rename the overlay image. Typing 'exit' and hitting enter will exit the Singularity container if you are currently inside it. You can tell if you're in a Singularity container because your prompt will be different, such as showing the prompt 'Singularity&gt;'  Singularity&gt; exit [NetID@cm001 pytorch-example]$ mv overlay-15GB-500K.ext3 my_pytorch.ext3   Test your PyTorch Singularity Image​  [NetID@cm001 pytorch-example]$ singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #output: 2.7.1+cu126   note the end ':ro' addition at the end of the pytorch ext3 image starts the image in read-only mode. To add packages you will need to use ':rw' to launch it in read-write mode.  ","version":"Next","tagName":"h3"},{"title":"Using your Singularity Container in a SLURM Batch Job​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#using-your-singularity-container-in-a-slurm-batch-job","content":" Below is an example script of how to call a python script, in this case torch-test.py, from a SLURM batch job using your new Singularity image  torch-test.py:  #!/bin/env python import torch print(torch.__file__) print(torch.__version__) # How many GPUs are there? print(torch.cuda.device_count()) # Get the name of the current GPU print(torch.cuda.get_device_name(torch.cuda.current_device())) # Is PyTorch using a GPU? print(torch.cuda.is_available())   Now we will write the SLURM job script, run-test.SBATCH, that will start our Singularity Image and call the torch-test.py script.  run-test.SBATCH:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --gres=gpu #SBATCH --job-name=torch module purge singularity exec --nv \\ --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro \\ /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\\ /bin/bash -c &quot;source /ext3/env.sh; python torch-test.py&quot;   You will notice that the singularity exec command features the --nv flag - this flag is required to pass the CUDA drivers from a GPU to the Singularity container.  Run the run-test.SBATCH script  [NetID@log-1 pytorch-example]$ sbatch run-test.SBATCH   Check your SLURM output for results, an example is shown below  [NetID@log-1 pytorch-example]$ cat slurm-3752662.out # example output: # /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py # 1.8.0+cu111 # 1 # Quadro RTX 8000 # True   ","version":"Next","tagName":"h3"},{"title":"Optional: Convert ext3 to a compressed, read-only squashfs filesystem​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#optional-convert-ext3-to-a-compressed-read-only-squashfs-filesystem","content":" Singularity images can be compressed into read-only squashfs filesystems to conserve space in your environment. Use the following steps to convert your ext3 Singularity image into a smaller squashfs filesystem.  [NetID@log-1 pytorch-example]$ srun -N1 -c4 singularity exec --overlay my_pytorch.ext3:ro /scratch/work/public/singularity/centos-8.2.2004.sif mksquashfs /ext3 /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf -keep-as-directory -processors 4 -noappend   Here is an example of the amount of compression that can be realized by converting:  [NetID@log-1 pytorch-example]$ ls -ltrsh my_pytorch.* 5.5G -rw-r--r-- 1 wang wang 5.5G Mar 14 20:45 my_pytorch.ext3 2.2G -rw-r--r-- 1 wang wang 2.2G Mar 14 20:54 my_pytorch.sqf   Notice that it saves over 3GB of storage in this case, though your results may vary.  Use a squashFS Image for Running Jobs​  You can use squashFS images similarly to the ext3 images.  [NetID@log-1 pytorch-example]$ singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #example output: /ext3/miniforge3/lib/python3.12/site-packages/torch/__init__.py #example output: 2.6.0+cu124   Adding Packages to a Full ext3 or squashFS Image​  If the first ext3 overlay image runs out of space or you are using a squashFS conda environment, but need to install a new package inside, please copy another writable ext3 overlay image to work together.  Open the first image in read only mode  [NetID@log-1 pytorch-example]$ cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . [NetID@log-1 pytorch-example]$ gunzip overlay-2GB-100K.ext3.gz [NetID@log-1 pytorch-example]$ singularity exec --overlay overlay-2GB-100K.ext3 --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash Singularity&gt; source /ext3/env.sh Singularity&gt; pip install tensorboard   note Please see Conda Environments for information on how to configure your conda environment.  tip Please also keep in mind that once the overlay image is opened in default read-write mode, the file will be locked. You will not be able to open it from a new process. Once the overlay is opened either in read-write or read-only mode, it cannot be opened in RW mode from other processes either. For production jobs to run, the overlay image should be open in read-only mode. You can run many jobs at the same time as long as they are run in read-only mode. In this ways, it will protect the computation software environment, software packages are not allowed to change when there are jobs running.  ","version":"Next","tagName":"h3"},{"title":"Julia Singularity Image​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#julia-singularity-image","content":" Singularity can be used to set up a Julia environment.  Create a directory for your julia work, such as /scratch/&lt;NetID&gt;/julia, and then change to your working directory to it. An example is shown below:  [NetID@log-1 NetID]$ mkdir /home/&lt;NetID&gt;/julia [NetID@log-1 NetID]$ cd /home/&lt;NetID&gt;/julia   Copy an overlay image, such as the 2GB 100K overlay, which generally has enough storage for Julia packages. Once copied, unzip to the same folder, rename to julia-pkgs.ext3  [NetID@log-1 julia]$ cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . [NetID@log-1 julia]$ gunzip overlay-2GB-100K.ext3.gz [NetID@log-1 julia]$ mv overlay-2GB-100K.ext3 julia-pkgs.ext3   Copy the following wrapper script in the directory  [NetID@log-1 julia]$ cp -rp /share/apps/utils/julia-setup/* .   Now launch writable Singularity overlay to install packages  [NetID@log-1 julia]$ module purge [NetID@log-1 julia]$ module load knitro/12.3.0 [NetID@log-1 julia]$ module load julia/1.5.3 [NetID@log-1 julia]$ ~/julia/my-julia-writable julia&gt; using Pkg julia&gt; Pkg.add(&quot;KNITRO&quot;) julia&gt; Pkg.add(&quot;JuMP&quot;)   Now exit from the container to launch a read only version to test (example below)  [NetID@log-1 julia]$ ~/julia/my-julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type &quot;?&quot; for help, &quot;]?&quot; for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.5.3 (2020-11-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia&gt; using Pkg julia&gt; using JuMP, KNITRO julia&gt; m = Model(with_optimizer(KNITRO.Optimizer)) A JuMP Model Feasibility problem with: Variables: 0 Model mode: AUTOMATIC CachingOptimizer state: EMPTY_OPTIMIZER Solver name: Knitro julia&gt; @variable(m, x1 &gt;= 0) x1 julia&gt; @variable(m, x2 &gt;= 0) x2 julia&gt; @NLconstraint(m, x1*x2 == 0) x1 * x2 - 0.0 = 0 julia&gt; @NLobjective(m, Min, x1*(1-x2^2)) julia&gt; optimize!(m)   You can make the above code into a julia script to test batch jobs. Save the following as test-knitro.jl  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   You can add additional packages with commands like the one below.  warning Please do not install new packages when you have Julia jobs running, this may create issues with your Julia installation  [NetID@log-1 julia]$ ~/julia/my-julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   Run a SLURM job to test with the following sbatch command (e.g. julia-test.SBATCH)  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge module load julia/1.5.3 module load knitro/12.3.0 ~/julia/my-julia test-knitro.jl   Then run the command with the following:  [NetID@log-1 julia]$ sbatch julia-test.SBATCH   Once the job completes, check the SLURM output (example below)  [NetID@log-1 julia]$ cat slurm-1022969.out ======================================= Academic License (NOT FOR COMMERCIAL USE) Artelys Knitro 12.3.0 ======================================= Knitro presolve eliminated 0 variables and 0 constraints. datacheck: 0 hessian_no_f: 1 par_numthreads: 1 Problem Characteristics ( Presolved) ----------------------- Objective goal: Minimize Objective type: general Number of variables: 2 ( 2) bounded below only: 2 ( 2) bounded above only: 0 ( 0) bounded below and above: 0 ( 0) fixed: 0 ( 0) free: 0 ( 0) Number of constraints: 1 ( 1) linear equalities: 0 ( 0) quadratic equalities: 0 ( 0) gen. nonlinear equalities: 1 ( 1) linear one-sided inequalities: 0 ( 0) quadratic one-sided inequalities: 0 ( 0) gen. nonlinear one-sided inequalities: 0 ( 0) linear two-sided inequalities: 0 ( 0) quadratic two-sided inequalities: 0 ( 0) gen. nonlinear two-sided inequalities: 0 ( 0) Number of nonzeros in Jacobian: 2 ( 2) Number of nonzeros in Hessian: 3 ( 3) Knitro using the Interior-Point/Barrier Direct algorithm. Iter Objective FeasError OptError ||Step|| CGits -------- -------------- ---------- ---------- ---------- ------- 0 0.000000e+00 0.000e+00 WARNING: The initial point is a stationary point and only the first order optimality conditions have been verified. EXIT: Locally optimal solution found. Final Statistics ---------------- Final objective value = 0.00000000000000e+00 Final feasibility error (abs / rel) = 0.00e+00 / 0.00e+00 Final optimality error (abs / rel) = 0.00e+00 / 0.00e+00 # of iterations = 0 # of CG iterations = 0 # of function evaluations = 1 # of gradient evaluations = 1 # of Hessian evaluations = 0 Total program time (secs) = 1.03278 ( 1.014 CPU time) Time spent in evaluations (secs) = 0.00000 ===============================================================================   ","version":"Next","tagName":"h3"},{"title":"Using CentOS 8 for Julia (for Module Compatibility)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/containers/singularity_with_conda/#using-centos-8-for-julia-for-module-compatibility","content":" Building on the previous Julia example, this will demonstrate how to set up a similar environment using the Singularity CentOS 8 image for additional customization. Using the CentOS 8 overlay allows for the loading of modules installed on Greene, such as Knitro 12.3.0  Copy overlay image  [NetID@log-1 julia]$ cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . [NetID@log-1 julia]$ gunzip overlay-2GB-100K.ext3.gz [NetID@log-1 julia]$ mv overlay-2GB-100K.ext3 julia-pkgs.ext3   note The path in this example is /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3  To use modules installed into /share/apps you can make two directories  [NetID@log-1 julia]$ mkdir julia-compiled julia-logs   note Now, in this example, the absolute paths are as follows /scratch/&lt;NetID&gt;/julia/julia-compiled /scratch/&lt;NetID&gt;/julia/julia-logs   To launch Singularity with overlay images in writable mode to install packages  [NetID@log-1 julia]$ singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash   Implement a wrapper script /ext3/env.sh  #/bin/bash export JULIA_DEPOT_PATH=/ext3/pkgs # this changes the default installation path to the environment source /opt/apps/lmod/lmod/init/bash module use /share/apps/modulefiles module purge module load knitro/12.3.0 module load julia/1.5.3   Load julia via the wrapper script and check that it loads properly  Singularity&gt; source /ext3/env.sh Singularity&gt; which julia # example output: /share/apps/julia/1.5.3/bin/julia Singularity&gt; julia --version # example output: julia version 1.5.3   Run julia to install packages  Singularity&gt; julia julia&gt; using Pkg julia&gt; Pkg.add(&quot;KNITRO&quot;) julia&gt; Pkg.add(&quot;JuMP&quot;)   Set up a similar test script like the test-knitro.jl script above. Name it test.jl:  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   Now implement a wrapper script named julia into ~/bin, the overlay image is in readonly mode  #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3:ro \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Make the wrapper executable  [NetID@log-1 julia]$ chmod 755 ~/bin/julia   Test your installation with a SLURM job example. The following code has been put into a file called test-julia-centos.SBATCH  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge julia test.jl   Run the above with the following:  [NetID@log-1 julia]$ sbatch test-julia-centos.SBATCH   Read the output (example below)  [NetID@log-1 julia]$ cat slurm-764085.out   Installing New Julia Packages Later​  Implement another writable julia-writable with overlay image writable in order to install new Julia packages later  [NetID@log-1 julia]$ cd /home/&lt;NetID&gt;/bin [NetID@log-1 julia]$ cp -rp julia julia-writable   #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Check the writable image  [NetID@log-1 julia]$ which julia-writable #example output: ~/bin/julia-writable   Install packages to the writable image  [NetID@log-1 julia]$ julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   If you do not need host packages installed in /share/apps, you can work with Singularity OS image  /scratch/work/public/singularity/ubuntu-20.04.1.sif   download Julia installation package from https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz  install Julia to /ext3, setup PATH properly. It will be easy to move to other servers in future. ","version":"Next","tagName":"h3"},{"title":"Available storage systems","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/","content":"","keywords":"","version":"Next"},{"title":"GPFS​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/#gpfs","content":" General Parallel File System (GPFS) storage cluster is a high-performance clustered file system developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/#configuration","content":" The NYU HPC cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1PB raw storage  ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/#performance","content":" Read Speed: 78 GB per second read speedsWrite Speed: 42 GB per second write speedsI/O Performance: up to 650k input/output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage, is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, If you have jobs to run with huge amount of tiny files, VAST may be a good candidate. If you and your lab members are interested, please reach out to hpc@nyu.edu for more information.  NVMe interfaceTotal size: 778 TB  note /vast is available for all users to read and available to approved users to write data.  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/available_storage_systems/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and code amongst project or lab members.  RPS directories are available on the Greene HPC cluster.There is no old-file purging policy on RPS.RPS is backed up.There is a cost per TB per year and inodes per year for RPS volumes.  Please see Research Project Space for more information. ","version":"Next","tagName":"h2"},{"title":"Best Practices on HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/best_practices/","content":"","keywords":"","version":"Next"},{"title":"User Quota Limits and the myquota command​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/best_practices/#user-quota-limits-and-the-myquota-command","content":" All users have quote limits set on HPC fie systems. There are several types of quota limits, such as limits on the amount of disk space (disk quota), number of files (inode quota) etc. The default user quota limits on HPC file systems are listed on our Data Management page.  warning One of the common issues users report is running out of inodes in their home directory. This usually occurs during software installation, for example installing conda environment under their home directory. Running out of quota causes a variety of issues such as running user jobs being interrupted or users being unable to finish the installation of packages under their home directory.  Users can check their current utilization of quota using the myquota command. The myquota command provides a report of the current quota limits on mounted file systems, the user's quota utilization, as well as the percentage of quota utilization.  In the following example the user who executes the myquota command is out of inodes in their home directory. The user inode quota limit on the /home file system 30.0K inodes and the user has 33000 inodes, thus 110% of the inode quota limit.  $ myquota Hostname: log-1 at Sun Mar 21 21:59:08 EDT 2021 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 8.96GB(17.91%)/33000(110.00%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 811.09GB(15.84%)/2437(0.24%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST No/Yes 2.0TB/5.0M 0.00GB(0.00%)/1(0.00%)   Users can find out the number of inodes (files) used per subdirectory under their home directory ($HOME), by running the following commands:  $cd $HOME $ for d in $(find $(pwd) -maxdepth 1 -mindepth 1 -type d | sort -u); do n_files=$(find $d | wc -l); echo $d $n_files; done /home/netid/.cache 1507 /home/netid/.conda 2 /home/netid/.config 2 /home/netid/.ipython 11 /home/netid/.jupyter 2 /home/netid/.keras 2 /home/netid/.local 24185 /home/netid/.nv 2 /home/netid/.sacrebleu 46 /home/netid/.singularity 1 /home/netid/.ssh 5 /home/netid/.vscode-server 7216   ","version":"Next","tagName":"h2"},{"title":"Large number of small files​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/best_practices/#large-number-of-small-files","content":" In case your dataset or workflow requires to use large number of small files, this can create a bottleneck due to read/write rates.  Please refer to our page on working with a large number of files to learn about some of the options we recommend to consider.  ","version":"Next","tagName":"h2"},{"title":"Installing Python packages​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/best_practices/#installing-python-packages","content":" warning Your home directory has a relatively small number of inodes. If you create a conda or python environment in you home directory, this can eat up all the inodes.  Please review the Package Management section of the Greene Software Page. ","version":"Next","tagName":"h2"},{"title":"Data Transfers","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Data-Transfer nodes​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#data-transfer-nodes","content":" Attached to the NYU HPC cluster Greene, the Greene Data Transfer Node (gDTN) are nodes optimized for transferring data between cluster file systems (e.g. scratch) and other endpoints outside the NYU HPC clusters, including user laptops and desktops. The gDTNs have 100-Gb/s Ethernet connections to the High Speed Research Network (HSRN) and are connected to the HDR Infiniband fabric of the HPC clusters. More information on the hardware characteristics is available at Greene spec sheet.  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Node Access​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#data-transfer-node-access","content":" The HPC cluster filesystems include /home, /scratch, /archive and the HPC Research Project Space are available on the gDTN. The Data-Transfer Node (DTN) can be accessed in a variety of ways  From NYU-net and the High Speed Research Network: use SSH to the DTN hostname gdtn.hpc.nyu.eduFrom the Greene cluster (e.g., the login nodes): the hostname can be shortened to gdtn  info For example, to log in to a DTN from the Greene cluster, to carry out some copy operation, and to log back out, you can use a command sequence like: ssh gdtn rsync ... logout   Via specific tools like Globus  ","version":"Next","tagName":"h3"},{"title":"Tools for Data Transfer​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#tools-for-data-transfer","content":" ","version":"Next","tagName":"h2"},{"title":"Linux & Mac Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#linux--mac-tools","content":" scp and rsync​  warning Please use Data Transfer Nodes (DTNs) with these tools. While one can transfer data while on login nodes, it is considered a bad practice because it can degrade the node's performance.  Sometimes these two tools are convenient for transferring small files. Using the DTNs does not require to set up an SSH tunnel; use the hostname dtn.hpc.nyu.edu for one-step copying. See below for examples of commands invoked on the command line on a laptop running a Unix-like operating system:  scp HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/ rsync -av HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/   In particular, rsync can also be used on the DTNs to copy directories recursively between filesystems, e.g. (assuming that you are logged in to a DTN),  rsync -av /scratch/username/project1 /rw/sharename/   where username would be your user name, project1 a directory to be copied to the Research Workspace, and sharename the name of a share on the Research Workspace (either your NetID or the name of a project you're a member of).  ","version":"Next","tagName":"h3"},{"title":"Windows Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#windows-tools","content":" File Transfer Clients​  Windows 10 machines may have the Linux Subsystem installed, which will allow for the use of Linux tools, as listed above, but generally it is recommended to use a client such as WinSCP or FileZilla to transfer data. Additionally, Windows users may also take advantage of Globus to transfer files.  ","version":"Next","tagName":"h3"},{"title":"Tunneling​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#tunneling","content":" Read the detailed instructions for setting up tunnels.  ","version":"Next","tagName":"h3"},{"title":"Globus​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#globus","content":" Globus is the recommended tool to use for large-volume data transfers. It features automatic performance tuning and automatic retries in cases of file-transfer failures. Data-transfer tasks can be submitted via a web portal. The Globus service will take care of the rest, to make sure files are copied efficiently, reliably, and securely. Globus is also a tool for you to share data with collaborators, for whom you only need to provide the email addresses.  The Globus endpoint for Greene is available at nyu#greene. The endpoint nyu#prince has been retired. Detailed instructions available at Globus  ","version":"Next","tagName":"h3"},{"title":"rclone​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#rclone","content":" rclone - rsync for cloud storage, is a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on DTNs. Please see the documentation for how to use it.  ","version":"Next","tagName":"h3"},{"title":"Open OnDemand (OOD)​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#open-ondemand-ood","content":" One can use Open OnDemand (OOD) interface to upload data.  warning Please only use OOD for small data transfers! Please use Data-Transfer Nodes (DTNs) for moving large data.  ","version":"Next","tagName":"h3"},{"title":"FDT​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/data_transfers/#fdt","content":" FDT stands for &quot;Fast Data Transfer&quot;. It is a command line application written in Java. With the plugin mechanism, FDT allows users to load user-defined classes for Pre- and Post-Processing of file transfers. Users can start their own server processes. If you have use cases for FDT, visit the download page to get fdt.jar to start. Please contact hpc@nyu.edu for any questions. ","version":"Next","tagName":"h3"},{"title":"Globus","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/","content":"","keywords":"","version":"Next"},{"title":"Transferring data between endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#transferring-data-between-endpoints","content":" ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#endpoint","content":" A globus Endpoint is a data transfer location, a location where data can be moved to or from using Globus transfer, sync and sharing service. An endpoint can either be a personal endpoint (on a user’s personal computer) or a server endpoint (located on a server, for use by multiple users). Please see Data Transfer With Globus for details.  ","version":"Next","tagName":"h3"},{"title":"Collection​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#collection","content":" A collection is a named set of files (or blobs), hierarchically organized in folders.  ","version":"Next","tagName":"h3"},{"title":"Data Sharing​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#data-sharing","content":" Please see How to Share Data Using Globus for details from Globus.  Instructions for NYU​  The first step in transferring data is to get a Globus account at https://www.globus.org/. Click on &quot;Log in&quot; at upper right corner. Select &quot;New York University&quot; from the pull-down menu and click on &quot;Continue&quot;.    Enter your NYU NetID and password in the familiar screen, and hit &quot;LOGIN&quot; then go through the Multi-Factor Authentication.    The &quot;File Manager&quot; panel should come up as the following image. In order to be able to transfer files, you will need to specify two Collections. A collection is defined on top of an endpoint. We can search for a collection using an endpoint name. The Server Endpoint on the NYU HPC storage is nyu#greene .        ","version":"Next","tagName":"h3"},{"title":"Server and Personal Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#server-and-personal-endpoints","content":" note The NYU HPC Server Endpoint: nyu#greene  Globus Connect Server is already installed on the NYU HPC cluster creating a Server Endpoint named nyu#greene, that is available to authorized users (users with a valid HPC account) using Globus. If you want to move data to or from your computer and the NYU HPC cluster, you need to install Globus Connect Personal on your computer, thus creating a Personal Endpoint on your computer.  ","version":"Next","tagName":"h2"},{"title":"Moving data between Server Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#moving-data-between-server-endpoints","content":" If you plan to transfer data between Server Endpoints, such as between the NYU server endpoint nyu#greene and a server endpoint at another institution, you do not need to install Globus Connect Personal on your computer.  Creating a Personal Endpoint on your computer​  This needs to be done only once on your personal computer.  After clicking &quot;Transfer or Sync to...&quot;, click &quot;Search&quot; on the upper right side. Then follow the link &quot;Install Globus Connect Personal&quot;.  More information about Globus Connect Personal and download links for Linux, Mac and Windows can be found at: https://www.globus.org/globus-connect-personal    ","version":"Next","tagName":"h3"},{"title":"Transfer files between your Personal Endpoint and NYU nyu#greene​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#transfer-files-between-your-personal-endpoint-and-nyu-nyugreene","content":" To transfer files you need to specify two collections (endpoints). Specify one of them as Greene scratch directory, or Greene archive directory or Greene home directory. The other endpoint is the one created for your personal computer (e.g. My Mac Laptop) if it is involved in the transfer. When you first use the Greene directory collection, authentication/consent is required for the Globus web app to manage collections on this endpoint on your behalf.    When writing to your Greene archive directory, please pay attention that there is a default inode limit of 20K per user.  When the second Endpoint is chosen to be your personal computer, your computer home directory content will show up. Now select directory and files (you may select multiple files when clicking on file names while pressing down &quot;shift&quot; key), click one of the two blue Start buttons to indicate the transfer direction. After clicking the blue Start button, you should see a message indicating a transfer request has been submitted successfully, and a transfer ID is generated. Globus file transfer service takes care of the actual copying.  When the transfer is done, you should receive an email notification. Click &quot;ACTIVITY&quot; on the Globus portal, select the transfer you want to check, a finished transfer should look like the following:    ","version":"Next","tagName":"h3"},{"title":"Small file download from web browsers​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/globus/#small-file-download-from-web-browsers","content":" Globus support HTTPS access to data. To download a small file from your web browser, select a file and right-click your mouse, then click 'Download' at the popup menu.    Additional info can be found at File Management How-Tos.  Please contact hpc@nyu.edu if you have any questions. Good luck! ","version":"Next","tagName":"h3"},{"title":"HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/intro_and_data_management/","content":"","keywords":"","version":"Next"},{"title":"Highlights​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/intro_and_data_management/#highlights","content":" 9.5 PB Total GPFS Storage Up to 78 GB per second read speedsUp to 650k input/output operations per second (IOPS) Research Project Space (RPS): RPS volumes provide working spaces for sharing data and code amongst project or lab members  ","version":"Next","tagName":"h2"},{"title":"Introduction to HPC Data Management​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/intro_and_data_management/#introduction-to-hpc-data-management","content":" The NYU HPC Environment provides access to a number of file systems to better serve the needs of researchers managing data during the various stages of the research data lifecycle (data capture, analysis, archiving, etc.). Each HPC file system comes with different features, policies, and availability.  In addition, a number of data management tools are available that enable data transfers and data sharing, recommended best practices, and various scenarios and use cases of managing data in the HPC Environment.  Multiple public data sets are available to all users of the HPC environment, such as a subset of The Cancer Genome Atlas (TCGA), the Million Song Database, ImageNet, and Reference Genomes.  Below is a list of file systems with their characteristics and a summary table. Reviewing the list of available file systems and the various Scenarios/Use cases that are presented below, can help select the right file systems for a research project. As always, if you have any questions about data storage in the HPC environment, you can request a consultation with the HPC team by sending email to hpc@nyu.edu.  ","version":"Next","tagName":"h2"},{"title":"Data Security Warning​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/intro_and_data_management/#data-security-warning","content":" warning Moderate Risk Data - HPC Approved​ The HPC Environment has been approved for storing and analyzing Moderate Risk research data, as defined in the NYU Electronic Data and System Risk Classification Policy.High Risk research data, such as those that include Personal Identifiable Information (PII) or electronic Protected Health Information (ePHI) or Controlled Unclassified Information (CUI) should NOT be stored in the HPC Environment. note only the Office of Sponsored Projects (OSP) and Global Office of Information Security (GOIS) are empowered to classify the risk categories of data. tip High Risk Data - Secure Research Data Environments (SRDE) Approved​ Because the HPC system is not approved for High Risk data, we recommend using an approved system like the Secure Research Data Environments (SRDE).  ","version":"Next","tagName":"h3"},{"title":"Data Storage options in the HPC Environment​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/intro_and_data_management/#data-storage-options-in-the-hpc-environment","content":" User Home Directories​  Every individual user has a home directory (under /home/$USER, environment variable $HOME) for permanently storing code and important configuration files. Home Directories provide limited storage space (50 GB) and inodes (files) 30,000 per user. Users can check their quota utilization using the myquota command.  User home directories are backed up daily and old files under $HOME are not purged.  The User home directories are available on all HPC clusters (Greene) and on every cluster node (login nodes, compute nodes) as well as and Data Transfer Node (gDTN).  warning Avoid changing file and directory permissions in your home directory to allow other users to access files.  User Home Directories are not ideal for sharing files and folders with other users. HPC Scratch or Research Project Space (RPS) are better file systems for sharing data.  warning One of the common issues that users report regarding their home directories is running out of inodes, i.e. the number of files stored under their home exceeds the inode limit, which by default is set to 30,000 files. This typically occurs when users install software under their home directories, for example, when working with Conda and Julia environments, that involve many small files.  tip To find out the current space and inode quota utilization and the distribution of files under your home directory, please see: Understanding user quota limits and the myquota command.Working with Conda environments: To avoid running out of inode limits in home directories, the HPC team recommends setting up conda environments with Singularity overlay images  HPC Scratch​  The HPC scratch file system is the HPC file system where most of the users store research data needed during the analysis phase of their research projects. The scratch file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated scratch directory (/scratch/$USER) with 5 TB disk quota and 1,000,000 inodes (files) limit per user.  The scratch file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the scratch file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Scratch file system, users should not put important source code, scripts, libraries, executables in /scratch. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Scratch: All files on the HPC Scratch file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your scratch directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Vast​  The HPC Vast all-flash file system is the HPC file system where users store research data needed during the analysis phase of their research projects, particularly for high I/O data that can bottleneck on the scratch file system. The Vast file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC vast file system are subject to the HPC Vast old file purging policy: Files on the /vast file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated vast directory (/vast/$USER) with 2 TB disk quota and 5,000,000 inodes (files) limit per user.  The vast file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the vastsc file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Vast file system, users should not put important source code, scripts, libraries, executables in /vast. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Vast: All files on the HPC Vast file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your vast directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Research Project Space​  The HPC Research Project Space (RPS) provides data storage space for research projects that is easily shared amongst collaborators, backed up, and not subject to the old file purging policy. HPC RPS was introduced to ease data management in the HPC environment and eliminate the need of having to frequently copying files between Scratch and Archive file systems by having all projects files under one area. These benefits of the HPC RPS come at a cost. The cost is determined by the allocated disk space and the number of files (inodes).  For detailed information about RPS see: HPC Research Project Space  HPC Work​  The HPC team makes available a number of public datasets that are commonly used in analysis jobs. The data sets are available Read-Only under /scratch/work/public.  For some of the datasets users must provide a signed usage agreement before accessing.  Public datasets available on the HPC clusters can be viewed on the Datasets page.  HPC Archive​  Once the Analysis stage of the research data lifecycle has completed, HPC users should tar their data and code into a single tar.gz file and then copy the file to their archive directory (/archive/$USER). The HPC Archive file system is not accessible by running jobs; it is suitable for long-term data storage. Each user has access to a default disk quota of 2TB and 20,000 inode (files) limit. The rather low limit on the number of inodes per user is intentional. The archive file system is available only on login nodes of Greene. The archive file system is backed up daily.  Here is an example tar command that combines the data in a directory named my_run_dir under $SCRATCH and outputs the tar file in the user's $ARCHIVE:  # to archive `$SCRATCH/my_run_dir` tar cvf $ARCHIVE/simulation_01.tar -C $SCRATCH my_run_dir   NYU (Google) Drive​  Google Drive (NYU Drive) is accessible from the NYU HPC environment and provides an option to users who wish to archive data or share data with external collaborators who do not have access to the NYU HPC environment.  As of December 2023, storage limits were applied to all faculty, staff, and sutdent NYU Google accounts. Please see Google Workspace Storage for details  There are also limits to the data transfer rate in moving to/from Google Drive. Thus, moving many small files to Google Drive is not going to be efficient.  Please read the Instructions on how to use cloud storage within the NYU HPC Environment.  HPC Storage Mounts Comparison Table​    Please see the next page for best practices for data management on NYU HPC systems. ","version":"Next","tagName":"h3"},{"title":"Large Number of Small Files","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/","content":"","keywords":"","version":"Next"},{"title":"Motivation​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#motivation","content":" Many datasets contain a large number of files (for example ImageNet contains 14 million images, with ~150 GB size). How to deal with this data? How to store it? How to use for computations? Long-term storage of data is not an issue - an archive like tar.gz can handle this pretty well. However, when you want to use data in computations, the performance may depend on how you handle the data on disk.  Here are some ideas you can try and evaluate performance for your own project  ","version":"Next","tagName":"h2"},{"title":"Squash file system with Singularity​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#squash-file-system-with-singularity","content":" Please see Squash File System and Singularity  ","version":"Next","tagName":"h2"},{"title":"Use jpg/png files on disk​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#use-jpgpng-files-on-disk","content":" One option is to store image files (like png or jpg) on the disk and read from disk directly.  warning An issue with this approach, is that many linux file system can hold only a limited number of files.  # One can open greene cluster and run the following command $ df -ih /scratch/ Filesystem Inodes IUsed IFree IUse% Mounted on 10.0.0.40@o2ib:10.0.0.41@o2ib:/scratch1 1.6G 209M 1.4G 14% /scratch   This shows us that the total number of files '/scratch' can hold (currently) is about 1.6 G. This looks like a large number. But let us translate this into number of datasets like ImageNet (14 mil images) -&gt; 100 datasets like that would almost fully occupy Total possible slots for files! This is a problem!  And even if you can ignore this on your own PC, on HPC, there is a limit of files each user can put on /scratch to prevent such problems.  This is the reason why you can't just extract all those files in /scratch  ","version":"Next","tagName":"h2"},{"title":"SLURM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#slurm_tmpdir","content":" Another option would be to start a SLURM job and extract everything into $SLURM_TMPDIR. This can work, but would require you to un-tar every time you run a SLURM command.  ","version":"Next","tagName":"h2"},{"title":"SLURM_RAM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#slurm_ram_tmpdir","content":" You can also use the custom-made RAM mapped disk using #SLURM_RAM_TMPDIR while submitting the job. In this case when you start a job you first un-tar your files to $SLURM_RAM_TMPDIR and then read from there.  warning This basically requires you to use 2*(size of the data) size of RAM just to hold the data.  ","version":"Next","tagName":"h2"},{"title":"Binary files (pickle, etc)​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#binary-files-pickle-etc","content":" Store data in some binary file (say pickle in Python) which you load fully when you start a SLURM job.  This option may require a lot of RAM - thus you may have to wait a long time for the scheduler to find resources for your job. Also this approach would not work on a regular PC without so much RAM, and thus your scripts are not transferable.  ","version":"Next","tagName":"h2"},{"title":"Container files, one-file databases​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#container-files-one-file-databases","content":" Special containers, which allow to either load data fast fully or access chosen elements without loading the whole dataset into RAM.  ","version":"Next","tagName":"h2"},{"title":"SQLite​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#sqlite","content":" If you have structured data, a good option would be to use SQLite. Please see SQLite: Handling Large Structured Data for more information.  ","version":"Next","tagName":"h3"},{"title":"HDF5​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#hdf5","content":" One can think about HDF5 file as a &quot;container file&quot; (database of a sort), which holds a lot of objects inside.  HDF5 files do not have a file size limitation, and can hold huge number of objects inside, providing fast read/write access to those objects.  It is easy to learn how to subset data and load to RAM only to those data objects that you need.  More info:  Developers websitehdf5 in Python book hdf5 in R  tip You can get the Python book for free with your NYU email address. Go to 'log in' at the top right of the page and select to log in using your Google credentials. Use your NYU email address.  hdf5 supports reading and writing in parallel, so you can use several nodes reading from the same file.  More info: Documentation, Tutorial, Help Desk  ","version":"Next","tagName":"h3"},{"title":"LMDB​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#lmdb","content":" LMDB (Lightning Memory-Mapped Database) is a light-weight, high-speed embedded database for key-value data.  Essentially, this is a large file sitting on the disk that contains a lot of smaller objects inside.  This is a memory-mapped database meaning, file can be larger than RAM. OS is responsible for managing the pages (like caching frequently uses pages).  For practical use it means: say you have 10 GB of RAM, and LMDB file of 100 GB. When you connect to this file, OS may decide to load 5GB to RAM, and the rest 95GB will be attached as virtual memory. Greene does not have a limit for virtual memory. Of course, if your RAM is larger than LMDB file, this database will perform the best, as OS will have enough resources to keep what is needed directly in RAM.  tip when you write key-value pairs to LMDB they have to be byte-encoded. For example, if you use Python you can use: for string st.encode(), for np.array use ar.tobytes(), or in general pickle.dumps()  warning LMDB uses B Tree, which has O(log n) complexity for search. Thus, when number of elements in LMDB becomes really big, search of specific element slows down considerably  More info:  Developer websitePython package for lmdR package for lmdbDeep Learning Tensorflow with LMDB examplePytorch with LMDB example  LMDB supports reading by many readers and many parallel thread from the same file  Formats inside HDF5/LMDB: binary, numpy, other..​  One can store data in different way inside LMDB or HDF5. For example we can store binary representation of jpeg, or we can store python numpy array. In the first case file can be read from any language, in the second - only from Python. We can also store objects from other languages - for example tibble in R  Other formats​  There are other formats like Bcolz, Zarr, and others. Some examples can be found here.  ","version":"Next","tagName":"h3"},{"title":"Benchmarking Code​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/large_number_of_small_files/#benchmarking-code","content":" A benchmarking of various ways of reading data was performed on now retired Prince HPC cluster. You can find the code used to perform that benchmarking and the results at this repository.For those of you interested in using multiple cores for data reading, this code example below may be useful. Multiple cores on the same node are used. Parallelization is based on joblib Python module ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/","content":"","keywords":"","version":"Next"},{"title":"Description​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#description","content":" Research Project Space (RPS) volumes provide working space for sharing data and code amongst project or lab members. RPS directories are built on the same parallel file system (GPFS) like HPC Scratch. They are mounted on the cluster Compute Nodes, and thus they can be accessed by running jobs. RPS directories are backed up and there is no old file purging policy. These features of RPS simplify the management of data in the HPC environment as users of the HPC Cluster can store their data and code on RPS directories and they do not need to move data between the HPC Scratch and the HPC Archive file systems.  note Due to limitations of the underlying parallel file system, the total number of RPS volumes that can be created is limited.There is an annual cost associated with RPS.The disk space and inode usage in RPS directories do not count towards quota limits in other HPC file systems (Home, Scratch, and Archive).  ","version":"Next","tagName":"h2"},{"title":"Calculating RPS Costs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#calculating-rps-costs","content":" The PI should estimate the cost of the RPS volume by taking into account storage size and number of inodes (files). The cost is calculated annually. Costs are divided into the total space, in terabytes, and the number of inodes, in blocks of 200,000.  1 TB of Storage Cost: $100200,000 inodes Cost: $100  An initial RPS volume request must include both storage space and inodes. Modifications of existing RPS volumes can include just Storage or just inode adjustments.  An initial request includes 1TB and 200,000 inodes for an annual cost of $200.  ","version":"Next","tagName":"h2"},{"title":"Example RPS Requests​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#example-rps-requests","content":" Requests can include more storage or files, as needed, such as 1TB and 400,000 inodes or 2TB and 200,000 inodes. Both of the previous examples would cost $300, since they are requesting an incremental increase of storage or inodes, respectively.  This would be the breakdown of the examples listed above:  1 TB ($100) + 400,000 inodes ($200) = $3002 TB ($200) + 200,000 inodes ($100) = $300  ","version":"Next","tagName":"h3"},{"title":"Submitting an RPS volume Request or Modification​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#submitting-an-rps-volume-request-or-modification","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Decide the size (in TB) and number of inodes (files) that is needed for one year​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#step-1-decide-the-size-in-tb-and-number-of-inodes-files-that-is-needed-for-one-year","content":" The minimum size of an RPS request (to create a new RPS volume or extend an existing one) is 1TB of space.  If this is a new/first request, you must purchase both storage and inodes. A typical request includes 200,000 inodes per TB of storage.  tip Before submitting an RPS request (request for a new RPS volume or extending the size of an existing volume) PIs should estimate the growth of their data (in terms of storage space and number of files) during the entire year, rather than submitting a request based on their data storage needs at the time of the request.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Determine the cost of the request​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#step-2-determine-the-cost-of-the-request","content":" Determine the total annual cost of the request and the contact info of the School/Department/Center finance person.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Verify that the project PI has a valid HPC user account​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#step-3-verify-that-the-project-pi-has-a-valid-hpc-user-account","content":" The PI administers the top level RTS directory and grants access to other users. Thus the PI must have a valid HPC user account at the time of request. Please note that the HPC user account of NYU faculty never expires and thus does not need to be renewed every year. If the PI does not have an HPC account, please request one here.  ","version":"Next","tagName":"h3"},{"title":"Step 4: The PI submits the request to the HPC team via email​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#step-4-the-pi-submits-the-request-to-the-hpc-team-via-email","content":" warning Only PIs can submit RPS requests.  Submit your RPS request by contacting the HPC team via email (hpc@nyu.edu). Please include in the request:  size in TBnumber of inodescontact information of the School/Department/Center finance person  The HPC team will review the request and will contact the PI with any questions. If the request is approved, the HPC team will create (or adjust) the RPS volume with the PI's HPC user account as the owner of the RPS directory. An invoice will be generated by the IT finance team.  ","version":"Next","tagName":"h3"},{"title":"Current HPC RPS Stakeholders​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#current-hpc-rps-stakeholders","content":" HPC RPS Stakeholders  ","version":"Next","tagName":"h2"},{"title":"FAQs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#faqs","content":" ","version":"Next","tagName":"h2"},{"title":"Data Retention and Backups​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#data-retention-and-backups","content":" How long can I keep the lab data in RPS? For as long as the lab pays for the RPS resources. Even if the current HPC cluster retires, the RPS volumes will be transferred to the next cluster How can I find out how much of the storage and inodes have I used in my lab RPS volume Please contact HPC support What kind of backups are provided? Backups are done once a day (daily incremental). Backups are kept for 30 days. This means that if something was deleted more than 30 days ago, it won't be in the back ups and thus it won't be recoverable. Where are backups stored? RPS backups are stored on public cloud (AWS S3 Storage buckets).  ","version":"Next","tagName":"h3"},{"title":"Billing and Payments​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/research_project_space/#billing-and-payments","content":" What happens if I do not pay my bill? If the invoice is not paid for more than 60 days, the lab RPS directory will be 'tar'-ed and copied to an archival area. If 60 more days pass and the invoice is still not paid the tar files will be deleted. Can I pay for RPS using a credit card? Unfortunately we're unable to process credit card payments Can I pay for multiple years instead of paying every year? Yes, we can arrange for multiyear agreement ","version":"Next","tagName":"h3"},{"title":"Sharing Data on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#introduction","content":" To share files on the cluster with other users, we recommend using file access control lists (FACL) for a user to share access to their data with others. FACL mechanism allows a fine-grained control access to any files by any users or groups of users. We discourage users from setting 777 permissions with chmod, because this can lead to data loss (by a malicious user or unintentionally, by accident). Linux commands getfacl and setfacl are used to view and set access.  ACL mechanism, just like regular Linux POSIX, allows three different levels of access control:  Read (r) - the permission to see the contents of a fileWrite (w) - the permission to edit a fileeXecute (X) - the permission to call a file or run it (in this case we use X instead of x because the X permission uses inherited executable permissions and not all files need execution)  This level of access can be granted to  user (owner of the file)group (owner group)other (everyone else)  ACL allows to grant the same type access without modifying file ownership and without changing POSIX permissions.  ","version":"Next","tagName":"h2"},{"title":"Viewing ACL​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#viewing-acl","content":" Use getfacl to retrieve access permissions for a file.  $ getfacl myfile.txt # file: myfile.txt # owner: ab123 # group: users user::rw- group::--- other::---   The example above illustrates that in most cases ACL looks just like the chmod-based permissions: owner of the file has read and write permission, members of the group and everyone else have no permissions at all.  ","version":"Next","tagName":"h2"},{"title":"Setting ACL​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#setting-acl","content":" Modify access permissions Use setfacl:  # general syntax: $ setfacl [option] [action/specification] file # most important options are # -m to modify ACL # -x to remove ACL # -R to apply the action recursively (apply to everything inside the directory) # To set permissions for a user (user is either the user name or ID): $ setfacl -m &quot;u:user:permissions&quot; &lt;file/dir&gt; ## To set permissions for a group (group is either the group name or ID): $ setfacl -m &quot;g:group:permissions&quot; &lt;file/dir&gt; # To set permissions for others: $ setfacl -m &quot;other:permissions&quot; &lt;file/dir&gt; # To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be copied into the directory afterwards): $ setfacl -dm &quot;entry&quot; &lt;dir&gt; # To remove a specific entry: $ setfacl -x &quot;entry&quot; &lt;file/dir&gt; # To remove the default entries: $ setfacl -k &lt;file/dir&gt; # To remove all entries (entries of the owner, group and others are retained): $ setfacl -b &lt;file/dir&gt;   tip Give Access to Parent Directories in the Path When you would like to set ACL to say /a/b/c/example.out, you also need to set appropriate ACLs to all the parent directories in the path. If you want to give read/write/execute permissions for the file /a/b/c/example.out, you would also need to give at least r-x permissions to the directories: /a, /a/b, and /a/b/c.  ","version":"Next","tagName":"h2"},{"title":"Remove All ACL Entries​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#remove-all-acl-entries","content":" $ setfacl -b abc   ","version":"Next","tagName":"h3"},{"title":"Check ACLs​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#check-acls","content":" $ getfacl abc # file: abc # owner: someone # group: someone user::rw- group::r-- other::r--   You can see with ls -l if a file has extended permissions set with setfacl: the + in the last column of the permissions field indicates that this file has detailed access permissions via ACLs:  $ ls -la total 304 drwxr-x---+ 18 ab123 users 4096 Apr 3 14:32 . drwxr-xr-x 1361 root root 0 Apr 3 09:35 .. -rw------- 1 ab123 users 4502 Mar 28 22:27 my_private_file -rw-r-xr--+ 1 ab123 users 29 Feb 11 23:18 dummy.txt   ","version":"Next","tagName":"h3"},{"title":"Flags​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#flags","content":" Please read man setfacl for possible flags. For example:  '-m' - modify'-x' - remove'-R' - recursive (apply ACL to all content inside a directory)'-d' - default (set given settings as default - useful for a directory - all the new content inside in the future will have given ACL)  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#examples","content":" ","version":"Next","tagName":"h2"},{"title":"File ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#file-acl-example","content":" Set read, write, and execute (rwX) permissions for user johnny to file named abc:  $ setfacl -m &quot;u:johnny:rwX&quot; abc   note We recommend for the permissions using a capital 'X' as using a lowercase 'x' will make all files executable, so we reommcned this: Check permissions: $ getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:rwX group::r-- mask::rwX other::r-- Change permissions for user johnny: $ setfacl -m &quot;u:johnny:r-X&quot; abc Check permissions: $ getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:r-X group::r-- mask::r-X other::r--   ","version":"Next","tagName":"h3"},{"title":"Directory ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/sharing_data_on_hpc/#directory-acl-example","content":" Let's say alice123 wants to share directory /scratch/alice123/shared/researchGroup/group1 with user bob123  ## Read/execute access to /scratch/alice123 setfacl -m u:bob123:r-X /scratch/alice123 ## Read/execute access to /scratch/alice123/shared setfacl -m u:bob123:r-X /scratch/alice123/shared ## Read/execute access to /scratch/alice123/shared/researchGroup setfacl -m u:bob123:r-X /scratch/alice123/shared/researchGroup ## Now I can finally can give access to directory /scratch/alice123/shared/researchGroup/group1 setfacl -Rm u:bob123:rwX /scratch/alice123/shared/researchGroup/group1   note user bob123 will be able to see content of the following directories /scratch/alise123//scratch/alise123/shared/scratch/alise123/shared/researchGroup//scratch/alise123/shared/researchGroup/group1 ","version":"Next","tagName":"h3"},{"title":"Transferring Cloud Storage Data with rclone","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/","content":"","keywords":"","version":"Next"},{"title":"Transferring files to and from Google Drive with RCLONE​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#transferring-files-to-and-from-google-drive-with-rclone","content":" Having access to Google Drive from the HPC environment provides an option to archive data and even share data with collaborators who have no access to the NYU HPC environment. Other options to archiving data include the HPC Archive file system and using Globus to share data with collaborators.  Access to Google Drive is provided by rclone - rsync for cloud storage - a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on Greene cluster as a module, the module versions currently available (March 2025) are:  rclone/1.68.2rclone/1.66.0rclone/1.60.1  For more details on how to use rclone to sync files to Google Drive, please see: RClone documentation for Google Drive  rclone can be invoked in one of the three modes:  Copy mode to just copy new/changed filesSync (one way) mode to make a directory identicalCheck mode to check for file hash equality  Please try with these options:  rclone --transfers=32 --checkers=16 --drive-chunk-size=16384k --drive-upload-cutoff=16384k copy source:sourcepath dest:destpath   This option works great for file sizes 1Gb+ to 250GB. Keep in mind that there is a rate limiting of 2 files/sec for upload into Google Drive. Small file transfers don’t work that well. If you have many small jobs, please tar the parent directory of such folders and split the tar file into 100GB chunks and then uploads then into Google Drive.  ","version":"Next","tagName":"h2"},{"title":"rclone Configuration​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#rclone-configuration","content":" You need to configure rclone before you will be able to move files between the HPC Environment and Google Drive  There are specific instruction on the rclone web site and here is an example of the process for configuring rclone for use on Greene:  ","version":"Next","tagName":"h2"},{"title":"Step 1: Login to Greene:​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-1-login-to-greene","content":" Follow instructions to log into the Greene HPC cluster.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Load the rclone module​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-2-load-the-rclone-module","content":" $ module load rclone/1.68.2   ","version":"Next","tagName":"h3"},{"title":"Step 3: Configure rclone​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-3-configure-rclone","content":" Configuring rclone and setting up remote access to your Google Drive, using the command:  $ rclone config   This will try to open the config files and you will see the below content:  You can select one of the options (here we show how to set up a new remote)  2025/03/14 16:39:23 NOTICE: Config file &quot;/Users/$USER/.config/rclone/rclone.conf&quot; not found - using defaults No remotes found, make a new one? n) New remote s) Set configuration password q) Quit config n/s/q&gt; n   Please enter n to create a new remote  Enter name for new remote. name&gt; nyu_google_drive   Please enter nyu_google_drive or something similar to name the new remote  Option Storage. Type of storage to configure. Choose a number from below, or type in your own value. 1 / 1Fichier \\ (fichier) 2 / Akamai NetStorage \\ (netstorage) 3 / Alias for an existing remote \\ (alias) 4 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, ArvanCloud, Ceph, ChinaMobile, Cloudflare, DigitalOcean, Dreamhost, GCS, HuaweiOBS, IBMCOS, IDrive, IONOS, LyveCloud, Leviia, Liara, Linode, Magalu, Minio, Netease, Petabox, RackCorp, Rclone, Scaleway, SeaweedFS, StackPath, Storj, Synology, TencentCOS, Wasabi, Qiniu and others \\ (s3) 5 / Backblaze B2 \\ (b2) 6 / Better checksums for other remotes \\ (hasher) 7 / Box \\ (box) 8 / Cache a remote \\ (cache) 9 / Citrix Sharefile \\ (sharefile) 10 / Combine several remotes into one \\ (combine) 11 / Compress a remote \\ (compress) 12 / Dropbox \\ (dropbox) 13 / Encrypt/Decrypt a remote \\ (crypt) 14 / Enterprise File Fabric \\ (filefabric) 15 / FTP \\ (ftp) 16 / Files.com \\ (filescom) 17 / Gofile \\ (gofile) 18 / Google Cloud Storage (this is not Google Drive) \\ (google cloud storage) 19 / Google Drive \\ (drive) 20 / Google Photos \\ (google photos) 21 / HTTP \\ (http) 22 / Hadoop distributed file system \\ (hdfs) 23 / HiDrive \\ (hidrive) 24 / ImageKit.io \\ (imagekit) 25 / In memory object storage system. \\ (memory) 26 / Internet Archive \\ (internetarchive) 27 / Jottacloud \\ (jottacloud) 28 / Koofr, Digi Storage and other Koofr-compatible storage providers \\ (koofr) 29 / Linkbox \\ (linkbox) 30 / Local Disk \\ (local) 31 / Mail.ru Cloud \\ (mailru) 32 / Mega \\ (mega) 33 / Microsoft Azure Blob Storage \\ (azureblob) 34 / Microsoft Azure Files \\ (azurefiles) 35 / Microsoft OneDrive \\ (onedrive) 36 / OpenDrive \\ (opendrive) 37 / OpenStack Swift (Rackspace Cloud Files, Blomp Cloud Storage, Memset Memstore, OVH) \\ (swift) 38 / Oracle Cloud Infrastructure Object Storage \\ (oracleobjectstorage) 39 / Pcloud \\ (pcloud) 40 / PikPak \\ (pikpak) 41 / Pixeldrain Filesystem \\ (pixeldrain) 42 / Proton Drive \\ (protondrive) 43 / Put.io \\ (putio) 44 / QingCloud Object Storage \\ (qingstor) 45 / Quatrix by Maytech \\ (quatrix) 46 / SMB / CIFS \\ (smb) 47 / SSH/SFTP \\ (sftp) 48 / Sia Decentralized Cloud \\ (sia) 49 / Storj Decentralized Cloud Storage \\ (storj) 50 / Sugarsync \\ (sugarsync) 51 / Transparently chunk/split large files \\ (chunker) 52 / Uloz.to \\ (ulozto) 53 / Union merges the contents of several upstream fs \\ (union) 54 / Uptobox \\ (uptobox) 55 / WebDAV \\ (webdav) 56 / Yandex Disk \\ (yandex) 57 / Zoho \\ (zoho) 58 / premiumize.me \\ (premiumizeme) 59 / seafile \\ (seafile) Storage&gt; 19   Please enter the number that corresponds to Google Drive. In the example above it is 19.  Option client_id. Google Application Client Id Setting your own is recommended. See https://rclone.org/drive/#making-your-own-client-id for how to create your own. If you leave this blank, it will use an internal key which is low performance. Enter a value. Press Enter to leave empty. client_id&gt;   Please leave this field blank and hit enter/return.  Option client_secret. OAuth Client Secret. Leave blank normally. Enter a value. Press Enter to leave empty. client_secret&gt;   Please leave this field blank and hit enter/return.  Scope that rclone should use when requesting access from drive. Enter a string value. Press Enter for the default (&quot;&quot;). Choose a number from below, or type in your own value 1 / Full access all files, excluding Application Data Folder. \\ &quot;drive&quot; 2 / Read-only access to file metadata and file contents. \\ &quot;drive.readonly&quot; / Access to files created by rclone only. 3 | These are visible in the drive website. | File authorization is revoked when the user deauthorizes the app. \\ &quot;drive.file&quot; / Allows read and write access to the Application Data folder. 4 | This is not visible in the drive website. \\ &quot;drive.appfolder&quot; / Allows read-only access to file metadata but 5 | does not allow any access to read or download file content. \\ &quot;drive.metadata.readonly&quot; scope&gt; 1   Please enter 1 or the number corresponding to the option for 'full access' if the options above have changed.  Service Account Credentials JSON file path Leave blank normally. Needed only if you want use SA instead of interactive login. Leading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`. Enter a string value. Press Enter for the default (&quot;&quot;). service_account_file&gt;   Please leave blank and hit enter/return.  Edit advanced config? (y/n) y) Yes n) No (default) y/n&gt; n   Please enter 'n'.  Use web browser to automatically authenticate rclone with remote? * Say Y if the machine running rclone has a web browser you can use * Say N if running rclone on a (remote) machine without web browser access If not sure try Y. If Y failed, try N. y) Yes (default) n) No y/n&gt;   Please enter 'n'  Option config_token. For this to work, you will need rclone available on a machine that has a web browser available. For more help and alternate methods see: https://rclone.org/remote_setup/ Execute the following on the machine with the web browser (same rclone version recommended): rclone authorize &quot;drive&quot; &quot;eyJzY29wZSI6ImRyaXZlIn0&quot; Then paste the result. Enter a value. config_token&gt;   As instructed, please copy the line in the output you get that is similar to:rclone authorize &quot;drive&quot; &quot;eyJzY29wZSI6ImRyaXZlIn0&quot;and paste that into a terminal on your local system and press enter/return.  That should open a web browser that will have you select your NYU account and will verify that you would like to give rclone access to your Google Drive. It will then provide you with a long code:  user@ITS tmp % rclone authorize &quot;drive&quot; &quot;eysi39xv82pmJzY29XZlIn0&quot; 2025/03/21 11:12:29 NOTICE: Make sure your Redirect URL is set to &quot;http://127.0.0.1:53682/&quot; in your custom config. 2025/03/21 11:12:29 NOTICE: If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=4c5yNwx6EsFWeise83svie 2025/03/21 11:12:29 NOTICE: Log in and authorize rclone for access 2025/03/21 11:12:29 NOTICE: Waiting for code... 2025/03/21 11:12:39 NOTICE: Got code Paste the following into your remote machine ---&gt; code removed &lt;---End paste   Please copy this code and paste it in as a response to the config_token&gt; prompt.  Configure this as a team drive? y) Yes n) No (default) y/n&gt; n   Please enter 'n'.  -------------------- [remote1] type = drive scope = drive token = {&quot;access_token&quot;:&quot;, removed &quot;} -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d&gt; y   Please enter 'y'.  Current remotes: Name Type ==== ==== remote1 drive e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; q   Please enter 'q' and we're done with configuration.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Transfer​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-4-transfer","content":" warning Please be sure to perform data transters on a data transfer node (DTN). It can degrade performance for other users to perform transfers on other types of nodes. For more information please see Data Transfers  Sample commands:  $ rclone lsd nyu_google_drive:   Transfer files to Google Drive, using the command below:  $ rclone copy &lt;source_folder or file&gt; &lt;remote_name&gt;:&lt;name_of_folder_on_gdrive&gt;   It looks something like below:  $ rclone copy /home/user1 nyu_google_drive:backup_home_user1   ","version":"Next","tagName":"h3"},{"title":"Step 5: Confirmation​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-5-confirmation","content":" The files are transferred and you can find the files on your Google Drive.  note Rclone only copies new files or files different from the already existing files on Google Drive. ","version":"Next","tagName":"h3"},{"title":"Slurm: Main Commands","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/","content":"","keywords":"","version":"Next"},{"title":"srun​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#srun","content":" Run a parallel job on cluster managed by Slurm, can be used:  Individual job submission where resources are allocated.In sbatch batch scripts as job steps making use of the allocated resource pool.within salloc instance making use of the resource pool.  man srun # for more information   Option\tDescription--help\tDisplay help information and exit --account\tCharge resource used by this job to a specified account --ntasks or --nodes\tRequest the number of tasks for the job Or Request the number of nodes to be allocated for this job --ntasks-per-node\tRequest that ntasks be invoked on each node. Meant to be used with --nodes --cpus-per-task\tRequest that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. --mem or --mem-per-cpu\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] Or Minimum memory required per allocated CPU --output\tRedirect stdout to a file --error\tRedirect stderr to a file --label\tPrepend task numbers to lines of stdout/err --partition\tRequest a specific partition for the resource allocation. If not specified, the default behavior is to allow the slurm controller to select the default partition as designated by the system administrator. --pty\tExecute task zero with pseudo terminal mode or using pseudo terminal specified by &lt;File Descriptor&gt;. --gres\tSpecifies a comma-delimited list of generic consumable resources, examples: --gres=gpu:1, --gres=gpu:v100:2, --gres=help or --gres=none --chdir\tSet the working directory of srun before it is executed  ","version":"Next","tagName":"h2"},{"title":"sbatch​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#sbatch","content":" man sbatch # for more information   Some of the popularly used directives are:  Option\tDescription#SBATCH --account\tCharge resource used by this jab to a specified account #SBATCH --nodes or #SBATCH --ntasks\tRequest allocation of minimum or maximum nodes for this job #SBATCH --ntasks-per-node\tRequest that ntasks be invoked on each node, used with --nodes #SBATCH --cpus-per-task\tAdvise the Slurm controller that ensuing job steps will require ncpus number of processors per task. Without this option, the controller will just try to allocate one processor per task #SBATCH --mem\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] #SBATCH --gres\tSpecifies a comma-delimited list of generic consumable resources. #SBATCH --output\tInstruct Slurm to connect the batch script's standard output directly to a specified filename #SBATCH --error\tInstruct Slurm to connect the batch script's standard error directly to a specified filename #SBATCH --mail-user\tUser to receive email notifications of state changes as defined by --mail-type #SBATCH --mail-type\tNotify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL etc. Multiple type values may be specified in a comma separated list. The user to be notified is indicated with --mail-user. #SBATCH --job-name\tSpecify a name for the job allocation, the default is the name of the batch script or just sbatch #SBATCH --constraint\tEnable constraints such as --constraint=&quot;nvidia&quot; to select any kind of nvidia GPUs or --constraint=&quot;amd&quot; to select any kind of amd GPUs or --constraint=&quot;a100|h100&quot; to select either any one of two GPUs #SBATCH --chdir\tSet the working directory of sbatch script before it is executed  ","version":"Next","tagName":"h2"},{"title":"salloc​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#salloc","content":" The options for salloc are similar to the ones used by srun or sbatch, consult the salloc manual pages for more information on additional options and their environment variables:  man salloc # for detailed information   ","version":"Next","tagName":"h2"},{"title":"sinfo​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#sinfo","content":" View information about slurm nodes and partitions.  man sinfo # for more information   sinfo --Format=Partition,GRES,CPUs,Features:26,NodeList   Format\tDescriptionAvailable\tState/availability of a partition CPUs\tNumber of CPUs per node CPUsState\tNumber of CPUs by state in the format &quot;allocated/idle/other/total&quot; Features:26\tFeatures available on the node, use : followed by a number which specifies the max number of characters printed for this column. sinfo prints max 20 characters by default per column Gres\tGeneric resource associated with the nodes GresUsed\tGeneric resource currently in use on the nodes MaxCPUsPerNode\tThe Max number of CPUs per node available to jobs in this partition Memory\tSize of memory per node in Megabytes NodeAI\tNumber of nodes by state in the format &quot;allocated/idle&quot; Nodes\tNumber of nodes NodeList\tList of node names Partition or PartitionName\tPartition name  ","version":"Next","tagName":"h2"},{"title":"squeue​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#squeue","content":" View information about jobs located on slurm scheduling queue.  man squeue # for more information   Options\tDescription--me\tPrints queued jobs for the current user --user\tPrints queued jobs under a specific user, or a comma list of users --job\tSpecify a comma separated list of job IDs to display --help\tPrint a help message describing all options squeue  ","version":"Next","tagName":"h2"},{"title":"sacct​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#sacct","content":" Displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.  man sacct # for more information   Most popularly used format options are:  Options\tDescription--format\tComma separated list of fields. (use &quot;--helpformat&quot; for a list of available fields). NOTE: When using the format option for listing various fields you can put a %NUMBER afterwards to specify how many characters should be printed. e.g. format=name%30 will print 30 characters of field name right justified. A %-30 will print 30 characters left justified. --helpformat\tPrint a list of fields that can be specified with --format option  Some popular options for --format are:  Format\tDescriptionJobID\tThe identification number of the job or job step JobName\tThe name of the job or job step State\tDisplays the job status or state, such as COMPLETED, TIMEOUT, FAILED etc AllocCPUS\tNumber of CPUs allocated to the job Elapsed\tElapsed time for the job Start\tInitiation time for the job End\tTermination time for the job  ","version":"Next","tagName":"h2"},{"title":"scancel​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_main_commands/#scancel","content":" Used to signal jobs or job steps that are under the control of slurm. A signal in the sense, send a termination signal to cancel a job.  Options\tDescription--interactive\tInteractive mode. Confirm each job_id.step_id before performing the cancel operation --jobname\tRestrict the scancel operations to a provided job name --me\tCancel all your jobs scancel &lt;a_job_id&gt;\tCancel a job and all it's steps scancel &lt;a_job_id&gt;.&lt;step_id_a&gt; &lt;a_job_id&gt;.&lt;step_id_b&gt;\tOnly cancel steps a and b for a given job, but not the rest of the steps scancel &lt;JobID_ArrayID&gt;\tOnly cancel a array id of an job array ","version":"Next","tagName":"h2"},{"title":"Slurm: Tutorial","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/","content":"","keywords":"","version":"Next"},{"title":"Introduction to High Performance Computing Clusters​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#introduction-to-high-performance-computing-clusters","content":" In a High Performance Computing Cluster, such as the NYU-IT HPC Greene cluster, there are hundreds of computing nodes interconnected by high-speed networks.  Linux operating system ( in our case Red Hat Enterprise Linux) runs on each of the nodes individually. The resources are shared among many users for their technical or scientific computing purposes.  Slurm is a cluster software layer built on top of the interconnected nodes, aiming at orchestrating the nodes' computing activities, so that the cluster could be viewed as a unified, enhanced and scalable computing system by its users.  In NYU HPC clusters the users coming from many departments with various disciplines and subjects, with their own computing projects, impose on us very diverse requirements regarding hardware, software resources, and processing parallelism. Users submit jobs, which compete for computing resources.  The Slurm software system is a resource manager and a job scheduler, which is designed to allocate resources and schedule jobs. Slurm is an open-source software, with a large user community, and has been installed on many top 500 supercomputers.  This tutorial assumes you have a NYU HPC account. If not, you may find the steps to apply for an account on the Getting and renewing an account page. It also assumes you are comfortable with Linux command-line environment. To learn about linux please read our Linux Tutorial. Please review the Hardware Specs page for more information on Greene's hardware specifications.  ","version":"Next","tagName":"h2"},{"title":"Slurm Commands​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#slurm-commands","content":" For an overview of useful Slurm commands, please read Slurm Main Commands page before continuing the tutorial.  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the modules made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  module load &lt;module-name&gt; : loads a module For example : module load python module unload &lt;module-name&gt; : unloads a module For example : module unload python module show &lt;module-name&gt; : see exactly what effect loading a module will have module purge : remove all loaded modules from your environment module whatis &lt;module-name&gt; : Find out more about a software package module list : check which modules are currently loaded in your environment module avail : check what software packages are available module help &lt;module-name&gt; : A module file may include more detailed help for software package  ","version":"Next","tagName":"h2"},{"title":"Batch Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#batch-job-example","content":" Batch jobs require a script file for the SLURM scheduler to interpret and execute. The SBATCH file contains both commands specific for SLURM to interpret as well as programs for it to execute. Below is a simple example of a batch job to run a Stata do file, the file is named myscript.sbatch :  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out module purge module load stata/14.2 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do   Below we will break down each line of the SBATCH script. More options can be found on the SchedMD website.  ## This tells the shell how to execute the script #!/bin/bash ## The #SBATCH lines are read by SLURM for options. ## In the lines below we ask for a single node, ## one task for that node, and one cpu for each task. #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 ## Time is the estimated time to complete, in this case 5 hours. #SBATCH --time=5:00:00 ## We expect no more than 2GB of memory to be needed #SBATCH --mem=2GB ## To make them easier to track, ## it's best to name jobs something recognizable. ## You can then use the name to look up reports with tools like squeue. #SBATCH --job-name=myTest ## These lines manage mail alerts for when the job ends, ## and who the email should be sent to. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu ## This places the standard output and standard error into the same file, ## in this case slurm_&lt;job_id&gt;.out #SBATCH --output=slurm_%j.out ## First we ensure a clean environment by purging the current one module purge ## Load the desired software, in this case stata 14.2 module load stata/14.2 ## Create a unique directory to run the job in. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR ## Set an environment variable for where the data is stored. DATADIR=$SCRATCH/my_project/data ## Change directories to the unique run directory cd $RUNDIR ## Execute the desired Stata do file script stata -b do $DATADIR/data_0706.do   You can submit the job with the following command:  sbatch myscript.sbatch   The command will result in the job queuing as it awaits resources to become available (which varies on the number of other jobs being run on the cluster and the resources requested). You can see the status of your jobs with the following command:  squeue --me   NOTE: Calling just squeue without passing the --me option will display all users' job queue status by default  Lastly, you can read the output of your job in the slurm-&lt;job_ID&gt;.out file produced by running your job. This is where logs regarding the execution of your job can be found, including errors or system messages. You can print the contents to the screen from the directory containing the output file with the following command:  cat slurm-&lt;job_ID&gt;.out   ","version":"Next","tagName":"h2"},{"title":"Interactive Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#interactive-job-example","content":" While the majority of the jobs on the cluster are submitted with the sbatch command, and executed in the background, there are also methods to run applications interactively through the srun command. Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface), providing an experience similar to working on a desktop or laptop. Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to obtain a rough idea of characteristics on the topic Getting graphical windows to run visualization Running software tools in interactive sessions  Interactive jobs also help avoid issues with the login nodes. If you are working on a login node and your job is too IO intensive, it may be removed without notice. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc.  In the srun example below, through --pty /bin/bash we request allocation of a pseudo terminal (pty) and start a bash shell session. By default the resource allocated is a single CPU core and 2GB memory for 1 hour time limit.  srun --pty /bin/bash   To request resources such as 4 CPU cores, 4 GB memory for 2 hours of maximum duration, you can add the following arguments:  srun --cpus-per-task=4 --time=2:00:00 --mem=4GB --pty /bin/bash   Similarly, to request one GPU card, 3 GB memory for a duration of 1.5 hours you can pass the following arguments to srun:  srun --time=1:30:00 --mem=3GB --gres=gpu:1 --pty /bin/bash   Once the job begins you will notice your prompt change, for example:  [mdw303@log-3 ~]$ srun --pty /bin/bash srun: job 7864254 queued and waiting for resources srun: job 7864254 has been allocated resources [mdw303@cs080 ~]$   You can see above that the prompt changed from log-3 ( one of the login nodes ) to cs080 ( one of the compute nodes ), meaning we have created a pseudo terminal and logged in with a bash shell on a compute node from our login node.  You can now load modules, software and run them interactively on the compute node having the resources ( CPUs, memory, GPUs etc ) that we asked for.  Below outlines the steps to start an interactive session and launch R:  [sk6404@log-1 ~]$ srun --cpus-per-task=1 --pty /bin/bash [sk6404@cs022 ~]$ module purge [sk6404@cs022 ~]$ module load r/intel/4.0.3 [sk6404@cs022 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.3 [sk6404@cs022 ~]$ R R version 4.0.3 (2020-10-10) -- &quot;Bunny-Wunnies Freak Out&quot; Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; q() Save workspace image? [y/n/c]: n [sk6404@cs022 ~]$ exit exit [sk6404@log-1 ~]$   ","version":"Next","tagName":"h2"},{"title":"MPI Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#mpi-job-example","content":" MPI stands for &quot;Message Passing Interface&quot; and is managed by a program, such as OpenMPI, to coordinate code and resources across the HPC cluster for your job to run workloads in parallel. You may have heard of HPC sometimes referred to as &quot;parallel computing&quot; because the ability to run many processes simultaneously - aka in parallel - is how the best efficiencies can be realized on the cluster. Users interested in MPI generally must compile the program they want to run using an MPI compiler.  Greene supports many MPI compilers. We'll be using the OpenMPI GCC compiler in this tutorial. It can be loaded as a module:  module load openmpi/gcc/4.1.6   Below we will illustrate an example of how to compile a C script for MPI. Copy this into your working directory as hellompi.c :  #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;mpi.h&gt; int main(int argc, char *argv[], char *envp[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); MPI_Get_processor_name(processor_name, &amp;namelen); printf(&quot;Process %d on %s out of %d\\n&quot;, rank, processor_name, numprocs); MPI_Finalize(); }   Once copied into your directory, load OpenMPI and compile it with the following:  module load openmpi/gcc/4.1.6 mpicc hellompi.c -o hellompi   Next, create a hellompi.sbatch script:  #!/bin/bash #SBATCH --nodes=4 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out # Load the default OpenMPI module. module purge module load openmpi/intel/4.1.1 # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi   Run the job with the following command:  sbatch hellompi.sbatch   After the job runs, cat the hellompi.out output file to see that your processes ran on multiple nodes. There may be some errors, but your output should contain something like the following, indicating the process was run in parallel on multiple nodes:  Process 0 on cs265.nyu.cluster out of 4 Process 1 on cs266.nyu.cluster out of 4 Process 2 on cs267.nyu.cluster out of 4 Process 3 on cs268.nyu.cluster out of 4   ","version":"Next","tagName":"h2"},{"title":"GPU Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#gpu-job-example","content":" To request one GPU card, use SBATCH directives in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use e.g. --gres=gpu:v100:1. The card types currently available are:  NVIDIA RTX 8000V100A100 NVIDIA 8358A100 NVIDIA 8380H100 NVIDIA AMD MI100MI250  As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  mkdir -p /scratch/$USER/myambertest cd /scratch/$USER/myambertest cp /share/apps/Tutorials/slurm/example/amberGPU/* . sbatch run-amber.s   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;.  NOTE: At the time of writing this you may need to update the run-amber.s script to load amber version 20.06, rather than the default 16.06.  The content of the job script &quot;run-amber.s&quot; should be as follows:  #!/bin/bash #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated. Check the one named mdout, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla V100 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Array Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_tutorial/#array-job-example","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both users and the scheduler system. Job arrays can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files. Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch run-jobarray.s, you submit 5 jobs to process each of these input files individually. Run the following commands to create the directory and submit the array job:  mkdir -p /scratch/$USER/myjarraytest cd /scratch/$USER/myjarraytest cp /share/apps/Tutorials/slurm/example/jobarray/* . ls   OUTPUT: run-jobarray.s sample-1.txt sample-2.txt sample-3.txt sample-4.txt sample-5.txt wordcount.py  sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --array=1-5 # this creates an array! #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python2 wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submissions create an environment variable called SLURM_ARRAY_TASK_ID, which is unique for each job in the array job. It is usually embedded somewhere so that at a job running time it's unique value is incorporated into producing a proper file name. Also as shown above: two additional options %A and %a, denoting the job ID and the task ID (i.e. job array index) respectively, are available for specifying a job's stdout, and stderr file names. ","version":"Next","tagName":"h2"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/support/support/","content":"Support Some of your questions may be already answered here Tutorial: Introduction to Using the Shell on GreeneTutorial: Introduction to High-Performance ComputingConsider to sign up for Training and Workshop. You can find the list of available HPC coruses can be viewed at nyu.libcal.com.Consider signing up for ACCESS workshops. As part of the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support program, NSF provides tutorials for HPC, OpenOnDemand, etc. Here's a list of upcoming workshops: link.Introductory HPC Video Playlist. NYU HPC offers personalized help through personal consultations for simple and advanced cases: Do you have trouble with something that seems trivial?Would you like to discuss how to better apply Deep Learning to your case?Something else?Contact us directly at hpc@nyu.edu","keywords":"","version":"Next"},{"title":"Greene System Status","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/system_status/","content":"","keywords":"","version":"Next"},{"title":"Resources Allocation and Queue (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/system_status/#resources-allocation-and-queue-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"Resource Allocation and Queue by partitions (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/system_status/#resource-allocation-and-queue-by-partitions-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"AMD Nodes System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/system_status/#amd-nodes-system-status","content":"   ","version":"Next","tagName":"h2"},{"title":"Storage System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/system_status/#storage-system-status","content":" Below you may find data for the following file system mounts  GPFS file system: /home, /scratch, /archiveVAST file system: /vast   ","version":"Next","tagName":"h2"},{"title":"Slurm: Submitting Jobs","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/","content":"","keywords":"","version":"Next"},{"title":"Batch vs Interactive Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-vs-interactive-jobs","content":" HPC workloads are usually better suited to batch processing than interactive working.A batch job is sent to the system when submitted with an sbatch command.The working pattern we are all familiar with is interactive - where we type ( or click ) something interactively, and the computer performs the associated action. Then we type ( or click ) the next thing.Comments at the start of the script, which match a special pattern ( #SBATCH ) are read as Slurm options.  ","version":"Next","tagName":"h2"},{"title":"The trouble with interactive environments​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#the-trouble-with-interactive-environments","content":" There is a reason why GUIs are less common in HPC environments: point-and-click is necessarily interactive. In HPC environments (as we'll see in session 3) work is scheduled in order to allow exclusive use of the shared resources. On a busy system there may be several hours wait between when you submit a job and when the resources become available, so a reliance on user interaction is not viable. In Unix, commands need not be run interactively at the prompt, you can write a sequence of commands into a file to be run as a script, either manually (for sequences you find yourself repeating frequently) or by another program such as the batch system.  tip The job might not start immediately, and might take hours or days, so we prefer a batch approach: Plan the sequence of commands which will perform the actions we need and write the commands into a script. You can now run the script interactively, which is a great way to save effort if i frequently use the same workflow, or ... Submit the script to a batch system, to run on dedicated resources when they become available.  ","version":"Next","tagName":"h3"},{"title":"Where does the output go ?​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#where-does-the-output-go-","content":" The batch system writes stdout and stderr from a job to a file named for example &quot;slurm-12345.out&quot; You can change either stdout or stderr using sbatch options. While a job is running, it caches the stdout an stderr in the job working directory.You can use redirection to send output of a specific command into a file.  ","version":"Next","tagName":"h3"},{"title":"Writing and Submitting a Job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#writing-and-submitting-a-job","content":" There are two aspects to a batch job script:  A set of SBATCH directives describing the resources required and other information about the job.The script itself, comprised of commands to set up and perform the computations without additional user interaction.  ","version":"Next","tagName":"h3"},{"title":"A simple example​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#a-simple-example","content":" A typical batch script on an NYU HPC cluster looks something like these two examples:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge module load stata/17.0 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do   #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge SRCDIR=$HOME/my_project/code RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR cd $RUNDIR module load fftw/intel/3.3.9 $SRCDIR/my_exec.exe &lt; my_input_params.inp   We'll work through them more closely in a moment. You submit the job with sbatch:  [NetID@log-1 ~]$ sbatch myscript.sh   And monitor it's progress with:  [NetID@log-1 ~]$ squeue -u $USER   What just happened ? Here's an annotated version of the first script:  #!/bin/bash # This line tells the shell how to execute this script, and is unrelated # to SLURM. # at the beginning of the script, lines beginning with &quot;#SBATCH&quot; are read by # SLURM and used to set queueing options. You can comment out a SBATCH # directive with a second leading #, eg: ##SBATCH --nodes=1 # we need 1 node, will launch a maximum of one task and use one cpu for the task: #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 # we expect the job to finish within 5 hours. If it takes longer than 5 # hours, SLURM can kill it: #SBATCH --time=5:00:00 # we expect the job to use no more than 2GB of memory: #SBATCH --mem=2GB # we want the job to be named &quot;myTest&quot; rather than something generated # from the script name. This will affect the name of the job as reported # by squeue: #SBATCH --job-name=myTest # when the job ends, send me an email at this email address. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu # both standard output and standard error are directed to the same file. # It will be placed in the directory I submitted the job from and will # have a name like slurm_12345.out #SBATCH --output=slurm_%j.out # once the first non-comment, non-SBATCH-directive line is encountered, SLURM # stops looking for SBATCH directives. The remainder of the script is executed # as a normal Unix shell script # first we ensure a clean running environment: module purge # and load the module for the software we are using: module load stata/17.0 # next we create a unique directory to run this job in. We will record its # name in the shell variable &quot;RUNDIR&quot;, for better readability. # SLURM sets SLURM_JOB_ID to the job id, ${SLURM_JOB_ID/.*} expands to the job # id up to the first '.' We make the run directory in our area under $SCRATCH, because at NYU HPC # $SCRATCH is configured for the disk space and speed required by HPC jobs. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # we will be reading data in from somewhere, so define that too: DATADIR=$SCRATCH/my_project/data # the script will have started running in $HOME, so we need to move into the # unique directory we just created cd $RUNDIR # now start the Stata job: stata -b do $DATADIR/data_0706.do The second script has the same SBATCH directives, but this time we are using code we compiled ourselves. Starting after the SBATCH directives: # first we ensure a clean running environment: module purge # and ensure we can find the executable: SRCDIR=$HOME/my_project/code # create a unique directory to run this job in, as per the script above RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # By default the script will have started running in the directory we ran sbatch from. # Let's assume our input file is in the same directory in this example. SLURM # sets some environment variables with information about the job, including # SLURM_SUBMIT_DIR which is the directory the job was submitted from. So lets # go there and copy the input file to the run directory on /scratch: cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR # go to the run directory to begin the run: cd $RUNDIR # load whatever environment modules the executable needs: module load fftw/intel/3.3.9 # run the executable (sending the contents of my_input_params.inp to stdin) $SRCDIR/my_exec.exe &lt; my_input_params.inp   ","version":"Next","tagName":"h3"},{"title":"Batch Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-jobs","content":" Jobs are submitted with the sbatch command:  sbatch options job-script   The options tell SLURM information about the job, such as what resources will be needed. These can be specified in the job-script as SBATCH directives, or on the command line as options, or both  note When SBATCH options are provided in both the script and the command line, the command line options take precedence should the two contradict each other.  For each option there is a corresponding SBATCH directive with the syntax:  #SBATCH option   For example, you can specify that a job needs 2 nodes and 4 cores on each node ( by default one CPU core per task ) on each node by adding to the script the directive:  #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4   or as a command-line option to sbatch when you submit the job:  [NetID@log-1 ~]$ sbatch --nodes=2 --ntasks-per-node=4 my_script.sh   ","version":"Next","tagName":"h2"},{"title":"Options to manage job output​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-manage-job-output","content":" -J jobname Give the job a name. The default is the filename of the job script. Within the job, $SLURM_JOB_NAME expands to the job name. -o path/for/stdout Send stdout to path/for/stdout. The default filename is slurm-${SLURM_JOB_ID}.out, e.g. slurm-12345.out, in the directory from which the job was submitted. -e path/for/stderr Send stderr to path/for/stderr. --mail-user=my_email_address@nyu.edu Send mail to my_email_address@nyu.edu when certain events occur. --mail-type=type Valid type values are NONE, BEGIN, END, FAIL, REQUIRE, ALL.  ","version":"Next","tagName":"h3"},{"title":"Options to set the job environment:​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-set-the-job-environment","content":" --export=VAR1,VAR2=&quot;some value&quot;,VAR3 Pass variables to the job, either with a specific value (the VAR= form) or from the submitting environment ( without &quot;=&quot; ) --get-user-env[=timeout][mode] Run something like &quot;su - &lt;username&gt; -c /usr/bin/env&quot; and parse the output. Default timeout is 8 seconds. The mode value can be &quot;S&quot;, or &quot;L&quot; in which case &quot;su&quot; is executed with &quot;-&quot; option.  ","version":"Next","tagName":"h3"},{"title":"Options to request compute resources​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-request-compute-resources","content":" -t, --time=time Set a limit on the total run time. Acceptable formats include &quot;minutes&quot;, &quot;minutes:seconds&quot;, &quot;hours:minutes:seconds&quot;, &quot;days-hours&quot;, &quot;days-hours:minutes&quot; and &quot;days-hours:minutes:seconds&quot;. --mem=MB Maximum memory per node the job will need in MegaBytes --mem-per-cpu=MB Memory required per allocated CPU in MegaBytes -N, --node=num Number of nodes are required. Default is 1 node.-n, --ntasks=numMaximum number tasks will be launched. Default is one task per node.--ntasks-per-node=ntasksRequest that ntasks be invoked on each node.-c, --cpus-per-task=ncpusRequire ncpus number of CPU cores per task. Without this option, allocate one core per task. Requesting the resources you need, as accurately as possible, allows your job to be started at the earliest opportunity as well as helping the system to schedule work efficiently to everyone's benefit.  ","version":"Next","tagName":"h3"},{"title":"Options for running interactively on the compute nodes with srun​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-interactively-on-the-compute-nodes-with-srun","content":" -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task. Don't just submit the job, but also wait for it to start and connect stdout, stderrand stdin to the current terminal. -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  ","version":"Next","tagName":"h3"},{"title":"Options for delaying starting a job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-delaying-starting-a-job","content":" --begin=time Delay starting this job until after the specified date and time, eg. --begin=9:42:00, to start the job at 9:42:00 am -d, --dependency=dependency_list (More info here https://slurm.schedmd.com/sbatch.html)Example 1 --dependency=afterok:12345, to delay starting this job until the job 12345 has completed successfully Example 2 Let us say job 1 uses sbatch file job1.sh, and job 2 uses job2.shInside the batch file of the second job (job2.sh) add#SBATCH --dependency=afterok:$job1Start the first job and get id of the jobjob1=$(echo $(sbatch job1.sh) | grep -Eo &quot;[0-9]+&quot;)Schedule second jobs to start when the first one endssbatch job2.sh  ","version":"Next","tagName":"h3"},{"title":"Options for running many similar jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-many-similar-jobs","content":" -a, --array=indexes Submit an array of jobs with array ids as specified. Array ids can be specified as a numerical range, a comma-separated list of numbers, or as some combination of the two. Each job instance will have an environment variable SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID. For example:--array=1-11, to start an array job with index from 1 to 11--array=1-7:2, to submit an array job with index step size 2--array=1-9%4, to submit an array job with simultaneously running job elements set to 4The srun command is similar to pbsdsh. It launches tasks on allocated resources  ","version":"Next","tagName":"h3"},{"title":"R Job Example​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-job-example","content":" Create a directory and an example R script  [NetID@log-1 ~]$ mkdir /scratch/$USER/examples [NetID@log-1 ~]$ cd /scratch/$USER/examples   Create example.R inside the examples directory:  df &lt;- data.frame(x=c(1,2,3,1), y=c(7,19,2,2)) df indices &lt;- order(df$x) order(df$x) df[indices,] df[rev(order(df$y)),]   Create the following SBATCH script named run-R.sbatch :  #!/bin/bash # #SBATCH --job-name=RTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=2GB #SBATCH --time=01:00:00 module purge module load r/intel/4.0.4 cd /scratch/$USER/examples R --no-save -q -f example.R &gt; example.out 2&gt;&amp;1   Run the job using sbatch.  [NetID@log-1 ~]$ sbatch run-R.sbatch   ","version":"Next","tagName":"h2"},{"title":"Array Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#array-jobs","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both shoulders of users and the scheduler system. Job array can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files.  Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch --array=1-5 run-jobarray.s, you submit 5 jobs to process each of these input files individually.  Prepare the data before submitting an array job:  [NetID@log-1 ~]$ mkdir -p /scratch/$USER/myjarraytest [NetID@log-1 ~]$ cd /scratch/$USER/myjarraytest [NetID@log-1 ~]$ cp /share/apps/Tutorials/slurm/example/jobarray/* . [NetID@log-1 ~]$ ls   Submit the array job:  [NetID@log-1 ~]$ sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submission introduces an environment variable, SLURM_ARRAY_TASK_ID, which is unique for each job array job. It is usually embedded somewhere so that when a job runs, its unique value is incorporated into producing a proper file name.  Also as shown above: two additional options %A and %a, denoting the job ID and the task ID ( i.e. job array index ) respectively, are available for specifying a job's stdout, and stderr file names.  ","version":"Next","tagName":"h2"},{"title":"More examples​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#more-examples","content":" You can find more examples in the slurm jobarray examples directory:  /scratch/work/public/examples/slurm/jobarry/   ","version":"Next","tagName":"h2"},{"title":"GPU Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#gpu-jobs","content":" To request one GPU card, use SBATCH directive in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use eg. --gres=gpu:v100:1. As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  [NetID@log-1 ~]$ mkdir -p /scratch/$USER/myambertest [NetID@log-1 ~]$ cd /scratch/$USER/myambertest [NetID@log-1 ~]$ cp /share/apps/Tutorials/slurm/example/amberGPU/* . [NetID@log-1 ~]$ sbatch run-amber.s   There are three NVIDIA GPU types and one AMD GPU type that can be used.  warning AMD GPUs require code to be compatible with ROCM drivers, not CUDA  To request NVIDIA GPUs  RTX8000  #SBATCH --gres=gpu:rtx8000:1   V100  #SBATCH --gres=gpu:v100:1   A100  #SBATCH --gres=gpu:a100:1   H100  #SBATCH --gres=gpu:h100:1   To request AMD GPUs  MI100  #SBATCH --gres=gpu:mi100:1   MI250  #SBATCH --gres=gpu:mi250:1   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;. The content of the job script &quot;run-amber.s&quot; is:  #!/bin/bash # #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated. Check the one named &quot;mdout&quot;, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla K80 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Interactive Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-jobs","content":" ","version":"Next","tagName":"h2"},{"title":"Bash Sessions​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#bash-sessions","content":" The majority of the jobs on the NYU HPC cluster are submitted with the sbatch command, and executed in the background. These jobs' steps and workflows are predefined by users, and their executions are driven by the scheduler system.  There are cases when users need to run applications interactively ( interactive jobs ). Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface ), providing an experience similar to working on a desktop or laptop.  Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to insights A graphical window to run visualization etc  To support interactive use in a batch environment, Slurm allows for interactive batch jobs.  warning Please do not run interactive jobs on the HPC Login nodes. Login nodes of the HPC cluster are shared between many users so running interactive jobs that require significant computing and IO resources on the login nodes will impact many users. For this reason running compute and IO intensive interactive jobs on the HPC login nodes is not allowed. Such jobs may be removed without notice!  tip Instead of running interactive jobs on Login nodes, users can run interactive jobs on the HPC Compute nodes using SLURM's srun utility. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc. note There is no partition on the HPC cluster that has been reserved for Interactive jobs.  ","version":"Next","tagName":"h3"},{"title":"Start an Interactive Job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#start-an-interactive-job","content":" When you start an interactive batch job the command prompt is not immediately returned. Instead, you wait until the resource is available when the prompt is returned and you are on a compute node and in a batch job - much like the process of logging in to a host with ssh. To end the session, type 'exit', again just like the process of logging in and out with ssh.  [NetID@log-1 ~]$ srun --pty /bin/bash srun: job 58699789 queued and waiting for resources srun: job 58699789 has been allocated resources [NetID@cm034 ~]$ hostname cm034.hpc.nyu.edu   To use any GUI-based program within the interactive batch session you will need to extend X forwarding with the --x11 option. This of course still relies on you having X forwarding at your login session. To test if you have X forwarding running, you can try running the gnuplot test as shown:  [NetID@log-1 ~]$ module load gnuplot/gcc/5.4.1 [NetID@log-1 ~]$ gnuplot gnuplot&gt; test   If a window opens on your display with a gnuplot test window, you know that Xforwarding is working. Please see SSH Tunneling and X11 Forwarding for details.  ","version":"Next","tagName":"h3"},{"title":"Request Resources​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#request-resources","content":" You can request resources for an interactive batch session just as you would for any other job, for example to request 4 processors with 4GB memory for 2 hours.  If you do not request resources you will get the default settings. If after some directory navigation in your interactive session, you can jump back to the directory you submitted from with:  [NetID@cm034 ~]$ cd $SLURM_SUBMIT_DIR   ","version":"Next","tagName":"h3"},{"title":"Interactive Job Options​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-job-options","content":" (Don't just submit the job, but also wait for it to start and connect stdout, stderr and stdin to the current terminal)  -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --gres=gpu:N To request N number of GPUs --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  Certain tasks need user interaction - such as debugging and some GUI-based applications. However the HPC clusters rely on batch job scheduling to efficiently allocate resources. Interactive batch jobs allow these apparently conflicting requirements to be met.  ","version":"Next","tagName":"h3"},{"title":"Interactive Bash Job Examples​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-bash-job-examples","content":" Example (Without x11 forwarding)  Through srun SLURM provides rich command line options for users to request resources from the cluster, to allow interactive jobs. Please see some examples and short accompanying explanations in the code block below, which should cover many of the use cases.  In the srun examples below, through --pty /bin/bash we request to start bash command shell session in pseudo terminal by default the resource allocated is single CPU core and 2GB memory for 1 hour:  [NetID@log-1 ~]$ srun --pty /bin/bash   To request 4 CPU cores, 4 GB memory, and 2 hour running duration:  [NetID@log-1 ~]$ srun -c4 -t2:00:00 --mem=4000 --pty /bin/bash   To request one GPU card, 3 GB memory, and 1.5 hour running duration:  [NetID@log-1 ~]$ srun -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   Example (x11 forwarding)  In srun there is an option &quot;–x11&quot;, which enables X forwarding, so programs using a GUI can be used during an interactive session (provided you have X forwarding to your workstation set up).  To request computing resources, and export x11 display on allocated node(s)  [NetID@log-1 ~]$ srun --x11 -c4 -t2:00:00 --mem=4000 --pty /bin/bash [NetID@cm034 ~]$ module load gnuplot/gcc/5.4.1 [NetID@cm034 ~]$ gnuplot gnuplot&gt; test   To request GPU card etc, and export x11 display:  [NetID@log-1 ~]$ srun --x11 -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   ","version":"Next","tagName":"h3"},{"title":"R interactive job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-interactive-job","content":" The following example shows how to work with Interactive R session on a compute node:  [NetID@log-1 ~]$ srun -c 1 --pty /bin/bash [NetID@cm034 ~]$ module purge [NetID@cm034 ~]$ module list No modules loaded [NetID@cm034 ~]$ module load r/gcc/4.4.0 [NetID@cm034 ~]$ module list Currently Loaded Modules: 1) r/intel/4.4.0 [NetID@cm034 ~]$ R R version 4.4.0 (2024-04-24) -- &quot;Puppy Cup&quot; Copyright (C) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; 6 ** 2 [1] 36 &gt; tan(45) [1] 1.619775 &gt; &gt; q() Save workspace image? [y/n/c]: n [NetID@cm034 ~]$ exit exit [NetID@log-1 ~]$  ","version":"Next","tagName":"h3"},{"title":"Conda Environments (Python, R)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/","content":"","keywords":"","version":"Next"},{"title":"What is Conda?​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#what-is-conda","content":" Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.  Please find more information at the official documentation page  Conda provides a great way to install packages that are already compiled, so you don't need to go over the long compilation process. If a package you need is not available, you can install it (and compile it when needed) using pip (Python) or install.packages (R).  note Reproducibility: One of the ways to ensure the reproducibility of your results is to have an independent conda environment in the directory of each project (one of the options shown below). This will also keep conda environment files away from your /home/$USER directory.  ","version":"Next","tagName":"h2"},{"title":"Advantages/disadvantages of using Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#advantagesdisadvantages-of-using-conda","content":" ","version":"Next","tagName":"h2"},{"title":"Advantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#advantages","content":" A lot of pre-compiled packages (fast and easy to install)  note For Python: pip also offers pre-compiled packages (wheels). List can be found on Python Wheels. However, Conda has a significantly larger number of pre-compiled packages.  Compiled packages use highly efficient Intel Math Kernel Library (MKL) library  ","version":"Next","tagName":"h3"},{"title":"Disadvantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#disadvantages","content":" Conda does not take advantage of packages already installed in the system (while virtualenv and venv do)As you will see below, you may need to do additional steps to keep track of all installed packages (including those installed by pip and/or install.packages)  Automatic deletion of your files This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged. You can read more this in Data Management. tip Thus you can consider the following options: Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - for details see Research Project SpaceUse Singularity and install packages within a corresponding overlay file - for details see Singularity with Conda  ","version":"Next","tagName":"h3"},{"title":"Initializing Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#initializing-conda","content":" Load anaconda module  module purge module load anaconda3/2024.02   Conda init can create problems with package installation, so we suggest using source activate instead of conda activate, even though conda activate is considered a best practice by the Anaconda developers.  There are two main ways to create conda environments. You can either define them with a name by using the -n or --name parameter or you can define them by full path (or prefix) using the -p or --prefix parameter.  tip NYU HPC recommends using prefix environments as a best practice because it makes reproducibility easier as we will see below. However, we do show how to use named environments with your /scratch space, so you don't fill up your /home space, if that's preferred.  ","version":"Next","tagName":"h2"},{"title":"Prefix Conda Environments​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#prefix-conda-environments","content":" You can store your program/project in /scratch and keep the conda environment with it by using the -p parameter. This will keep all the files inside the project's directory, instead of putting in in your /home/$USER. Here are details about how to do this for Python and R projects:  ","version":"Next","tagName":"h2"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#python","content":" 1. Load anaconda module and create local prefix environment​  module purge module load anaconda3/2024.02 conda create -p ./penv python ## environment will be created in project directory source activate ./penv   2. Create a symbolic link so conda will download files for packages to be installed into scratch, not your home directory.​  mkdir /home/&lt;NetID&gt;/.conda mkdir /scratch/&lt;NetID&gt;/conda_pkgs ln -s /scratch/&lt;NetID&gt;/conda_pkgs /home/&lt;NetID&gt;/.conda/pkgs   Install pre-compiled packages available in conda  conda install -c anaconda pandas   3. Other packages may be installed (and compiled when needed) using pip​  pip install &lt;package_name&gt;   note Conda and packages install by default to ~/.local/lib/python&lt;version&gt;, so if you did use 'pip install --user' to install some packages (without conda or other virtual environment), they will be available in ~/.local/lib/python&lt;version&gt;  warning The primary takeaway: Let say you have tornado v.6 installed in ~/.local/lib/python&lt;version&gt;, and tornado v.5 installed by conda install. When you will do source activate you will have tornado v.6 available!! Not v.5!! (this behaviour is the same for packages installed by to ~/.local/lib/python&lt;version&gt; before or after you create your conda environment) pip freeze will give v.6 conda list will give v.5 Solution To overcome this, do export PYTHONNOUSERSITE=True after source activate  ","version":"Next","tagName":"h3"},{"title":"R​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#r","content":" 1. Load anaconda module and create local prefix environment​  module load anaconda3/2024.02 conda create -p ./renv r=3.5 ## environment will be created in project directory ## OR conda create -c conda-forge -p ./renv r-base=3.6.3 ## environment will be created in project directory source activate ./renv   2. Install pre-compiled packages available in conda:​  https://docs.anaconda.com/anaconda/packages/r-language-pkg-docs/  conda install -c r r-dplyr   3. Other packages may be installed (and compiled) using install.packages()​  install.packages(&quot;&lt;package_name&gt;&quot;)   ","version":"Next","tagName":"h3"},{"title":"Named Conda Environments​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#named-conda-environments","content":" You can create a named conda environment with a command like:  conda create --name &lt;my-env&gt;   You'll need to replace &lt;my-env&gt; with the name of your environment.  We defined where the prefix environment is created in the creation command, so where is the named environment created for named conda environments?  We can find this out by using the info command with conda:  $ conda info   This should output something like:   active environment : None user config file : /home/NetID/.condarc populated config files : conda version : 24.1.2 conda-build version : 24.1.2 python version : 3.11.7.final.0 solver : libmamba (default) virtual packages : __archspec=1=cascadelake __conda=24.1.2=0 __glibc=2.34=0 __linux=5.14.0=0 __unix=0=0 base environment : /share/apps/anaconda3/2024.02 (read only) conda av data dir : /share/apps/anaconda3/2024.02/etc/conda conda av metadata url : None channel URLs : https://repo.anaconda.com/pkgs/main/linux-64 https://repo.anaconda.com/pkgs/main/noarch https://repo.anaconda.com/pkgs/r/linux-64 https://repo.anaconda.com/pkgs/r/noarch package cache : /share/apps/anaconda3/2024.02/pkgs /home/NetID/.conda/pkgs envs directories : /home/NetID/.conda/envs /share/apps/anaconda3/2024.02/envs platform : linux-64 user-agent : conda/24.1.2 requests/2.31.0 CPython/3.11.7 Linux/5.14.0-284.86.1.el9_2.x86_64 rhel/9.2 glibc/2.34 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.6 aau/0.4.3 c/U4Vmh2Rw4idTcwqzLhNX1g s/2MYchdSIR4EFcYeOthelfQ UID:GID : 402570:402570 netrc file : None offline mode : False   You can see in this output the default location for your package cache and your envs directories. They're currently set to subdirectories of our home directory. If we create named environments with these setting we will very quickly fill up our limited home directory space. The solution to this is to create or modify our personal conda config file to change these settings. You can see the setting above for user config file tells us the location of the file where we can make changes to these defaults. If the file doesn't exist you'll need to create it. You may also need to change the config file location if your output from conda info differs from the above.  $ nano /home/NetID/.condarc   Now add the following lines to your config file:  envs_dirs: - /scratch/NetID/conda_envs pkgs_dirs: - /scratch/NetID/conda_pkgs always_copy: true   You'll need to create those directories if they don't exist:  $ mkdir /scratch/NetID/conda_envs $ mkdir /scratch/NetID/conda_pkgs   Don't forget to change NetID above to your NetID.  You should now see that the package cache and envs directories entries have changed to your /scratch space when you run conda info and packages and environments will be saved on your /scratch space for all named conda environments.  ","version":"Next","tagName":"h2"},{"title":"Reproducibility​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#reproducibility","content":" ","version":"Next","tagName":"h2"},{"title":"Packages installed only using conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#packages-installed-only-using-conda","content":" Save a list of packages (so you are able to report the environment in a publication, and to restore/reproduce the environment on another machine at any time)  # save conda list --export &gt; requirements.txt # restore conda create -p ./penv --file requirements.txt   note This will not list packages installed by pip or install.packages()  ","version":"Next","tagName":"h3"},{"title":"Packages installed using conda and pip (Python)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#packages-installed-using-conda-and-pip-python","content":" In this case you can use:  export PYTHONNOUSERSITE=True ## to ignore packages in ~/.local/lib/python&lt;version&gt; # save conda list --export &gt; conda_requirements.txt pip freeze &gt; pip_requirements.txt # restore conda create -p ./penv --file conda_requirements.txt pip install -r pip_requirements.txt   tip Alternatively, you can use conda env export &gt; all_requirements.txt, which will save both: packages installed by conda and pip. warning However, this may fail if your conda environment is created as a sub-directory of your project's directory (which we recommend)  ","version":"Next","tagName":"h3"},{"title":"Packages installed using install.packages (R)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#packages-installed-using-installpackages-r","content":" The command conda list --export will not include packages installed by install.packages. So, only use conda install to install R and use renv to maintain information about packages installed by install.packages.  renv​  Please see details of using renv with conda for reproducibilty on R packages with renv.  ","version":"Next","tagName":"h3"},{"title":"Use conda env in a batch script​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#use-conda-env-in-a-batch-script","content":" The part of the batch script that will call the command should look like:  ","version":"Next","tagName":"h2"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#python-1","content":" Single node​  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2024.02; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; export PATH_TO_ENV=&lt;full path to penv&gt; source activate $PATH_TO_ENV; export PATH=$PATH_TO_ENV/bin:$PATH; python test.py   You'll need to replace &lt;full path to penv&gt; with the full path to your penv directory. It's probably something like /scratch/NetID/conda_tests/penv  Multiple nodes, using MPI​  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 export PATH_TO_ENV=&quot;&lt;full path to penv&gt;&quot; module purge; module load openmpi/gcc/4.1.6; mpiexec bash -c &quot;module purge; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2024.02; source /share/apps/anaconda3/2024.02/etc/profile.d/conda.sh; source activate &quot;$PATH_TO_ENV&quot;; export PATH=&quot;$PATH_TO_ENV/bin:$PATH&quot;; python test.py&quot;   Again, you'll need to replace &lt;full path to penv&gt; above with the full path to your penv directory.  ","version":"Next","tagName":"h3"},{"title":"R (conda packages only)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#r-conda-packages-only","content":" #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2024.02; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2024.02/etc/profile.d/conda.sh; export PATH_TO_ENV=&lt;full path to renv&gt;; source activate $PATH_TO_ENV; export PATH=$PATH_TO_ENV/bin:$PATH; Rscript r_script.R   You'll again need to replace &lt;full path to renv&gt; above with the full path to your renv directory.  Multiple nodes, using MPI​  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=2 #SBATCH --cpus-per-task=2 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2024.02; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2024.02/etc/profile.d/conda.sh; export PATH_TO_ENV=&quot;&lt;full path to renv&gt;&quot;; source activate $PATH_TO_ENV; export PATH=$PATH_TO_ENV/bin:$PATH; Rscript test.R   You'll again need to replace &lt;full path to renv&gt; above with the full path to your renv directory.  ","version":"Next","tagName":"h3"},{"title":"R (conda with renv combination)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/conda_environments/#r-conda-with-renv-combination","content":" In this case, when you use sbatch you would activate conda in sbatch script, and R script will pickup packages installed in renv  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=2 #SBATCH --cpus-per-task=2 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00module purge module load anaconda3/2024.02 source /share/apps/anaconda3/2024.02/etc/profile.d/conda.sh source activate &lt;full path to renv&gt; Rscript test.R   You'll again need to replace &lt;full path to renv&gt; above with the full path to your renv directory. ","version":"Next","tagName":"h3"},{"title":"Python Packages with Virtual Environments","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/","content":"","keywords":"","version":"Next"},{"title":"Create project directory and load Python module​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-project-directory-and-load-python-module","content":" ## Find python version you need module avail python ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## load python module (different versions available) module load python/intel/3.8.6   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (see HPC Storage for details).  Thus you can consider the following options  Reinstall your packages if some of the files get deleted You can do this manuallyYou can also do this automatically. For example, within a workflow of pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - see details on the Research Project Space page.Use Singularity and install packages within a corresponding overlay file - See instructions on our Singularity with Conda page.  ","version":"Next","tagName":"h2"},{"title":"Create virtual environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-virtual-environment","content":" It is advisable to create private environment inside the project directory. This boosts reproducibility and does not use space in /home/$USER  ","version":"Next","tagName":"h2"},{"title":"virtualenv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#virtualenv","content":" virtualenv is a tool to create isolated Python environments  Since Python 3.3, a subset of it has been integrated into the standard library under the venv module.  You can create new virtual environment in two ways:  emptyinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## Create an EMPTY virtual environment virtualenv venv ## Create an virtual environment that inherits system packages virtualenv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"venv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#venv","content":" venv is package shipped with Python. It provides subset of options available in virtualenv tool (link).  python -m venv venv   Create new virtual environment in current directory  emptyinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ##EMPTY ## (use venv command to create environment called &quot;venv&quot;) python -m venv venv ## Inhering all packages python -m venv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"Install packages. Keep things reproducible​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#install-packages-keep-things-reproducible","content":" ## activate source venv/bin/activate ## install packages pip install &lt;package you need&gt; ## If package was inherited, but you want to install it in your own env anyway pip install &lt;package you need&gt; --ignore-installed ## export list of packages (to report together with paper and/or to reproduce environment on another computer) pip freeze &gt; requirements.txt ## restore pip install -r requirements.txt   ","version":"Next","tagName":"h2"},{"title":"Close an Activated Virtual Environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#close-an-activated-virtual-environment","content":" If you have activated a virtual environment, you can exit it with the following command:  deactivate   ","version":"Next","tagName":"h2"},{"title":"Use with sbatch​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#use-with-sbatch","content":" When you use this env in sbatch script, please use  module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py   If you use mpi  mpiexec bash -c &quot;module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py&quot;  ","version":"Next","tagName":"h2"},{"title":"R Packages with renv","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/","content":"","keywords":"","version":"Next"},{"title":"Is this needed:​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#is-this-needed","content":" tip If you would like to use conda with renv you need to add following steps: After you activate conda AND before loading R export R_RENV_DEFAULT_LIBPATHS=&lt;path_to_project_directory&gt;/renv/lib/x86_64-conda_cos6-linux-gnu/&lt;version&gt;/ Start R and execute .libPaths(c(.libPaths(), Sys.getenv(&quot;R_RENV_SYSTEM_LIBRARY&quot;)))   You may use the renv R package to create a personal R Project environment for R packages. Documentation on renv can be found on the RStudio site.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#setup","content":" Say your R code is in directory /scratch/$USER/projects/project1  cd /scratch/$USER/projects/project1 module purge module load r/gcc/4.5.0 R   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged. Please see details at HPC Storage.  Thus you can consider the following options:  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - Details available at Research Project SpaceUse Singularity and install packages within a corresponding overlay file - Details available at Squash File System and Singularity  ","version":"Next","tagName":"h3"},{"title":"Cache directory setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#cache-directory-setup","content":" By default, renv will cache package installation files to your home directory (most likely either in ~/.local/share/renv or ~/.cache/R/renv/ or something similar).  To avoid filling up your home directory, we advise to set up path to alternative cache directory (otherwise your home directory may fill up quickly)  Create directory  mkdir -p /scratch/$USER/.cache/R/renv   Create a file in you project directory named .Renviron and put the following in in the file. It is the R project directory (/scratch/$USER/projects/project1) in this example.  RENV_PATHS_ROOT=/scratch/&lt;USER_NETID&gt;/.cache/R/renv   ","version":"Next","tagName":"h3"},{"title":"Init renv​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#init-renv","content":" The renv package is already installed for module r/gcc/4.5.0. You need to install it yourself if you use other R module version  ## Do this if renv is not available (already installed for r/gcc/4.4.0) # install.packages(&quot;renv&quot;) ## By default this will install renv package into a sub-directory within your home directory ## init renv in project's directory renv::init(&quot;.&quot;)   Restart R for renv to take effect. Once you start R, your renv environment will be loaded automatically.  R version 4.5.0 (2025-04-11) -- &quot;How About a Twenty-Six&quot; ... * Project '/scratch/$USER/projects/project1' loaded. [renv 1.1.4]   ","version":"Next","tagName":"h3"},{"title":"Check​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#check","content":" You can check your library paths with the .libPaths() command  &gt; .libPaths() [1] &quot;/scratch/$USER/projects/project1/renv/library/R-4.1/x86_64-pc-linux-gnu&quot;   You can check where the cache is set with the following:  renv::paths$cache() #[1] &quot;/home/$USER/.cache/R/renv/cache/v5/R-4.1/x86_64-pc-linux-gnu&quot;   ","version":"Next","tagName":"h3"},{"title":"Add/remove, etc. packages​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#addremove-etc-packages","content":" Install a package, such as reshape2. Below we can see it is not yet installed and then install it.  R library(reshape2) Error in library(reshape2) : there is no package called ‘reshape2’ install.packages(&quot;reshape2&quot;)   note You must be in the project1 directory for renv to load your project and the appropriate personal environment that you have created. If you want to copy your environment to a new location, use the bundle package, as shown below.  Test R file  print(&quot;hello&quot;) renv::restore() library(reshape2) names(airquality) &lt;- tolower(names(airquality)) head(airquality) aql &lt;- melt(airquality) print(&quot;hello again&quot;)   For testing run it as  srun --pty /bin/bash Rscript test.R   note Your .Rprofile file will include line source(&quot;renv/activate.R&quot;)  The file will output the following:  [1] &quot;hello&quot; * The library is already synchronized with the lockfile. ozone solar.r wind temp month day 1 41 190 7.4 67 5 1 2 36 118 8.0 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 No id variables; using all as measure variables [1] &quot;hello again&quot;   ","version":"Next","tagName":"h3"},{"title":"Clean up​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#clean-up","content":" Keep only the packages that you use in this particular project (not all the packages available on the system)  R # launch R renv::clean() # remove packages not recorded in the lockfile from the target library   ","version":"Next","tagName":"h3"},{"title":"Recommended Workflow​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#recommended-workflow","content":" The general workflow when working with renv is:  Call renv::init() to initialize a new project-local environment with a private R library,Work in the project as normal, installing and removing new R packages as they are needed in the project,Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock), By default, renv::snapshot() will only capture packages listed in your R scripts within the R Project. For more options read the renv::snapshot() documentation. Continue working on your project, installing and updating R packages as needed.If needed, call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.  The renv::init() function attempts to ensure the newly-created project library includes all R packages currently used by the project. It does this by crawling R files within the project for dependencies with the renv::dependencies() function. The discovered packages are then installed into the project library with the renv::hydrate() function, which will also attempt to save time by copying packages from your user library (rather than reinstalling from CRAN) as appropriate.  Calling renv::init() will also write out the infrastructure necessary to automatically load and use the private library for new R sessions launched from the project root directory. This is accomplished by creating (or amending) a project-local .Rprofile with the necessary code to load the project when the R session is started.  If you’d like to initialize a project without attempting dependency discovery and installation – that is, you’d prefer to manually install the packages your project requires on your own – you can use renv::init(bare = TRUE) to initialize a project with an empty project library.  ","version":"Next","tagName":"h3"},{"title":"Use with sbatch​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#use-with-sbatch","content":" When you launch a job with sbatch, R will check if there is an renv directory, and if renv is on it will pick up packages, installed using renv in the current directory.  Before you launch an sbatch job, you need to make sure your project renv environment is ready, as outlined in the previous section.  ","version":"Next","tagName":"h2"},{"title":"Store and Share your R Project's R version and R Package Versions​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#store-and-share-your-r-projects-r-version-and-r-package-versions","content":" ","version":"Next","tagName":"h2"},{"title":"Reproduce Environment​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#reproduce-environment","content":" If you already have file renv.lock or bundle file skip step 1  In the original location (your own laptop for example) go to project directory and execute  warning Make sure the whole path to project directory and names of your script files don't have empty spaces!  R # install.packages(&quot;renv&quot;) ## if needed renv::init() renv::snapshot()   Take file renv.lock and copy it to a new location for the projectAt the new location - restore environment: go to directory of the project and execute.  warning Make sure version of R is the same  ## Reproduce environment module purge module load r/gcc/4.5.0 R renv::restore() renv::init()   renv will install/compile what is needed on any system (Linux, Windows, etc). You can share your code with other researchers no matter what system they use. However, you should be careful that the same version of R is used between systems, as mentioned above.  ","version":"Next","tagName":"h3"},{"title":"What to save/publish/commit with Git​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#what-to-savepublishcommit-with-git","content":" In order to have your work reproducible by you and/or others:  save and/or commit your code in gitbe sure to include renv.lock (which lists all packages and versions that you use including the version of R)  ","version":"Next","tagName":"h3"},{"title":"Migrating from Packrat​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#migrating-from-packrat","content":" The renv package has replaced the now deprecated Packrat package. The renv::migrate() function makes it possible to migrate projects from Packrat to renv. See the ?migrate documentation for more details. In essence, calling renv::migrate(&quot;&lt;project path&gt;&quot;) will be enough to migrate the Packrat library and lockfile such that they can then be used by renv.  ","version":"Next","tagName":"h3"},{"title":"Useful links​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/r_packages_with_renv/#useful-links","content":" https://rstudio.github.io/renv/articles/renv.html ","version":"Next","tagName":"h2"},{"title":"Software on Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/","content":"","keywords":"","version":"Next"},{"title":"Software Overview​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#software-overview","content":" There are different types of software packages available  Use module avail command to see preinstalled software. This includes the licensed software listed below Singularity Containers You can find those already built and ready to use, at location /scratch/work/public/singularity/For more information on running software with Singularity, See our Containers Intro. Python/R/Julia packages can be installed by a user  If you need another linux program installed, please contact us at hpc@nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the module made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  Command\tFunctionmodule unload &lt;module-name&gt;\tunload a module module show &lt;module-name&gt;\tsee exactly what effect loading the module will have module purge\tremove all loaded modules from your environment module load &lt;module-name&gt;\tload a module module whatis &lt;module-name&gt;\tfind out more about a software package module list\tcheck which modules are currently loaded in your environment module avail\tcheck what software packages are available module help &lt;module-name&gt;\tA module file may include more detailed help for the software package  ","version":"Next","tagName":"h2"},{"title":"Package Management for R, Python, & Julia, and Conda in general​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#package-management-for-r-python--julia-and-conda-in-general","content":" Conda environments (Python, R)Using virtual environments for PythonManaging R packages with renv  ","version":"Next","tagName":"h2"},{"title":"Examples of software usage on Greene​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#examples-of-software-usage-on-greene","content":" Examples can be found under /scratch/work/public/examples/ and include the following   alphafold\tknitro\tSingularity amd GPUs\tlammps\tslurm comsol\tmatlab\tspark c-sharp\tmathematica\tstata crystal17\tnamd\tsquashfs fluent\torca\ttrinity gaussian\tquantum-espresso\tvnc hadoop-streaming\tR\tvscode julia\tsas\txvfb jupyter notebooks\tschrodinger\t  ","version":"Next","tagName":"h2"},{"title":"Accessing Datasets with Singularity​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#accessing-datasets-with-singularity","content":" Singularity for Datasets  ","version":"Next","tagName":"h2"},{"title":"Licensed Software​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#licensed-software","content":" ","version":"Next","tagName":"h2"},{"title":"SCHRODINGER​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#schrodinger","content":" Schrödinger provides a complete suite of software solutions with the latest advances in pharmaceutical research and computational chemistry. The NYU New York campus has a limited number of licenses for the Biologics Suite (ConfGen, Epik, Jaguar, Jaguar pKa, MacroModel, Prime, QSite, SiteMap), BioLuminate and the Basic Docking Suite.  note Schrödinger can be used for non-commercial, academic purposes ONLY.  Using SCHRODINGER on HPC Cluster​  To load Schrodinger module execute  $ module load schrodinger/2024.4   Using SCHRODINGER on NYU Lab Computers​  Request your account at: https://www.schrodinger.com/request-accountDownload the software at: https://www.schrodinger.com/downloads/releasesContact NYU-HPC team to request your license file.  These license servers are accessible from NYU subnet.  Please see the following links for installation of the license file:  https://www.schrodinger.com/kb/377238https://www.schrodinger.com/license-installation-instructions  To check licenses status  # module load schrodinger/2021-1 # load schrodinger if not already loaded # licadmin STAT # licutil -jobs ## For example: [wang@cs001 ~]$ licutil -jobs ######## Server /share/apps/schrodinger/schrodinger.lic Product &amp; job type Jobs BIOLUMINATE 10 BIOLUMINATE, Docking 1 BIOLUMINATE, Shared 10 CANVAS 50 COMBIGLIDE, Grid Generation 11 COMBIGLIDE, Library Generation 50 COMBIGLIDE, Protein Prep 11 COMBIGLIDE, Reagent Prep 1 EPIK 11 GLIDE, Grid Generation 11 GLIDE, Protein Prep 11 GLIDE, SP Docking 1 GLIDE, XP Descriptors 1 GLIDE, XP Docking 1 IMPACT 11 JAGUAR 5 JAGUAR, PKA 5 KNIME 50 LIGPREP, Desalter 1 LIGPREP, Ionizer 3511 LIGPREP, Ligparse 1 LIGPREP, Neutralizer 1 LIGPREP, Premin Bmin 1 LIGPREP, Ring Conf 1 LIGPREP, Stereoizer 1 LIGPREP, Tautomerizer 1 MACROMODEL 5 MACROMODEL, Autoref 5 MACROMODEL, Confgen 5 MACROMODEL, Csearch Mbae 5 MAESTRO, Unix 1000 MMLIBS 3511 PHASE, CL Phasedb Confsites 1 PHASE, CL Phasedb Convert 1 PHASE, CL Phasedb Manage 1 PHASE, DPM Ligprep Clean Structures 1 PHASE, DPM Ligprep Generate Conformers 5 PHASE, MD Create sites 1 PRIME, CM Build Membrane 2 PRIME, CM Build Structure 2 PRIME, CM Edit Alignment 2 PRIME, CM Struct Align 18 PRIME, Threading Search 2 QSITE 5 SITEMAP 10   Schrodinger Example Files​  Example SBATCH jobs and outputs are available to review here:  /scratch/work/public/examples/schrodinger/   ","version":"Next","tagName":"h3"},{"title":"COMSOL​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#comsol","content":" COMSOL is a problem-solving simulation environment, enforcing compatibility guarantees consistent multiphysics models. COMSOL Multiphysics is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. The package is cross-platform (Windows, Mac, Linux). The COMSOL Desktop helps you organize your simulation by presenting a clear overview of your model at any point. It uses functional form, structure, and aesthetics as the means to achieve simplicity for modeling complex realities.  note This license is for academic use only with Floating Network Licensing in nature i.e., authorized users are allowed to use the software on desktops. Please contact hpc@nyu.edu for the license. However, COMSOL is also available on NYU HPC cluster Greene.  In order to check what Comsol licenses are available on Greene use comsol_licenses command in your terminal session.  Several versions of COMSOL are available on the HPC cluster. To use COMSOL on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load comsol/6.3   To submit a COMSOL job in a parallel fashion, running on multiple processing cores, follow the steps below:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory  cp /scratch/work/public/examples/comsol/run-comsol.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/comsol/test-input.mph /scratch/&lt;net_id&gt;/example/   Edit the slurm batch script file (run-comsol.sbatch) to match your case (for example chance location of the run directory).Once the slurm batch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file for detail output information.  sbatch run-comsol.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATHEMATICA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#mathematica","content":" Mathematica is a general computing environment with organizing algorithmic, visualization, and user interface capabilities. The many mathematical algorithms included in Mathematica make computation easy and fast.  To run Mathematica on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load mathematica/14.1.0   note In the example below the module is loaded already in the sbatch script.  To submit a batch Mathematica job for running in a parallel mode on multiple processing cores, follow below steps:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/mathematica/basic/example.m /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/mathematica/basic/run-mathematica.sbatch /scratch/&lt;net_id&gt;/example   Edit the slurm batch script file (run-mathematica.sbatch) to match your case (for example chance location of the run directory).Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-mathematica.sbatch   ","version":"Next","tagName":"h3"},{"title":"SAS​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#sas","content":" SAS is a software package which enables programmers to perform many tasks, including:  Information retrievalData managementReport writing &amp; graphicsStatistical analysis and data miningBusiness planningForecasting and decision supportOperations research and project managementQuality improvementApplications developmentData warehousing (extract, transform, load)Platform independent and remote computing.  There are licenses for 2 CPUs on the HPC Cluster.  Running a parallel SAS job on HPC cluster (Greene):​  To submit a SAS job for running on multiple processing elements, follow below steps:  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/sas/test.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/test2.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/run-sas.sbatch /scratch/&lt;net_id&gt;/example/   Submit as shown below. After successful completion of job, verify output log file generated.  sbatch run-sas.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATLAB​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#matlab","content":" MATLAB is a technical computing environment for high performance numeric computation and visualization. MATLAB integrates numerical analysis, matrix computation, signal processing, and graphics in an easy to use environment without using traditional programming.  MATLAB on personal computers and laptops​  NYU has a Total Academic Headcount (TAH) license which provides campus-wide access to MATLAB, Simulink, and a variety of add-on products. All faculty, researchers, and students (on any NYU campus) can use MATLAB on their personal computers and laptops and may go to the following site to download the NYU site license software free of charge.  https://www.mathworks.com/academia/tah-portal/new-york-university-618777.html  MATLAB can be used for non-commercial, academic purposes.  There are several versions of Matlab available on the cluster and the relevant version can be loaded.  module load matlab/2024b   In order to run MATLAB interactively on the cluster, start an interactive slurm job, load the matlab module and launch an interactive matlab session in the terminal.  Mathworks has provided a Greene Matlab User Guide that presents useful tips and practices for using Matlab on the cluster.  ","version":"Next","tagName":"h3"},{"title":"STATA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#stata","content":" Stata is a command and menu-driven software package for statistical analysis. It is available for Windows, Mac, and Linux operating systems. Most of its users work in research. Stata's capabilities include data management, statistical analysis, graphics, simulations, regression and custom programming.  Running a parallel STATA job on HPC cluster (Greene):​  To submit a STATA job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/stata/run-stata.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/stata/stata-test.do /scratch/&lt;net_id&gt;/example/   Submit using sbatch. After successful completion of job, verify output log file generated.  sbatch run-stata.sbatch   ","version":"Next","tagName":"h3"},{"title":"GAUSSIAN​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#gaussian","content":" Gaussian uses basic quantum mechanic electronic structure programs. This software is capable of handling proteins and large molecules using semi-empirical, ab initio molecular orbital (MO), density functional, and molecular mechanics calculations.  The NYU Gaussian license only covers PIs at the Washington Square Park campus. We will grant access to you after verifying your WSP affiliation. For access, please email hpc@nyu.edu.  Running a parallel Gaussian job on HPC cluster (Greene):​  To submit a Gaussian job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example #Copy example files to your newly created directory. cp /scratch/work/public/examples/gaussian/basic/test435.com /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/gaussian/basic/run-gaussian.sbatch /scratch/&lt;net_id&gt;/example/   Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-gaussian.sbatch   ","version":"Next","tagName":"h3"},{"title":"Knitro​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/software_on_greene/#knitro","content":" Knitro is a commercial software package for solving large scale mathematical optimization problems. Knitro is specialized for nonlinear optimization, but also solves linear programming problems, quadratic programming problems, systems of nonlinear equations, and problems with equilibrium constraints. The unknowns in these problems must be continuous variables in continuous functions; however, functions can be convex or nonconvex. Knitro computes a numerical solution to the problem—it does not find a symbolic mathematical solution. Knitro versions 9.0.1 and 10.1.1 are available.  Running a parallel Knitro job on HPC cluster (Greene):​  To submit a Knitro job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/knitro/knitro.py /scratch/&lt;net_id&gt;/example/   There is no sample sbatch script available for knitro.After creating your own sbatch script you can execute it as follows:  sbatch &lt;script&gt;.sbatch  ","version":"Next","tagName":"h3"},{"title":"SQLite: Handling Large Structured Data","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#overview","content":" Storing your data in the SQLite format allows you to get benefits of a database, and at the same time the simplicity of storage of data in a file on a disk.  SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. The SQLite file format is stable, cross-platform, and backwards compatible and the developers pledge to keep it that way through at least the year 2050. -- SQLite website:  ","version":"Next","tagName":"h2"},{"title":"Some use-cases​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#some-use-cases","content":" You think you need MySQL, PostreSQL, etc for your ML project. Usually you don'tYou have to deal with hundreds of GB of table-structured data (or larger) and your script (for whatever reason) can't be made parallel.You would request a lot of RAM and work with data slowly.  warning This would be a waste of RAM.  tip It is better in this case to request smaller amount of RAM and read data (efficiently) from disk - for example using SQLite  ","version":"Next","tagName":"h3"},{"title":"Benefits:​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#benefits","content":" You are not limited by RAM any longerCompared to other file formats SQLite is very good in selecting certain lines (especially if you use indexing)You can use familiar dplyr syntax or execute SQL queries directly dplyr is an interface for working with data in a database, not for modifying remote tables.DBI package allows to both read and modify tables SQLite is actually faster for common data analysis tasks than other popular databases.You can have multiple threads accessing an SQLite database simultaneously (for read operations. Writing is more tricky)Merging/Joining datasets on disk  ","version":"Next","tagName":"h3"},{"title":"Major benefits of SQLite compared to MySQL (PostgreSQL, etc)​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#major-benefits-of-sqlite-compared-to-mysql-postgresql-etc","content":" You control your own data (sqlite file). You don't depend on any service like MySQLYou can copy a file to your own laptop and work with itAgain, SQLite is faster!  ","version":"Next","tagName":"h3"},{"title":"Limits​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#limits","content":" SQLite has some limitations in terms of concurrency, which usually don't apply for typical ML/AI jobs.See Four Different Ways To Handle SQLite Concurrency for more information.  ","version":"Next","tagName":"h3"},{"title":"Command line (CLI) example​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#command-line-cli-example","content":" Create environment  $ mkdir projects/sqlite-test $ cd projects/sqlite-test $ conda create -p ./cenv $ source activate ./cenv $ conda install -y sqlite   Then follow this SQLite example.  $ sqlite3 db_file.sqlite sqlite&gt; create table tbl1(one varchar(10), two smallint); sqlite&gt; insert into tbl1 values('hello!', 10); sqlite&gt; insert into tbl1 values('goodbye', 20); sqlite&gt; select * from tbl1; hello!|10 goodbye|20   Now Close session (Ctrl-D).  Reopen session to check if changes are saved  $ sqlite3 db_file.sqlite sqlite&gt; select * from tbl1; hello!|10 goodbye|20   ","version":"Next","tagName":"h2"},{"title":"R example​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#r-example","content":" ","version":"Next","tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#install","content":" Here we use conda, as a great way to keep everything isolated and reproducible.  note conda will install pre-compiled packages. Which is good (faster) and bad (not fully optimized for a specific hardware)  tip Alternative: install packages to a local directory or use renv as described in R Packages with renv mkdir /scratch/$USER/projects/myTempProject cd /scratch/$USER/projects/myTempProject module load anaconda3/2024.02 conda create -p ./cenv -c conda-forge r=4.5 source activate ./cenv conda install -c r r-rsqlite conda install -c r r-tidyverse conda install -c conda-forge r-remotes conda install -c r r-feather conda install -c r r-nycflights13 note window functions (row_number in particular) require newer version of rsqlite R remotes::install_github(&quot;r-dbi/RSQLite&quot;) ## update ALL   tip Save list of installed packages for reproducibility ## conda list --export &gt; requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Use​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#use","content":" Many examples can be found here:  SQL syntaxdplyr syntax  library(tidyverse) library(DBI) # Create RSQLite database file with name &quot;allData&quot; con &lt;- dbConnect(RSQLite::SQLite(), &quot;allData&quot;)   Copy data frame to database (dplyr)  copy_to(con, nycflights13::flights, &quot;fl&quot;, temporary=FALSE)   Or copy data to database using DBI  dbCreateTable(con, &quot;fl&quot;, nycflights13::flights, temporary = FALSE) dbAppendTable(con, &quot;fl&quot;, nycflights13::flights)   Connect to a specific table  dbListTables(con) df_con &lt;- tbl(con, &quot;fl&quot;) ## check number of rows df_con %&gt;% count()   Subset  df_temp &lt;- df_con %&gt;% filter( row_number() %in% c(1, 3) ) %&gt;% collect   Save as feather  feather::write_feather(df_temp, &quot;my_data.feather&quot;)   ","version":"Next","tagName":"h3"},{"title":"Alternative: read csv file to SQLite directly​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#alternative-read-csv-file-to-sqlite-directly","content":" If you already have a large csv file on disk, and you don't want to read it to RAM, you can read it to SQLite file directly  conda install -c conda-forge r-sqldf R library(sqldf) ## create data file # sqldf(&quot;attach allData as new&quot;) # make csv file for this example write.csv(df_con, &quot;df_con.csv&quot;, row.names = FALSE) # read file directly from csv to sqlite read.csv.sql(file = &quot;df_con.csv&quot;, sql = &quot;create table states_data as select * from file&quot;, dbname = &quot;allData&quot;) # verify data in data frame dbListTables(con) [1] &quot;fl&quot; &quot;sqlite_stat1&quot; &quot;sqlite_stat4&quot; &quot;states_data&quot; df_con_sd &lt;- tbl(con, &quot;states_data&quot;) df_con_sd # Source: table&lt;`states_data`&gt; [?? x 19] # Database: sqlite 3.50.1 [/scratch/netID/myTempProject/allData] year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 7 2013 1 1 555 600 -5 913 854 8 2013 1 1 557 600 -3 709 723 9 2013 1 1 557 600 -3 838 846 10 2013 1 1 558 600 -2 753 745 # ℹ more rows # ℹ 11 more variables: arr_delay &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;, # hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;int&gt; # ℹ Use `print(n = ...)` to see more rows   ","version":"Next","tagName":"h3"},{"title":"UI for SQLite - SQLiteStudio​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#ui-for-sqlite---sqlitestudio","content":" Once you have SQLite file, you can easily transfer it to your own laptop and explore it using SQLiteStudio, if you like to use UI instead of terminal ","version":"Next","tagName":"h2"},{"title":"Environment Variables","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/environment_variables/","content":"","keywords":"","version":"Next"},{"title":"Showing the Value of a Variable​","type":1,"pageTitle":"Environment Variables","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/environment_variables/#showing-the-value-of-a-variable","content":" Let’s show the value of the variable HOME:  $ echo HOME HOME   That just prints “HOME”, which isn’t what we wanted (though it is what we actually asked for). Let’s try this instead:  $ echo $HOME /home/NetID   The dollar sign tells the shell that we want the value of the variable rather than its name. This works just like wildcards: the shell does the replacement before running the program we’ve asked for. Thanks to this expansion, what we actually run is echo /home/NetID, which displays the right thing.  ","version":"Next","tagName":"h2"},{"title":"Creating and Changing Variables​","type":1,"pageTitle":"Environment Variables","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/environment_variables/#creating-and-changing-variables","content":" Creating a variable is easy — we just assign a value to a name using “=” (we just have to remember that the syntax requires that there are no spaces around the =!):  $ SECRET_IDENTITY=Dracula $ echo $SECRET_IDENTITY Dracula   To change the value, just assign a new one:  $ SECRET_IDENTITY=Camilla $ echo $SECRET_IDENTITY Camilla   ","version":"Next","tagName":"h2"},{"title":"Environment variables​","type":1,"pageTitle":"Environment Variables","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/environment_variables/#environment-variables-1","content":" When we ran the printenv command we saw there were a lot of variables whose names were in upper case. That’s because, by convention, variables that are also available to use by other programs are given upper-case names. Such variables are called environment variables as they are shell variables that are defined for the current shell and are inherited by any child shells or processes.  To create an environment variable you need to export a shell variable. For example, to make our SECRET_IDENTITY available to other programs that we call from our shell we can do:  $ SECRET_IDENTITY=Camilla $ export SECRET_IDENTITY   You can also create and export the variable in a single step:  $ export SECRET_IDENTITY=Camilla   Using environment variables to change program behaviour Set a shell variable TIME_STYLE to have a value of iso and check this value using the echo command. Now, run the command ls with the option -l (which gives a long format). export the variable and rerun the ls -l command. Do you notice any difference? [Click for Solution] Solution The TIME_STYLE variable is not seen by ls until is exported, at which point it is used by ls to decide what date format to use when presenting the timestamp of files.  You can see the complete set of environment variables in your current shell session with the command env (which returns a subset of what the command set gave us). The complete set of environment variables is called your runtime environment and can affect the behaviour of the programs you run.  Job environment variables When Slurm runs a job, it sets a number of environment variables for the job. One of these will let us check what directory our job script was submitted from. The SLURM_SUBMIT_DIR variable is set to the directory from which our job was submitted. Using the SLURM_SUBMIT_DIR variable, modify your job so that it prints out the location from which the job was submitted. [Click for Solution] Solution [NetID@log-1 ~]$ nano example-job.sh [NetID@log-1 ~]$ cat example-job.sh #!/bin/bash #SBATCH -t 00:00:30 echo -n &quot;This script is running on &quot; hostname echo &quot;This job was launched in the following directory:&quot; echo ${SLURM_SUBMIT_DIR}   To remove a variable or environment variable you can use the unset command, for example:  $ unset SECRET_IDENTITY   ","version":"Next","tagName":"h2"},{"title":"The PATH Environment Variable​","type":1,"pageTitle":"Environment Variables","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/environment_variables/#the-path-environment-variable","content":" Similarly, some environment variables (like PATH) store lists of values. In this case, the convention is to use a colon ‘:’ as a separator. If a program wants the individual elements of such a list, it’s the program’s responsibility to split the variable’s string value into pieces.  Let’s have a closer look at that PATH variable. Its value defines the shell’s search path for executables, i.e., the list of directories that the shell looks in for runnable programs when you type in a program name without specifying what directory it is in.  For example, when we type a command like analyze, the shell needs to decide whether to run ./analyze or /bin/analyze. The rule it uses is simple: the shell checks each directory in the PATH variable in turn, looking for a program with the requested name in that directory. As soon as it finds a match, it stops searching and runs the program.  To show how this works, here are the components of PATH listed one per line:  /share/apps/singularity/bin /share/apps/local/bin /home/NetID/.local/bin /home/NetID/bin /share/apps/singularity/bin /share/apps/local/bin /usr/local/bin /usr/bin /usr/local/sbin /usr/sbin /usr/lpp/mmfs/bin /opt/slurm/bin   On our computer, there are actually three programs called analyze in three different directories: /bin/analyze, /usr/local/bin/analyze, and /users/NetID/analyze. Since the shell searches the directories in the order they’re listed in PATH, it finds /bin/analyze first and runs that. Notice that it will never find the program /users/NetID/analyze unless we type in the full path to the program, since the directory /users/NetID isn’t in PATH.  This means that I can have executables in lots of different places as long as I remember that I need to update my PATH so that my shell can find them.  What if I want to run two different versions of the same program? Since they share the same name, if I add them both to my PATH the first one found will always win. In the next episode we’ll learn how to use helper tools to help us manage our runtime environment to make that possible without us needing to do a lot of bookkeeping on what the value of PATH (and other important environment variables) is or should be.  Key Points Shell variables are by default treated as stringsVariables are assigned using = and recalled using the variable’s name prefixed by $Use export to make an variable available to other programsThe PATH variable defines the shell’s search path ","version":"Next","tagName":"h2"},{"title":"Introduction to High-Performance Computing","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/intro_hpc/","content":"Introduction to High-Performance Computing This tutorial is an introduction to using the Greene high-performance computing systems at NYU effectively. It is not intended to be an exhaustive course on parallel programming. The goal is to give new users of Greene an introduction and overview of the tools available and how to use them effectively. Prerequisites Command line experience is necessary for this lesson. We recommend the participants to go through our Introduction to Using the Shell on Greene, if new to the command line (also known as terminal or shell). Objectives By the end of this workshop, students will know how to: Identify problems a cluster can help solveUse the UNIX shell (also known as terminal or command line) to connect to a cluster.Transfer files onto a cluster.Submit and manage jobs on a cluster using a scheduler.Observe the benefits and limitations of parallel execution.","keywords":"","version":"Next"},{"title":"Accessing software via Modules","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/modules/","content":"","keywords":"","version":"Next"},{"title":"Environment Modules​","type":1,"pageTitle":"Accessing software via Modules","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/modules/#environment-modules","content":" Environment modules are the solution to these problems. A module is a self-contained description of a software package – it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.  There are a number of different environment module implementations commonly used on HPC systems: the two most common are TCL modules and Lmod. Both of these use similar syntax and the concepts are the same so learning to use one will allow you to use whichever is installed on the system you are using. In both implementations the module command is used to interact with environment modules. An additional subcommand is usually added to the command to specify what you want to do. For a list of subcommands you can use module -h or module help. As for all commands, you can access the full help on the man pages with man module.  On login you may start out with a default set of modules loaded or you may start out with an empty environment; this depends on the setup of the system you are using.  Listing Available Modules  To see available software modules, use module avail:  [NetID@log-1 ~]$ module avail ----------------------------------------------- /share/apps/modulefiles ----------------------------------------------- abyss/intel/2.3.0 liftoff/1.6.1 admixtools/intel/7.0.2 liftoff/1.6.3 admixture/1.3.0 llvm/11.0.0 advanpix-mct/4.9.3.15018 lofreq/2.1.5 amber/openmpi/24.00 lp_solve/intel/5.5.2.9 amber/openmpi/intel/20.06 lua/5.3.6 amber/openmpi/intel/20.11 macs2/2.1.1.20160309 amber/openmpi/intel/22.00 macs2/intel/2.2.7.1 amber/openmpi/intel/22.03 macs3/intel/3.0.0a5 ampl/20240308 mafft/intel/7.475 amplgurobilink/11.0.1 mambaforge/23.1.0 [removed most of the output here for clarity]   ","version":"Next","tagName":"h2"},{"title":"Listing Currently Loaded Modules​","type":1,"pageTitle":"Accessing software via Modules","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/modules/#listing-currently-loaded-modules","content":" You can use the module list command to see which modules you currently have loaded in your environment. If you have no modules loaded, you will see a message telling you so  [NetID@log-1 ~]$ module list No modules loaded   ","version":"Next","tagName":"h2"},{"title":"Loading and Unloading Software​","type":1,"pageTitle":"Accessing software via Modules","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/modules/#loading-and-unloading-software","content":" To load a software module, use module load. In this example we will use R.  Initially, R is not loaded. We can test this by using the which command. which looks for programs the same way that Bash does, so we can use it to tell us where a particular piece of software is stored.  [NetID@log-1 ~]$ which R /usr/bin/which: no R in (share/apps/singularity/bin: /share/apps/local/bin: /home/yourUsername/.local/bin: /home/yourUsername/bin: /share/apps/singularity/bin: /share/apps/local/bin: /usr/local/bin: /usr/bin: /usr/local/sbin: /usr/sbin: /usr/lpp/mmfs/bin: /opt/slurm/bin:)   We can load the R command with module load:  [NetID@log-1 ~]$ module load r/gcc/4.4.0 [NetID@log-1 ~]$ which R /share/apps/r/4.4.0/gcc/bin/R   So, what just happened?  To understand the output, first we need to understand the nature of the $PATH environment variable. $PATH is a special environment variable that controls where a UNIX system looks for software. Specifically $PATH is a list of directories (separated by :) that the OS searches through for a command before giving up and telling us it can’t find it. As with all environment variables we can print it out using echo.  [NetID@log-1 ~]$ echo $PATH /share/apps/r/4.4.0/gcc/bin: /share/apps/singularity/bin: /share/apps/local/bin: /home/yourUsername/.local/bin: /home/yourUsername/bin: /usr/local/bin: /usr/bin: /usr/local/sbin: /usr/sbin: /usr/lpp/mmfs/bin: /opt/slurm/bin:   You’ll notice a similarity to the output of the which command. In this case, there’s only one difference: the different directory at the beginning. When we ran the module load command, it added a directory to the beginning of our $PATH. Let’s examine what’s there:  [NetID@log-1 ~]$ ls /share/apps/r/4.4.0/gcc/bin R Rscript __run_base.bash   Taking this to its conclusion, module load will add software to your $PATH. It “loads” software. A special note on this - depending on which version of the module program that is installed at your site, module load will also load required software dependencies.  Let's see an example of this when loading the module lammps/openmpi/intel/20231214:  To start, let’s use module list. module list shows all loaded software modules.  [NetID@log-1 ~]$ module list No modules loaded [NetID@log-1 ~]$ module load lammps/openmpi/intel/20231214 [NetID@log-1 ~]$ module list Currently Loaded Modules: 1) szip/intel/2.1.1 5) gsl/intel/2.6 9) python/intel/3.8.6 2) hdf5/intel/1.12.0 6) openmpi/intel/4.0.5 10) boost/intel/1.74.0 3) netcdf-c/intel/4.7.4 7) fftw/openmpi/intel/3.3.9 11) plumed/openmpi/intel/2.8.3 4) pnetcdf/openmpi/intel/1.12.1 8) intel/19.1.2 12) lammps/openmpi/intel/20231214   So in this case, loading the lammps module (a molecular dynamics software package), also loaded 11 other modules as well. Let’s try unloading the lammps package.  [NetID@log-1 ~]$ module unload lammps/openmpi/intel/20231214 [NetID@log-1 ~]$ module list No modules loaded   So using module unload “un-loads” a module along with its dependencies. If we wanted to unload everything at once, we could run module purge (unloads everything).  [NetID@log-1 ~]$ module purge No modules loaded   note This module loading process happens principally through the manipulation of environment variables like $PATH. There is usually little or no data transfer involved.  The module loading process manipulates other special environment variables as well, including variables that influence where the system looks for software libraries, and sometimes variables which tell commercial software packages where to find license servers.  The module command also restores these shell environment variables to their previous state when a module is unloaded.  ","version":"Next","tagName":"h2"},{"title":"Software Versioning​","type":1,"pageTitle":"Accessing software via Modules","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/modules/#software-versioning","content":" So far, we’ve learned how to load and unload software packages. This is very useful. However, we have not yet addressed the issue of software versioning. At some point or other, you will run into issues where only one particular version of some software will be suitable. Perhaps a key bugfix only happened in a certain version, or version X broke compatibility with a file format you use. In either of these example cases, it helps to be very specific about what software is loaded.  Let’s examine the output of module avail more closely.  [NetID@log-1 ~]$ module avail ----------------------------------------------- /share/apps/modulefiles ----------------------------------------------- abyss/intel/2.3.0 liftoff/1.6.1 admixtools/intel/7.0.2 liftoff/1.6.3 admixture/1.3.0 llvm/11.0.0 advanpix-mct/4.9.3.15018 lofreq/2.1.5 amber/openmpi/24.00 lp_solve/intel/5.5.2.9 amber/openmpi/intel/20.06 lua/5.3.6 amber/openmpi/intel/20.11 macs2/2.1.1.20160309 amber/openmpi/intel/22.00 macs2/intel/2.2.7.1 amber/openmpi/intel/22.03 macs3/intel/3.0.0a5 ampl/20240308 mafft/intel/7.475 amplgurobilink/11.0.1 mambaforge/23.1.0 [removed most of the output here for clarity]   Let’s take a closer look at the nextflow module. Nextflow is a scientific workflow system predominantly used for bioinformatic data analysis In this case, we have:   nextflow/20.07.1 nextflow/20.11.0-edge nextflow/21.10.5 nextflow/23.04.1 nextflow/20.10.0 nextflow/21.04.3 nextflow/21.10.6 nextflow/24.04.3   How do we load each copy and which copy is the default?  [NetID@log-1 ~]$ module load Nextflow Lmod has detected the following error: The following module(s) are unknown: &quot;Nextflow&quot; Please check the spelling or version number. Also try &quot;module spider ...&quot; It is also possible your cache file is out-of-date; it may help to try: $ module --ignore-cache load &quot;Nextflow&quot; Also make sure that all modulefiles written in TCL start with the string #%Module   To load a software module we must specify the full module name:  [NYUNetID@log-1 ~]$ module load nextflow/24.04.3 [NYUNetID@log-1 ~]$ nextflow -version   Using Software Modules in Scripts Create a job that is able to run R --version. Remember, no software is loaded by default! Running a job is just like logging on to the system (you should not assume a module loaded on the login node is loaded on a compute node). [Click for Solution] Solution [NetID@log-1 ~]$ nano r-module.sh [NetID@log-1 ~]$ cat r-module.sh #!/bin/bash #SBATCH #SBATCH -t 00:00:30 module load r/gcc/4.4.0 R --version [NetID@log-1 ~]$ sbatch r-module.sh   Key Points Load software with module load softwareName.Unload software with module unloadThe module system handles software versioning and package conflicts for you automatically. ","version":"Next","tagName":"h2"},{"title":"Exploring Remote Resources","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/exploring_remote_resources/","content":"","keywords":"","version":"Next"},{"title":"Look Around the Remote System​","type":1,"pageTitle":"Exploring Remote Resources","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/exploring_remote_resources/#look-around-the-remote-system","content":" If you have not already connected to Greene, please do so now:  [NetID@glogin-1 ~]$ ssh NetID@greene.hpc.nyu.edu   Take a look at your home directory on the remote system:  [NetID@log-1 ~]$ ls   What’s different between your machine and the remote? Open a second terminal window on your local computer and run the ls command (without logging in to Greene). What differences do you see? [Click for Solution] Solution You would likely see something more like this: [user@laptop ~]$ ls Applications Documents Library Music Public Desktop Downloads Movies Pictures The remote computer’s home directory shares almost nothing in common with the local computer: they are completely separate systems!  Most high-performance computing systems run the Linux operating system, which is built around the UNIX Filesystem Hierarchy Standard. Instead of having a separate root for each hard drive or storage medium, all files and devices are anchored to the “root” directory, which is /:  [NetID@log-1 ~]$ ls / afs bin@ dev gpfs lib@ media mnt opt root sbin@ share state tmp var archive boot etc home lib64@ misc net proc run scratch srv sys usr vast   The /home/NetID directory is the one where we generally want to keep all of our files. Other folders on a UNIX OS contain system files and change as you install new software or upgrade your OS.  Using HPC filesystems On Geene, you have a number of places where you can store your files. These differ in both the amount of space allocated and whether or not they are backed up. Home – data stored here is available throughout the HPC system, and often backed up periodically. Please note the limit on the number of files (inodes) which can get used up easily. Use the myquota command to ensure that you are not running out of inodes!Scratch – used for temporary file storage while running jobs. It is not backed up and files that are unused for over 60 days are purged. It should not be used for long term storage.Vast – flash-drive based system that is optimal for workloads with high I/O rates. Like the scratch storage, it is also not backed up and files that are unused for over 60 days are purged.Research Project Space (RPS) – provides data storage space for research projects that is easily shared amongst collaborators, backed up, and not subject to the old file purging policy. Note that it is a paid service.Archive – provides a space for long-term storage of research output. It is only accessible from the login nodes, so it is unaccessible by running jobs.  ","version":"Next","tagName":"h2"},{"title":"Nodes​","type":1,"pageTitle":"Exploring Remote Resources","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/exploring_remote_resources/#nodes","content":" Individual computers that compose a cluster are typically called nodes (although you will also hear people call them servers, computers and machines). On a cluster, there are different types of nodes for different types of tasks. The node where you are right now is called the login node, head node, landing pad, or submit node. A login node serves as an access point to the cluster.  As a gateway, the login node should not be used for time-consuming or resource-intensive tasks. It is well suited for uploading and downloading small files, setting up software, and running tests. Generally speaking, in these lessons, we will avoid running jobs on the login node.  Who else is logged in to the login node?  [NYUNetID@log-1 ~]$ who   This may show only your user ID, but there are likely several other people (including fellow learners) connected right now.  Dedicated Transfer Nodes If you want to transfer larger amounts of data to or from the cluster, Greene offers dedicated nodes for data transfers only. The motivation for this lies in the fact that larger data transfers should not obstruct operation of the login node for anybody else. As a rule of thumb, consider all transfers of a volume larger than 500 MB to 1 GB as large. But these numbers change, e.g., depending on the network connection of yourself and of your cluster or other factors.  The real work on a cluster gets done by the compute (or worker) nodes. compute nodes come in many shapes and sizes, but generally are dedicated to long or hard tasks that require a lot of computational resources.  All interaction with the compute nodes is handled by a specialized piece of software called a scheduler (the scheduler used in this lesson is called Slurm). We’ll learn more about how to use the scheduler to submit jobs next, but for now, it can also tell us more information about the compute nodes.  For example, we can view all of the compute nodes by running the command sinfo.  [NetID@log-1 ~]$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cs up infinite 1 drain* cs524 cs up infinite 1 drng cs477 cs up infinite 3 drain cs[001,478,523] cs up infinite 117 mix cs[005-009,014,023,027-028,030,093-104,109-115,124,127-128,132-133,135-148,151,155-156,159-168,170-178,181-192,194-214,274,305-306,322,338-340,346,400,436,442,444-445,488] cs up infinite 1 resv cs402 cs up infinite 400 alloc cs[003-004,010-013,015-022,024-026,029,031-092,105-108,116-123,125-126,129-131,134,149-150,152-154,157-158,169,179-180,193,215-273,275-304,307-321,323-337,341-345,347-399,401,403-435,437-441,443,446-476,479-487,489-522] cm up infinite 37 mix cm[001-003,005,010,013-044] cm up infinite 7 alloc cm[004,006-009,011-012] cl up infinite 2 mix cl[003-004] cl up infinite 2 alloc cl[001-002] v100 up infinite 4 resv gv[001-004] v100 up infinite 6 mix gv[005-010] rtx8000 up infinite 1 resv gr030 rtx8000 up infinite 47 mix gr[001-004,007-008,010-027,029,031,034-042,049-060] rtx8000 up infinite 12 alloc gr[005-006,009,028,032-033,043-048] a100_1 up infinite 9 mix ga[001-009] a100_2 up infinite 1 drng ga037 a100_2 up infinite 18 mix ga[010-024,041-043] a100_2 up infinite 15 alloc ga[025-036,038-040] cds_rtx_d up infinite 1 drain gr073 cds_rtx_d up infinite 19 mix gr[054-072] cds_rtx_a up infinite 1 drain gr073 cds_rtx_a up infinite 1 resv gr030 cds_rtx_a up infinite 59 mix gr[001-004,007-008,010-027,029,031,034-042,049-072] cds_rtx_a up infinite 12 alloc gr[005-006,009,028,032-033,043-048] cilvr_a100 up infinite 9 mix ga[001-009] cilvr_a100_1 up infinite 8 mix ga[001-008] tandon_a100_1 up infinite 1 mix ga009 cds_a100_2 up infinite 6 mix ga[010-012,041-043] tandon_a100_2 up infinite 12 mix ga[013-024] tandon_a100_2 up infinite 2 alloc ga[025-026] chemistry_a100_2 up infinite 1 drng ga037 chemistry_a100_2 up infinite 13 alloc ga[027-036,038-040] stake_a100_1 up infinite 9 mix ga[001-009] stake_a100_2 up infinite 1 drng ga037 stake_a100_2 up infinite 18 mix ga[010-024,041-043] stake_a100_2 up infinite 15 alloc ga[025-036,038-040] tandon_h100_1 up infinite 15 mix gh[001-015] stake_h100_1 up infinite 15 mix gh[001-015] h100_1 up infinite 15 mix gh[001-015] cpu_a100_1 up infinite 9 mix ga[001-009] cpu_a100_2 up infinite 1 drng ga037 cpu_a100_2 up infinite 18 mix ga[010-024,041-043] cpu_a100_2 up infinite 15 alloc ga[025-036,038-040] cpu_gpu up infinite 1 drain gr073 cpu_gpu up infinite 5 resv gr030,gv[001-004] cpu_gpu up infinite 65 mix gr[001-004,007-008,010-027,029,031,034-042,049-072],gv[005-010] cpu_gpu up infinite 12 alloc gr[005-006,009,028,032-033,043-048] mi50 up infinite 1 resv gm011 mi50 up infinite 16 mix gm[001-008,012,014-020] mi50 up infinite 3 idle gm[009-010,013] mi100 up infinite 3 mix gm[021-023] mi250 up infinite 1 resv gm025 mi250 up infinite 1 mix gm024 gpu_misc_v100 up infinite 1 mix gv012 chem_cpu0 up infinite 1 mix cs488 chem_cpu0 up infinite 22 alloc cs[489-510] xwang up infinite 10 alloc cs[511-520] short up infinite 38 mix cm[001-003,005,010,013-045] short up infinite 7 alloc cm[004,006-009,011-012] short up infinite 5 idle cm[046-050]   A lot of the nodes are busy running work for other users: we are not alone here!  There are also specialized machines used for managing disk storage, user authentication, and other infrastructure-related tasks. Although we do not typically logon to or interact with these machines directly, they enable a number of key features like ensuring our user account and files are available throughout the HPC system.  ","version":"Next","tagName":"h2"},{"title":"What’s in a Node?​","type":1,"pageTitle":"Exploring Remote Resources","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/exploring_remote_resources/#whats-in-a-node","content":" All of the nodes in an HPC system have the same components as your own laptop or desktop: CPUs (sometimes also called processors or cores), memory (or RAM), and disk space. CPUs are a computer’s tool for actually running programs and calculations. Information about a current task is stored in the computer’s memory. Disk refers to all storage that can be accessed like a file system. This is generally storage that can hold data permanently, i.e. data is still there even if the computer has been restarted. While this storage can be local (a hard drive installed inside of it), it is more common for nodes to connect to a shared, remote fileserver or cluster of servers.    Explore Your Computer Try to find out the number of CPUs and amount of memory available on your personal computer. Note that, if you’re logged in to the remote computer cluster, you need to log out first. To do so, type Ctrl+d or exit: [NYUNetID@log-1 ~]$ exit [user@laptop ~]$ [Click for Solution] Solution There are several ways to do this. Most operating systems have a graphical system monitor, like the Windows Task Manager. More detailed information can be found on the command line: Run system utilities: [user@laptop ~]$ nproc --all [user@laptop ~]$ free -m Read from /proc: [user@laptop ~]$ cat /proc/cpuinfo [user@laptop ~]$ cat /proc/meminfo Run system monitor [user@laptop ~]$ htop   Explore the Login Node Now compare the resources of your computer with those of the login node. [Click for Solution] Solution [user@laptop ~]$ ssh NetID@greene.hpc.nyu.edu [NetID@log-1 ~]$ nproc --all [NetID@log-1 ~]$ free -m You can get more information about the processors using lscpu, and a lot of detail about the memory by reading the file /proc/meminfo: [NetID@log-1 ~]$ less /proc/meminfo You can also explore the available filesystems using df to show disk free space. The -h flag renders the sizes in a human-friendly format, i.e., GB instead of B. The type flag -T shows what kind of filesystem each resource is. [NYUNetID@log-1 ~]$ df -Th Different results from df The local filesystems (ext, tmp, xfs, zfs) will depend on whether you’re on the same login node (or compute node, later on).Networked filesystems (beegfs, cifs, gpfs, nfs, pvfs) will be similar – but may include NetID, depending on how it is mounted. Shared Filesystems This is an important point to remember: files saved on one node (computer) are often available everywhere on the cluster!  Explore a Worker Node Finally, let’s look at the resources available on the worker nodes where your jobs will actually run. Try running this command to see the name, CPUs and memory available on a worker node: sinfo --node cs012 -o &quot;%n %c %m&quot;   Compare Your Computer, the Login Node and the Compute Node Compare your laptop’s number of processors and memory with the numbers you see on the cluster login node and compute node. What implications do you think the differences might have on running your research work on the different systems and nodes? [Click for Solution] Solution Compute nodes have substantially more memory (RAM) installed than a personal computer. More, faster memory is key for large or complex numerical tasks.  Differences Between Nodes Many HPC clusters have a variety of nodes optimized for particular workloads. Some nodes may have larger amount of memory, or specialized resources such as Graphics Processing Units (GPUs or “video cards”).  With all of this in mind, we will now cover how to talk to the cluster’s scheduler, and use it to start running our scripts and programs!  Key Points An HPC system is a set of networked machines.HPC systems typically provide login nodes and a set of compute nodes.The resources found on independent (worker) nodes can vary in volume and type (amount of RAM, processor architecture, availability of network mounted filesystems, etc.).Files saved on shared storage are available on all nodes.The login node is a shared machine: be considerate of other users. ","version":"Next","tagName":"h2"},{"title":"Scheduler Fundamentals","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/","content":"","keywords":"","version":"Next"},{"title":"Job Scheduler​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#job-scheduler","content":" An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when.  The following illustration compares these tasks of a job scheduler to a waiter in a restaurant. If you can relate to an instance where you had to wait for a while in a queue to get in to a popular restaurant, then you may now understand why sometimes your job do not start instantly as in your laptop.    The scheduler used in this lesson is Slurm. Although Slurm is not used everywhere, running jobs is quite similar regardless of what software is being used. The exact syntax might change, but the concepts remain the same.  ","version":"Next","tagName":"h2"},{"title":"Running a Batch Job​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#running-a-batch-job","content":" The most basic use of the scheduler is to run a command non-interactively. Any command (or series of commands) that you want to run on the cluster is called a job, and the process of using a scheduler to run the job is called batch job submission.  In this case, the job we want to run is a shell script – essentially a text file containing a list of UNIX commands to be executed in a sequential manner. Our shell script will have three parts:  On the very first line, add #!/bin/bash. The #! (pronounced “hash-bang” or “shebang”) tells the computer what program is meant to process the contents of this file. In this case, we are telling it that the commands that follow are written for the command-line shell (what we’ve been doing everything in so far).Anywhere below the first line, we’ll add an echo command with a friendly greeting. When run, the shell script will print whatever comes after echo in the terminal. echo -n will print everything that follows, without ending the line by printing the new-line character. On the last line, we’ll invoke the hostname command, which will print the name of the machine the script is run on.  [NetID@log-1 ~]$ nano example-job.sh   #!/bin/bash echo -n &quot;This script is running on &quot; hostname   Creating Our Test Job Run the script. Does it execute on the cluster or just our login node? [Click for Solution] Solution [NetID@log-1 ~]$ bash example-job.sh This script is running on log-1   This script ran on the login node, but we want to take advantage of the compute nodes: we need the scheduler to queue up example-job.sh to run on a compute node.  To submit this task to the scheduler, we use the sbatch command. This creates a job which will run the script when dispatched to a compute node which the queuing system has identified as being available to perform the work.  [NetID@log-1 ~]$ sbatch example-job.sh Submitted batch job 137860   And that’s all we need to do to submit a job. Our work is done – now the scheduler takes over and tries to run the job for us. While the job is waiting to run, it goes into a list of jobs called the queue. To check on our job’s status, we check the queue using the command squeue -u NetID.  [NetID@log-1 ~]$ squeue -u NetID JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 137860 normal example- usernm R 0:02 1 c5-59   Where’s the Output? On the login node, this script printed output to the terminal – but now, when squeue shows the job has finished, nothing was printed to the terminal. Cluster job output is typically redirected to a file in the directory you launched it from. Use ls to find the file and cat to read it.  ","version":"Next","tagName":"h2"},{"title":"Customising a Job​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#customising-a-job","content":" The job we just ran used all of the scheduler’s default options. In a real-world scenario, that’s probably not what we want. The default options represent a reasonable minimum. Chances are, we will need more cores, more memory, more time, among other special considerations. To get access to these resources we must customize our job script.  Comments in UNIX shell scripts (denoted by #) are typically ignored, but there are exceptions. For instance the special #! comment at the beginning of scripts specifies what program should be used to run it (you’ll typically see #!/usr/bin/env bash). Schedulers like Slurm also have a special comment used to denote special scheduler-specific options. Though these comments differ from scheduler to scheduler, Slurm’s special comment is #SBATCH. Anything following the #SBATCH comment is interpreted as an instruction to the scheduler.  Let’s illustrate this by example. By default, a job’s name is the name of the script, but the -J option can be used to change the name of a job. Add an option to the script:  [NetID@log-1 ~]$ cat example-job.sh #!/bin/bash #SBATCH -J hello-world echo -n &quot;This script is running on &quot; hostname   Submit the job and monitor its status:  [NetID@log-1 ~]$ sbatch example-job.sh [NetID@log-1 ~]$ squeue -u NetID JOBID ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS 38191 yourAccount hello-wo PD Priority N/A 0:00 1:00:00 1 1   Fantastic, we’ve successfully changed the name of our job!  ","version":"Next","tagName":"h2"},{"title":"Resource Requests​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#resource-requests","content":" What about more important changes, such as the number of cores and memory for our jobs? One thing that is absolutely critical when working on an HPC system is specifying the resources required to run a job. This allows the scheduler to find the right time and place to schedule our job. If you do not specify requirements (such as the amount of time you need), you will likely be stuck with your site’s default resources, which is probably not what you want.  The following are several key resource requests:  --ntasks=&lt;ntasks&gt; or -n &lt;ntasks&gt;: How many CPU cores does your job need, in total?--time &lt;days-hours:minutes:seconds&gt; or -t &lt;days-hours:minutes:seconds&gt;: How much real-world time (walltime) will your job take to run? The &lt;days&gt; part can be omitted.--mem=&lt;megabytes&gt;: How much memory on a node does your job need in megabytes? You can also specify gigabytes using by adding a little g afterwards (example: --mem=5g)--nodes=&lt;nnodes&gt; or -N &lt;nnodes&gt;: How many separate machines does your job need to run on? Note that if you set ntasks to a number greater than what one machine can offer, Slurm will set this value automatically.  note Just requesting these resources does not make your job run faster, nor does it necessarily mean that you will consume all of these resources. It only means that these are made available to you. Your job may end up using less memory, or less time, or fewer nodes than you have requested, and it will still run.  It’s best if your requests accurately reflect your job’s requirements. We’ll talk more about how to make sure that you’re using resources effectively in a later episode of this lesson.  Submitting Resource Requests Modify our hostname script so that it runs for a minute, then submit a job for it on the cluster. [Click for Solution] Solution NetID@log-1 ~]$ cat example-job.sh #!/bin/bash #SBATCH -t 00:01 # timeout in HH:MM echo -n &quot;This script is running on &quot; sleep 20 # time in seconds hostname [NetID@log-1 ~]$ sbatch example-job.sh Why are the Slurm runtime and sleep time not identical?  Resource requests are typically binding. If you exceed them, your job will be killed. Let’s use wall time as an example. We will request 1 minute of wall time, and attempt to run a job for two minutes.  [NetID@log-1 ~]$ cat example-job.sh   #!/bin/bash #SBATCH -J long_job #SBATCH -t 00:01 # timeout in HH:MM echo &quot;This script is running on ... &quot; sleep 240 # time in seconds hostname   Submit the job and wait for it to finish. Once it is has finished, check the log file.  [NetID@log-1 ~]$ sbatch example-job.sh [NetID@log-1 ~]$ squeue -u NetID cat slurm-38193.out This job is running on: c1-14 slurmstepd: error: *** JOB 38193 ON gra533 CANCELLED AT 2017-07-02T16:35:48 DUE TO TIME LIMIT ***   Our job was killed for exceeding the amount of resources it requested. Although this appears harsh, this is actually a feature. Strict adherence to resource requests allows the scheduler to find the best possible place for your jobs. Even more importantly, it ensures that another user cannot use more resources than they’ve been given. If another user messes up and accidentally attempts to use all of the cores or memory on a node, Slurm will either restrain their job to the requested resources or kill the job outright. Other jobs on the node will be unaffected. This means that one user cannot mess up the experience of others, the only jobs affected by a mistake in scheduling will be their own.  ","version":"Next","tagName":"h2"},{"title":"Cancelling a Job​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#cancelling-a-job","content":" Sometimes we’ll make a mistake and need to cancel a job. This can be done with the scancel command. Let’s submit a job and then cancel it using its job number (remember to change the walltime so that it runs long enough for you to cancel it before it is killed!).  [NetID@log-1 ~]$ sbatch example-job.sh [NetID@log-1 ~]$ squeue -u NetID Submitted batch job 38759 JOBID ACCOUNT NAME ST REASON TIME TIME_LEFT NODES CPUS 38759 yourAccount example-job.sh PD Priority 0:00 1:00 1 1   Now cancel the job with its job number (printed in your terminal). A clean return of your command prompt indicates that the request to cancel the job was successful.  [NetID@log-1 ~]$ scancel 38759 # It might take a minute for the job to disappear from the queue... [NetID@log-1 ~]$ squeue -u NetID JOBID USER ACCOUNT NAME ST REASON START_TIME TIME TIME_LEFT NODES CPUS   Cancelling multiple jobs We can also all of our jobs at once using the -u option. This will delete all jobs for a specific user (in this case us). Note that you can only delete your own jobs. Try submitting multiple jobs and then cancelling them all with scancel -u NetID.  ","version":"Next","tagName":"h2"},{"title":"Other Types of Jobs​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#other-types-of-jobs","content":" Up to this point, we’ve focused on running jobs in batch mode. Slurm also provides the ability to start an interactive session.  There are very frequently tasks that need to be done interactively. Creating an entire job script might be overkill, but the amount of resources required is too much for a login node to handle. A good example of this might be building a genome index for alignment with a tool like HISAT2. Fortunately, we can run these types of tasks as a one-off with srun.  srun runs a single command on the cluster and then exits. Let’s demonstrate this by running the hostname command with srun. (We can cancel an srun job with Ctrl-c.)   srun hostname gra752   srun accepts all of the same options as sbatch. However, instead of specifying these in a script, these options are specified on the command-line when starting a job. To submit a job that uses 2 CPUs for instance, we could use the following command:  srun -n 2 echo &quot;This job will use 2 CPUs.&quot; This job will use 2 CPUs. This job will use 2 CPUs.   Typically, the resulting shell environment will be the same as that for sbatch.  ","version":"Next","tagName":"h2"},{"title":"Interactive jobs​","type":1,"pageTitle":"Scheduler Fundamentals","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals/#interactive-jobs","content":" Sometimes, you will need a lot of resource for interactive use. Perhaps it’s our first time running an analysis or we are attempting to debug something that went wrong with a previous job. Fortunately, Slurm makes it easy to start an interactive job with srun:  srun --pty bash   You should be presented with a bash prompt. Note that the prompt will likely change to reflect your new location, in this case the compute node we are logged on. You can also verify this with hostname.  Creating remote graphics To see graphical output inside your jobs, you need to use X11 forwarding. To connect with this feature enabled, use the -Y option when you login with ssh with the command ssh -Y username@host. To demonstrate what happens when you create a graphics window on the remote node, use gnuplot as shown in the following example after you have created your ssh session with X11 forwarding: [NetID@log-1 ~]$ srun --x11 -c4 -t2:00:00 --mem=4000 --pty /bin/bash [NetID@cm034 ~]$ module load gnuplot/gcc/5.4.1 [NetID@cm034 ~]$ gnuplot gnuplot&gt; test If X11 forwarding is working you will see a test plot window open. Greene has the slurm-spank-x11 plugin installed, so you can ensure X11 forwarding within interactive jobs by using the --x11 option for srun with the command srun --x11 --pty bash.  When you are done with the interactive job, type exit to quit your session.  Key Points The scheduler handles how compute resources are shared between users.A job is just a shell script.Request slightly more resources than you will need. ","version":"Next","tagName":"h2"},{"title":"Transferring files with remote computers","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/transferring_files_remote/","content":"","keywords":"","version":"Next"},{"title":"Archiving Files​","type":1,"pageTitle":"Transferring files with remote computers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/transferring_files_remote/#archiving-files","content":" One of the biggest challenges we often face when transferring data between remote HPC systems is that of large numbers of files. There is an overhead to transferring each individual file and when we are transferring large numbers of files these overheads combine to slow down our transfers to a large degree.  The solution to this problem is to archive multiple files into smaller numbers of larger files before we transfer the data to improve our transfer efficiency. Sometimes we will combine archiving with compression to reduce the amount of data we have to transfer and so speed up the transfer. The most common archiving command you will use on a (Linux) HPC cluster is tar.  tar can be used to combine files and folders into a single archive file and, optionally, compress the result. Let’s look at the file we downloaded from the lesson site, amdahl.tar.gz.  The .gz part stands for gzip, which is a compression library. It’s common (but not necessary!) that this kind of file can be interpreted by reading its name: it appears somebody took files and folders relating to something called “amdahl,” wrapped them all up into a single file with tar, then compressed that archive with gzip to save space.  Let’s see if that is the case, without unpacking the file. tar prints the “table of contents” with the -t flag, for the file specified with the -f flag followed by the filename. Note that you can concatenate the two flags: writing -t -f is interchangeable with writing -tf together. However, the argument following -f must be a filename, so writing -ft will not work.  [user@laptop ~]$ tar -tf amdahl.tar.gz hpc-carpentry-amdahl-46c9b4b/ hpc-carpentry-amdahl-46c9b4b/.github/ hpc-carpentry-amdahl-46c9b4b/.github/workflows/ hpc-carpentry-amdahl-46c9b4b/.github/workflows/python-publish.yml hpc-carpentry-amdahl-46c9b4b/.gitignore hpc-carpentry-amdahl-46c9b4b/LICENSE hpc-carpentry-amdahl-46c9b4b/README.md hpc-carpentry-amdahl-46c9b4b/amdahl/ hpc-carpentry-amdahl-46c9b4b/amdahl/__init__.py hpc-carpentry-amdahl-46c9b4b/amdahl/__main__.py hpc-carpentry-amdahl-46c9b4b/amdahl/amdahl.py hpc-carpentry-amdahl-46c9b4b/requirements.txt hpc-carpentry-amdahl-46c9b4b/setup.py   This example output shows a folder which contains a few files, where 46c9b4b is an 8-character git commit hash that will change when the source material is updated.  Now let’s unpack the archive. We’ll run tar with a few common flags:  -x to extract the archive-v for verbose output-z for gzip compression-f «tarball» for the file to be unpacked  Extract the Archive Using the flags above, unpack the source code tarball into a new directory named “amdahl” using tar. [user@laptop ~]$ tar -xvzf amdahl.tar.gz hpc-carpentry-amdahl-46c9b4b/ hpc-carpentry-amdahl-46c9b4b/.github/ hpc-carpentry-amdahl-46c9b4b/.github/workflows/ hpc-carpentry-amdahl-46c9b4b/.github/workflows/python-publish.yml hpc-carpentry-amdahl-46c9b4b/.gitignore hpc-carpentry-amdahl-46c9b4b/LICENSE hpc-carpentry-amdahl-46c9b4b/README.md hpc-carpentry-amdahl-46c9b4b/amdahl/ hpc-carpentry-amdahl-46c9b4b/amdahl/__init__.py hpc-carpentry-amdahl-46c9b4b/amdahl/__main__.py hpc-carpentry-amdahl-46c9b4b/amdahl/amdahl.py hpc-carpentry-amdahl-46c9b4b/requirements.txt hpc-carpentry-amdahl-46c9b4b/setup.py Note that we did not need to type out -x -v -z -f, thanks to flag concatenation, though the command works identically either way – so long as the concatenated list ends with f, because the next string must specify the name of the file to extract.  The folder has an unfortunate name, so let’s change that to something more convenient.  [user@laptop ~]$ mv hpc-carpentry-amdahl-46c9b4b amdahl   Check the size of the extracted directory and compare to the compressed file size, using du for “disk usage”.  [user@laptop ~]$ du -sh amdahl.tar.gz 8.0K amdahl.tar.gz [user@laptop ~]$ du -sh amdahl 48K amdahl   Text files (including Python source code) compress nicely: the “tarball” is one-sixth the total size of the raw data!  If you want to reverse the process – compressing raw data instead of extracting it – set a c flag instead of x, set the archive filename, then provide a directory to compress:  [user@laptop ~]$ tar -cvzf compressed_code.tar.gz amdahl amdahl/ amdahl/.github/ amdahl/.github/workflows/ amdahl/.github/workflows/python-publish.yml amdahl/.gitignore amdahl/LICENSE amdahl/README.md amdahl/amdahl/ amdahl/amdahl/__init__.py amdahl/amdahl/__main__.py amdahl/amdahl/amdahl.py amdahl/requirements.txt amdahl/setup.py   If you give amdahl.tar.gz as the filename in the above command, tar will update the existing tarball with any changes you made to the files. That would mean adding the new amdahl folder to the existing folder (hpc-carpentry-amdahl-46c9b4b) inside the tarball, doubling the size of the archive!  Working with Windows When you transfer text files from a Windows system to a Unix system (Mac, Linux, BSD, Solaris, etc.) this can cause problems. Windows encodes its files slightly different than Unix, and adds an extra character to every line. On a Unix system, every line in a file ends with a \\n (newline). On Windows, every line in a file ends with a \\r\\n (carriage return + newline). This causes problems sometimes. Though most modern programming languages and software handles this correctly, in some rare instances, you may run into an issue. The solution is to convert a file from Windows to Unix encoding with the dos2unix command. You can identify if a file has Windows line endings with cat -A filename. A file with Windows line endings will have ^M$ at the end of every line. A file with Unix line endings will have $ at the end of a line. To convert the file, just run dos2unix filename. (Conversely, to convert back to Windows format, you can run unix2dos filename.)  ","version":"Next","tagName":"h2"},{"title":"Transferring Single Files and Folders With scp​","type":1,"pageTitle":"Transferring files with remote computers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/transferring_files_remote/#transferring-single-files-and-folders-with-scp","content":" To copy a single file to or from the cluster, we can use scp (“secure copy”). The syntax can be a little complex for new users, but we’ll break it down. The scp command is a relative of the ssh command we used to access the system, and can use the same public-key authentication mechanism.  To upload to another computer, the template command is  [user@laptop ~]$ scp local_file NetID@greene.hpc.nyu.edu:remote_destination   in which @ and : are field separators and remote_destination is a path relative to your remote home directory, or a new filename if you wish to change it, or both a relative path and a new filename. If you don’t have a specific folder in mind you can omit the remote_destination and the file will be copied to your home directory on the remote computer (with its original name). If you include a remote_destination, note that scp interprets this the same way cp does when making local copies: if it exists and is a folder, the file is copied inside the folder; if it exists and is a file, the file is overwritten with the contents of local_file; if it does not exist, it is assumed to be a destination filename for local_file.  Upload the lesson material to your remote home directory like so:  [user@laptop ~]$ scp amdahl.tar.gz NetID@greene.hpc.nyu.edu:   ","version":"Next","tagName":"h2"},{"title":"Transferring a Directory​","type":1,"pageTitle":"Transferring files with remote computers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/transferring_files_remote/#transferring-a-directory","content":" To transfer an entire directory, we add the -r flag for “recursive”: copy the item specified, and every item below it, and every item below those… until it reaches the bottom of the directory tree rooted at the folder name you provided.  [user@laptop ~]$ scp -r amdahl NetID@greene.hpc.nyu.edu:   Caution For a large directory – either in size or number of files – copying with -r can take a long time to complete.  When using scp, you may have noticed that a : always follows the remote computer name. A string after the : specifies the remote directory you wish to transfer the file or folder to, including a new name if you wish to rename the remote material. If you leave this field blank, scp defaults to your home directory and the name of the local material to be transferred.  On Linux computers, / is the separator in file or directory paths. A path starting with a / is called absolute, since there can be nothing above the root /. A path that does not start with / is called relative, since it is not anchored to the root.  If you want to upload a file to a location inside your home directory – which is often the case – then you don’t need a leading /. After the :, you can type the destination path relative to your home directory. If your home directory is the destination, you can leave the destination field blank, or type ~ – the shorthand for your home directory – for completeness.  With scp, a trailing slash on the target directory is optional, and has no effect. A trailing slash on a source directory is important for other commands, like rsync.  A Note on rsync As you gain experience with transferring files, you may find the scp command limiting. The rsync utility provides advanced features for file transfer and is typically faster compared to both scp and sftp (see below). It is especially useful for transferring large and/or many files and for synchronizing folder contents between computers. The syntax is similar to scp. To transfer to another computer with commonly used options: [user@laptop ~]$ rsync -avP amdahl.tar.gz NetID@greene.hpc.nyu.edu: The options are: -a (archive) to preserve file timestamps, permissions, and folders, among other things; implies recursion-v (verbose) to get verbose output to help monitor the transfer-P (partial/progress) to preserve partially transferred files in case of an interruption and also displays the progress of the transfer. To recursively copy a directory, we can use the same options: [user@laptop ~]$ rsync -avP amdahl NetID@greene.hpc.nyu.edu:~/ As written, this will place the local directory and its contents under your home directory on the remote system. If a trailing slash is added to the source, a new directory corresponding to the transferred directory will not be created, and the contents of the source directory will be copied directly into the destination directory. To download a file, we simply change the source and destination: [user@laptop ~]$ rsync -avP NetID@greene.hpc.nyu.edu:amdahl ./   File transfers using both scp and rsync use SSH to encrypt data sent through the network. So, if you can connect via SSH, you will be able to transfer files. By default, SSH uses network port 22. If a custom SSH port is in use, you will have to specify it using the appropriate flag, often -p, -P, or --port. Check --help or the man page if you’re unsure.  Change the Rsync Port Say we have to connect rsync through port 768 instead of 22. How would we modify this command? [user@laptop ~]$ rsync amdahl.tar.gz NetID@greene.hpc.nyu.edu: Hint: check the man page or “help” for rsync. [Click for Solution] Solution [user@laptop ~]$ man rsync [user@laptop ~]$ rsync --help | grep port --port=PORT specify double-colon alternate port number See http://rsync.samba.org/ for updates, bug reports, and answers [user@laptop ~]$ rsync --port=768 amdahl.tar.gz NetID@greene.hpc.nyu.edu: note This command will fail, as the correct port in this case is the default: 22.  ","version":"Next","tagName":"h2"},{"title":"Transferring Files Interactively with FileZilla​","type":1,"pageTitle":"Transferring files with remote computers","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/transferring_files_remote/#transferring-files-interactively-with-filezilla","content":" FileZilla is a cross-platform client for downloading and uploading files to and from a remote computer. It is absolutely fool-proof and always works quite well. It uses the sftp protocol. You can read more about using the sftp protocol in the command line in the lesson discussion.  Download and install the FileZilla client from https://filezilla-project.org. After installing and opening the program, you should end up with a window with a file browser of your local system on the left hand side of the screen. When you connect to the cluster, your cluster files will appear on the right hand side.  To connect to the cluster, we’ll just need to enter our credentials at the top of the screen:  Host: sftp://greene.hpc.nyu.eduUser: Your NetIDPassword: Your NetID passwordPort: (leave blank to use the default port)  Hit “Quickconnect” to connect. You should see your remote files appear on the right hand side of the screen. You can drag-and-drop files between the left (local) and right (remote) sides of the screen to transfer files.  Finally, if you need to move large files (typically larger than a gigabyte) from one remote computer to another remote computer, SSH in to the computer hosting the files and use scp or rsync to transfer over to the other. This will be more efficient than using FileZilla (or related applications) that would copy from the source to your local machine, then to the destination machine.  Key Points wget and curl -O download a file from the internet.scp and rsync transfer files to and from your computer.You can use an SFTP client like FileZilla to transfer files through a GUI. ","version":"Next","tagName":"h2"},{"title":"Running a parallel job","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/","content":"","keywords":"","version":"Next"},{"title":"Install the Amdahl Program​","type":1,"pageTitle":"Running a parallel job","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/#install-the-amdahl-program","content":" With the Amdahl source code on the cluster, we can install it, which will provide access to the amdahl executable.  The Amdahl code has one dependency: mpi4py. Package Installer for Python (pip) will collect mpi4py from the Internet and install it for you, but it needs an active mpi module. Here we will load openmpi/gcc/4.1.6 to build mpi4py.  Move into the extracted directory, the use pip, to install it in your (“user”) home directory:  [NetID@log-1 ~]$ cd amdahl [NetID@log-1 ~]$ module load python/intel/3.8.6 [NetID@log-1 ~]$ python -m venv ./test_venv [NetID@log-1 ~]$ source ./test_venv/bin/activate [NetID@log-1 ~]$ module load openmpi/gcc/4.1.6 [NetID@log-1 ~]$ python -m pip install . Processing /home/NetID/packages/temp/hpc-carpentry-amdahl-46c9b4b Collecting mpi4py Using cached mpi4py-4.0.0.tar.gz (464 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing wheel metadata ... done Building wheels for collected packages: amdahl, mpi4py Building wheel for amdahl (setup.py) ... done Created wheel for amdahl: filename=amdahl-0.3.1-py3-none-any.whl size=6996 sha256=13a95c3e6fbc53fde1c90a4a9bbb3fd3179d5e3afa3e19b4131a05d9ac798981 Stored in directory: /home/NetID/.cache/pip/wheels/2c/53/fc/19c3053b3a1d3625ac26158b28f263783f66ec258df97aefcf Building wheel for mpi4py (PEP 517) ... done Created wheel for mpi4py: filename=mpi4py-4.0.0-cp38-cp38-linux_x86_64.whl size=5169079 sha256=9afceb56e22608a7de33442a60bbde3cbd4aa06947d48de5f6dc63932d34bc9f Stored in directory: /home/NetID/.cache/pip/wheels/31/3b/6f/dc579e9ff3e2273078596b0cbc1e8d6cbf5a3a05cfad4a380a Successfully built amdahl mpi4py Installing collected packages: mpi4py, amdahl Successfully installed amdahl-0.3.1 mpi4py-4.0.0 WARNING: You are using pip version 20.2.3; however, version 24.2 is available. You should consider upgrading via the '/home/NetID/packages/temp/hpc-carpentry-amdahl-46c9b4b/test_venv/bin/python3 -m pip install --upgrade pip' command.   Amdahl is Python Code The Amdahl program is written in Python, and installing or using it requires locating the python executable on the login node.  ","version":"Next","tagName":"h2"},{"title":"Help!​","type":1,"pageTitle":"Running a parallel job","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/#help","content":" Many command-line programs include a “help” message. Try it with amdahl:  [NetID@log-1 ~]$ amdahl --help usage: amdahl [-h] [-p [PARALLEL_PROPORTION]] [-w [WORK_SECONDS]] [-t] [-e] [-j [JITTER_PROPORTION]] optional arguments: -h, --help show this help message and exit -p [PARALLEL_PROPORTION], --parallel-proportion [PARALLEL_PROPORTION] Parallel proportion: a float between 0 and 1 -w [WORK_SECONDS], --work-seconds [WORK_SECONDS] Total seconds of workload: an integer greater than 0 -t, --terse Format output as a machine-readable object for easier analysis -e, --exact Exactly match requested timing by disabling random jitter -j [JITTER_PROPORTION], --jitter-proportion [JITTER_PROPORTION] Random jitter: a float between -1 and +1   This message doesn’t tell us much about what the program does, but it does tell us the important flags we might want to use when launching it.  ","version":"Next","tagName":"h2"},{"title":"Running the Job on a Compute Node​","type":1,"pageTitle":"Running a parallel job","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/#running-the-job-on-a-compute-node","content":" Create a submission file, requesting one task on a single node, then launch it.  [NetID@log-1 ~]$ nano serial-job.sh [NetID@log-1 ~]$ cat serial-job.sh   #!/bin/bash #SBATCH -J solo-job #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --mem=3G # Load the computing environment we need module load python/intel/3.8.6 module load openmpi/gcc/4.1.6 source /home/yourUsername/amdahl/test_venv/bin/activate # Execute the task srun amdahl   [NetID@log-1 ~]$ sbatch serial-job.sh   As before, use the Slurm status commands to check whether your job is running and when it ends:  [NetID@log-1 ~]$ squeue -u NetID   Use ls to locate the output file. The -t flag sorts in reverse-chronological order: newest first. What was the output?  Read the Job Output [Click for Output] Output The cluster output should be written to a file in the folder you launched the job from. For example, [NetID@log-1 ~]$ ls -t slurm-347087.out serial-job.sh amdahl README.md LICENSE.txt [NetID@log-1 ~]$ cat slurm-347087.out Doing 30.000 seconds of 'work' on 1 processor, which should take 30.000 seconds with 0.850 parallel proportion of the workload. Hello, World! I am process 0 of 1 on cs. I will do all the serial 'work' for 4.500 seconds. Hello, World! I am process 0 of 1 on cs. I will do parallel 'work' for 25.500 seconds. Total execution time (according to rank 0): 30.033 seconds   As we saw before, two of the amdahl program flags set the amount of work and the proportion of that work that is parallel in nature. Based on the output, we can see that the code uses a default of 30 seconds of work that is 85% parallel. The program ran for just over 30 seconds in total, and if we run the numbers, it is true that 15% of it was marked ‘serial’ and 85% was ‘parallel’.  Since we only gave the job one CPU, this job wasn’t really parallel: the same processor performed the ‘serial’ work for 4.5 seconds, then the ‘parallel’ part for 25.5 seconds, and no time was saved. The cluster can do better, if we ask.  ","version":"Next","tagName":"h2"},{"title":"Running the Parallel Job​","type":1,"pageTitle":"Running a parallel job","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/#running-the-parallel-job","content":" The amdahl program uses the Message Passing Interface (MPI) for parallelism – this is a common tool on HPC systems.  What is MPI? The Message Passing Interface is a set of tools which allow multiple tasks running simultaneously to communicate with each other. Typically, a single executable is run multiple times, possibly on different machines, and the MPI tools are used to inform each instance of the executable about its sibling processes, and which instance it is. MPI also provides tools to allow communication between instances to coordinate work, exchange information about elements of the task, or to transfer data. An MPI instance typically has its own copy of all the local variables.  While MPI-aware executables can generally be run as stand-alone programs, in order for them to run in parallel they must use an MPI run-time environment, which is a specific implementation of the MPI standard. To activate the MPI environment, the program should be started via a command such as mpiexec (or mpirun, or srun, etc. depending on the MPI run-time you need to use), which will ensure that the appropriate run-time support for parallelism is included.  MPI Runtime Arguments On their own, commands such as mpiexec can take many arguments specifying how many machines will participate in the execution, and you might need these if you would like to run an MPI program on your own (for example, on your laptop). In the context of a queuing system, however, it is frequently the case that MPI run-time will obtain the necessary parameters from the queuing system, by examining the environment variables set when the job is launched.  Let’s modify the job script to request more cores and use the MPI run-time.  [NetID@log-1 ~]$ cp serial-job.sh parallel-job.sh [NetID@log-1 ~]$ nano parallel-job.sh [NetID@log-1 ~]$ cat parallel-job.sh #!/bin/bash #SBATCH -J parallel-pi #SBATCH --nodes=4 #SBATCH --ntasks-per-node=1 #SBATCH --mem=3G # Load the computing environment we need module load python/intel/3.8.6 module load openmpi/gcc/4.1.6 source /home/yourUsername/amdahl/test_venv/bin/activate # Execute the task srun amdahl   Then submit your job. Note that the submission command has not really changed from how we submitted the serial job: all the parallel settings are in the batch file rather than the command line.  [NetID@log-1 ~]$ sbatch parallel-job.sh   As before, use the status commands to check when your job runs.  [NetID@log-1 ~]$ ls -t slurm-347178.out parallel-job.sh slurm-347087.out serial-job.sh amdahl README.md LICENSE.txt [NetID@log-1 ~]$ cat slurm-347178.out Doing 30.000 seconds of 'work' on 4 processors, which should take 10.875 seconds with 0.850 parallel proportion of the workload. Hello, World! I am process 0 of 4 on cs. I will do all the serial 'work' for 4.500 seconds. Hello, World! I am process 2 of 4 on cs. I will do parallel 'work' for 6.375 seconds. Hello, World! I am process 1 of 4 on cs. I will do parallel 'work' for 6.375 seconds. Hello, World! I am process 3 of 4 on cs. I will do parallel 'work' for 6.375 seconds. Hello, World! I am process 0 of 4 on cs. I will do parallel 'work' for 6.375 seconds. Total execution time (according to rank 0): 10.888 seconds   Is it 4× faster? The parallel job received 4× more processors than the serial job: does that mean it finished in ¼ the time? [Click for Solution] Solution The parallel job did take less time: 11 seconds is better than 30! But it is only a 2.7× improvement, not 4×. Look at the job output: While “process 0” did serial work, processes 1 through 3 did their parallel work.While process 0 caught up on its parallel work, the rest did nothing at all. Process 0 always has to finish its serial task before it can start on the parallel work. This sets a lower limit on the amount of time this job will take, no matter how many cores you throw at it. This is the basic principle behind Amdahl’s Law, which is one way of predicting improvements in execution time for a fixed workload that can be subdivided and run in parallel to some extent.  ","version":"Next","tagName":"h2"},{"title":"How Much Does Parallel Execution Improve Performance?​","type":1,"pageTitle":"Running a parallel job","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/running_parallel_job/#how-much-does-parallel-execution-improve-performance","content":" In theory, dividing up a perfectly parallel calculation among n MPI processes should produce a decrease in total run time by a factor of n. As we have just seen, real programs need some time for the MPI processes to communicate and coordinate, and some types of calculations can’t be subdivided: they only run effectively on a single CPU.  Additionally, if the MPI processes operate on different physical CPUs in the computer, or across multiple compute nodes, even more time is required for communication than it takes when all processes operate on a single CPU.  In practice, it’s common to evaluate the parallelism of an MPI program by:  running the program across a range of CPU countsrecording the execution time on each runcomparing each execution time to the time when using a single CPU  Since “more is better” – improvement is easier to interpret from increases in some quantity than decreases – comparisons are made using the speedup factor S, which is calculated as the single-CPU execution time divided by the multi-CPU execution time. For a perfectly parallel program, a plot of the speedup S versus the number of CPUs n would give a straight line, S = n.  Let’s run one more job, so we can see how close to a straight line our amdahl code gets.  [NetID@log-1 ~]$ nano parallel-job.sh [NetID@log-1 ~]$ cat parallel-job.sh #!/bin/bash #SBATCH -J parallel-pi #SBATCH --nodes=8 #SBATCH --ntasks-per-node=1 #SBATCH --mem=3G # Load the computing environment we need module load python/intel/3.8.6 module load openmpi/gcc/4.1.6 source /home/yourUsername/amdahl/test_venv/bin/activate # Execute the task srun amdahl   Then submit your job. Note that the submission command has not really changed from how we submitted the serial job: all the parallel settings are in the batch file rather than the command line.  [NetID@log-1 ~]$ sbatch parallel-job.sh   As before, use the status commands to check when your job runs.  [NetID@log-1 ~]$ ls -t slurm-347271.out parallel-job.sh slurm-347178.out slurm-347087.out serial-job.sh amdahl README.md LICENSE.txt [NetID@log-1 ~]$ cat slurm-347178.out which should take 7.688 seconds with 0.850 parallel proportion of the workload. Hello, World! I am process 4 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 0 of 8 on cs. I will do all the serial 'work' for 4.500 seconds. Hello, World! I am process 2 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 1 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 3 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 5 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 6 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 7 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Hello, World! I am process 0 of 8 on cs. I will do parallel 'work' for 3.188 seconds. Total execution time (according to rank 0): 7.697 seconds   Non-Linear Output When we ran the job with 4 parallel workers, the serial job wrote its output first, then the parallel processes wrote their output, with process 0 coming in first and last. With 8 workers, this is not the case: since the parallel workers take less time than the serial work, it is hard to say which process will write its output first, except that it will not be process 0!  Now, let’s summarize the amount of time it took each job to run:  Number of CPUs\tRuntime (sec)1\t30.033 4\t10.888 8\t7.697  Then, use the first row to compute speedups S, using Python as a command-line calculator:  [NetID@log-1 ~]$ for n in 30.033 10.888 7.697; do python3 -c &quot;print(30.033 / $n)&quot;; done   Number of CPUs\tSpeedup\tIdeal1\t1.0\t1 4\t2.75\t4 8\t3.90\t8  The job output files have been telling us that this program is performing 85% of its work in parallel, leaving 15% to run in serial. This seems reasonably high, but our quick study of speedup shows that in order to get a 4× speedup, we have to use 8 or 9 processors in parallel. In real programs, the speedup factor is influenced by  CPU designcommunication network between compute nodesMPI library implementationsdetails of the MPI program itself  Using Amdahl’s Law, you can prove that with this program, it is impossible to reach 8× speedup, no matter how many processors you have on hand. We'll discuss estimating job resources more in the next tutorial Using Resources Effectively.  In an HPC environment, we try to reduce the execution time for all types of jobs, and MPI is an extremely common way to combine dozens, hundreds, or thousands of CPUs into solving a single problem. To learn more about parallelization, see the parallel novice lesson.  Key Points  Parallel programming allows applications to take advantage of parallel hardware.The queuing system facilitates executing parallel tasks.Performance improvements from parallel execution do not scale linearly. ","version":"Next","tagName":"h2"},{"title":"Using resources effectively","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_effectively/","content":"","keywords":"","version":"Next"},{"title":"Estimating Required Resources Using the Scheduler​","type":1,"pageTitle":"Using resources effectively","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_effectively/#estimating-required-resources-using-the-scheduler","content":" Although we covered requesting resources from the scheduler earlier with the Amdahl Program, how do we know what type of resources the software will need in the first place, and its demand for each? In general, unless the software documentation or user testimonials provide some idea, we won’t know how much memory or compute time a program will need.  Read the Documentation Most HPC facilities maintain documentation as a wiki, a website, or a document sent along when you register for an account. Take a look at these resources, and search for the software you plan to use: somebody might have written up guidance for getting the most out of it.  A convenient way of figuring out the resources required for a job to run successfully is to submit a test job, and then ask the scheduler about its impact using sacct -u $USER. You can use this knowledge to set up the next job with a closer estimate of its load on the system. A good general rule is to ask the scheduler for 20% to 30% more time and memory than you expect the job to need. This ensures that minor fluctuations in run time or memory use will not result in your job being cancelled by the scheduler. Keep in mind that if you ask for too much, your job may not run even though enough resources are available, because the scheduler will be waiting for other people’s jobs to finish and free up the resources needed to match what you asked for.  ","version":"Next","tagName":"h2"},{"title":"Stats​","type":1,"pageTitle":"Using resources effectively","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_effectively/#stats","content":" Since we already submitted amdahl to run on the cluster, we can query the scheduler to see how long our job took and what resources were used. We will use sacct -u $USER to get statistics about parallel-job.sh.  [NYUNetID@log-1 ~]$ sacct -u $USER JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 991167 Sxxxx normal nn9299k 128 COMPLETED 0:0   This shows all the jobs we ran today (note that there are multiple entries per job). To get info about a specific job (for example, 347087), we change command slightly.  [NYUNetID@log-1 ~]$ sacct -u $USER -l -j 347087   It will show a lot of info; in fact, every single piece of info collected on your job by the scheduler will show up here. It may be useful to redirect this information to less to make it easier to view (use the left and right arrow keys to scroll through fields).  [NYUNetID@log-1 ~]$ sacct -u $USER -l -j 347087 | less -S   Discussion This view can help compare the amount of time requested and actually used, duration of residence in the queue before launching, and memory footprint on the compute node(s). How accurate were our estimates?  ","version":"Next","tagName":"h2"},{"title":"Improving Resource Requests​","type":1,"pageTitle":"Using resources effectively","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_effectively/#improving-resource-requests","content":" From the job history, we see that amdahl jobs finished executing in at most a few minutes, once dispatched. The time estimate we provided in the job script was far too long! This makes it harder for the queuing system to accurately estimate when resources will become free for other jobs. Practically, this means that the queuing system waits to dispatch our amdahl job until the full requested time slot opens, instead of “sneaking it in” a much shorter window where the job could actually finish. Specifying the expected runtime in the submission script more accurately will help alleviate cluster congestion and may get your job dispatched earlier.  Narrow the Time Estimate Edit parallel_job.sh to set a better time estimate. How close can you get? Hint: use -t. [Click for Solution] Solution The following line tells Slurm that our job should finish within 2 minutes: #SBATCH -t 00:02:00   Key Points Accurate job scripts help the queuing system efficiently allocate shared resources. ","version":"Next","tagName":"h2"},{"title":"Using shared resources responsibly","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_responsibly/","content":"","keywords":"","version":"Next"},{"title":"Be Kind to the Login Nodes​","type":1,"pageTitle":"Using shared resources responsibly","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_responsibly/#be-kind-to-the-login-nodes","content":" The login node is often busy managing all of the logged in users, creating and editing files and compiling software. If the machine runs out of memory or processing capacity, it will become very slow and unusable for everyone. While the machine is meant to be used, be sure to do so responsibly – in ways that will not adversely impact other users’ experience.  Login nodes are always the right place to launch jobs. Cluster policies vary, but they may also be used for proving out workflows, and in some cases, may host advanced cluster-specific debugging or development tools. The cluster may have modules that need to be loaded, possibly in a certain order, and paths or library versions that differ from your laptop, and doing an interactive test run on the head node is a quick and reliable way to discover and fix these issues.  Login Nodes Are a Shared Resource Remember, the login node is shared with all other users and your actions could cause issues for other people. Think carefully about the potential implications of issuing commands that may use large amounts of resource. Unsure? Ask your friendly systems administrator (“sysadmin”) if the thing you’re contemplating is suitable for the login node, or if there’s another mechanism to get it done safely. Please email hpc@nyu.edu with questions.  You can always use the commands top and ps ux to list the processes that are running on the login node along with the amount of CPU and memory they are using. If this check reveals that the login node is somewhat idle, you can safely use it for your non-routine processing task. If something goes wrong – the process takes too long, or doesn’t respond – you can use the kill command along with the PID to terminate the process.  Login Node Etiquette Which of these commands would be a routine task to run on the login node? python physics_sim.pymakecreate_directories.shmolecular_dynamics_2tar -xzf R-3.3.0.tar.gz [Click for Solution] Solution Building software, creating directories, and unpacking software are common and acceptable tasks for the login node: options #2 (make), #3 (mkdir), and #5 (tar) are probably OK. note Script names do not always reflect their contents though, so before launching #3, please less create_directories.sh and make sure it’s not a Trojan horse. Running resource-intensive applications is frowned upon. Unless you are sure it will not affect other users, do not run jobs like #1 (python) or #4 (custom MD code). If you’re unsure, ask your friendly sysadmin for advice by emailing hpc@nyu.edu.  If you experience performance issues with a login node you should report it to the system staff by sending email to hpc@nyu.edu for them to investigate.  ","version":"Next","tagName":"h2"},{"title":"Test Before Scaling​","type":1,"pageTitle":"Using shared resources responsibly","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_responsibly/#test-before-scaling","content":" Remember that you are generally charged for usage on shared systems. A simple mistake in a job script can end up costing a large amount of resource budget. Imagine a job script with a mistake that makes it sit doing nothing for 24 hours on 1000 cores or one where you have requested 2000 cores by mistake and only use 100 of them! This problem can be compounded when people write scripts that automate job submission (for example, when running the same calculation or analysis over lots of different parameters or files). When this happens it hurts both you (as you waste lots of charged resource) and other users (who are blocked from accessing the idle compute nodes). On very busy resources you may wait many days in a queue for your job to fail within 10 seconds of starting due to a trivial typo in the job script. This is extremely frustrating!  Most systems provide dedicated resources for testing that have short wait times to help you avoid this issue.  Test Job Submission Scripts That Use Large Amounts of Resources Before submitting a large run of jobs, submit one as a test first to make sure everything works as expected. Before submitting a very large or very long job submit a short truncated test to ensure that the job starts as expected.  ","version":"Next","tagName":"h2"},{"title":"Have a Backup Plan​","type":1,"pageTitle":"Using shared resources responsibly","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_responsibly/#have-a-backup-plan","content":" Although many HPC systems keep backups, it does not always cover all the file systems available and may only be for disaster recovery purposes (i.e. for restoring the whole file system if lost rather than an individual file or directory you have deleted by mistake). Please see the available storage options on Greene for specific information about Greene, but remember that rotecting critical data from corruption or deletion is primarily your responsibility: keep your own backup copies.  Version control systems (such as Git) often have free, cloud-based offerings (e.g., GitHub and GitLab) that are generally used for storing source code. Even if you are not writing your own programs, these can be very useful for storing job scripts, analysis scripts and small input files.  If you are building software, you may have a large amount of source code that you compile to build your executable. Since this data can generally be recovered by re-downloading the code, or re-running the checkout operation from the source code repository, this data is also less critical to protect.  For larger amounts of data, especially important results from your runs, which may be irreplaceable, you should make sure you have a robust system in place for taking copies of data off the HPC system wherever possible to backed-up storage. Tools such as rsync can be very useful for this.  Your access to the shared HPC system will generally be time-limited so you should ensure you have a plan for transferring your data off the system before your access finishes. The time required to transfer large amounts of data should not be underestimated and you should ensure you have planned for this early enough (ideally, before you even start using the system for your research).  In all these cases, please contact hpc@nyu.edu if you have questions about data transfer and storage for the volumes of data you will be using.  Your Data Is Your Responsibility Make sure you understand what the backup policy is on the file systems on the system you are using and what implications this has for your work if you lose your data on the system. Plan your backups of critical data and how you will transfer data off the system throughout the project.  ","version":"Next","tagName":"h2"},{"title":"Transferring Data​","type":1,"pageTitle":"Using shared resources responsibly","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/using_resources_responsibly/#transferring-data","content":" As mentioned above, many users run into the challenge of transferring large amounts of data off HPC systems at some point (this is more often in transferring data off than onto systems but the advice below applies in either case). Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going.  The components between your data’s source and destination have varying levels of performance, and in particular, may have different capabilities with respect to bandwidth and latency.  Bandwidth is generally the raw amount of data per unit time a device is capable of transmitting or receiving. It’s a common and generally well-understood metric.  Latency is a bit more subtle. For data transfers, it may be thought of as the amount of time it takes to get data out of storage and into a transmittable form. Latency issues are the reason it’s advisable to execute data transfers by moving a small number of large files, rather than the converse.  Some of the key components and their associated issues are:  Disk speed: File systems on HPC systems are often highly parallel, consisting of a very large number of high performance disk drives. This allows them to support a very high data bandwidth. Unless the remote system has a similar parallel file system you may find your transfer speed limited by disk performance at that end.Meta-data performance: Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system can also interact strongly with those you perform so reducing the number of such operations you use (by combining multiple files into a single file) may reduce variability in your transfer rates and increase transfer speeds.Network speed: Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. If you are transferring to your laptop/workstation, this is likely to be its connection (either via LAN or WiFi).Firewall speed: Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve.  As mentioned above, if you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger archive file for long term storage and transfer. A single large file makes more efficient use of the file system and is easier to move, copy and transfer because significantly fewer metadata operations are required. Archive files can be created using tools like tar and zip. We have already met tar when we talked about data transfer earlier.  Schematic diagram of bandwidth and latency for disk and network I/O. Each of the components on the figure is connected by a blue line of width proportional to the interface bandwidth. The small mazes at the link points illustrate the latency of the link, with more tortuous mazes indicating higher latency.  Consider the Best Way to Transfer Data If you are transferring large amounts of data you will need to think about what may affect your transfer performance. It is always useful to run some tests that you can use to extrapolate how long it will take to transfer your data. Say you have a “data” folder containing 10,000 or so files, a healthy mix of small and large ASCII and binary data. Which of the following would be the best way to transfer them to Greene? [user@laptop ~]$ scp -r data NYUNetID@greene.hpc.nyu.edu:~/ [user@laptop ~]$ rsync -ra data NYUNetID@greene.hpc.nyu.edu:~/ [user@laptop ~]$ rsync -raz data NYUNetID@greene.hpc.nyu.edu:~/ [user@laptop ~]$ tar -cvf data.tar data [user@laptop ~]$ rsync -raz data.tar NYUNetID@greene.hpc.nyu.edu:~/ [user@laptop ~]$ tar -cvzf data.tar.gz data [user@laptop ~]$ rsync -ra data.tar.gz NYUNetID@greene.hpc.nyu.edu:~/ [Click for Solution] Solution scp will recursively copy the directory. This works, but without compression.rsync -ra works like scp -r, but preserves file information like creation times. This is marginally better.rsync -raz adds compression, which will save some bandwidth. If you have a strong CPU at both ends of the line, and you’re on a slow network, this is a good choice.This command first uses tar to merge everything into a single file, then rsync -z to transfer it with compression. If you have a large number of files, metadata overhead can hamper your transfer, so this is a good idea.This command uses tar -z to compress the archive, then rsync to transfer it. This may perform similarly to the command directly above, but in most cases (for large datasets), it’s the best combination of high throughput and low latency (making the most of your time and network connection).  Key Points Be careful how you use the login node.Your data on the system is your responsibility.Plan and test large data transfers.It is often best to convert many files to a single archive file before transferring. ","version":"Next","tagName":"h2"},{"title":"Why Use a Cluster?","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/why_use_cluster/","content":"","keywords":"","version":"Next"},{"title":"Why Use These Computers?​","type":1,"pageTitle":"Why Use a Cluster?","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/why_use_cluster/#why-use-these-computers","content":" What do you need? How does computing help you do your research? How could more computing help you do more or better research?  Frequently, research problems that use computing can outgrow the desktop or laptop computer where they started:  A statistics student wants to do cross-validate their model. This involves running the model 1000 times — but each run takes an hour. Running on their laptop will take over a month!A genomics researcher has been using small datasets of sequence data, but soon will be receiving a new type of sequencing data that is 10 times as large. It’s already challenging to open the datasets on their computer — analyzing these larger datasets will probably crash it.An engineer is using a fluid dynamics package that has an option to run in parallel. So far, they haven’t used this option on their desktop, but in going from 2D to 3D simulations, simulation time has more than tripled and it might be useful to take advantage of that feature.  In all these cases, what is needed is access to more computers than can be used at the same time. Luckily, large scale computing systems — shared computing resources with lots of computers — are available at many universities, labs, or through national networks. These resources usually have more central processing units(CPUs), CPUs that operate at higher speeds, more memory, more storage, and faster connections with other computer systems. They are frequently called “clusters”, “supercomputers” or resources for “high performance computing” or HPC. In this lesson, we will usually use the terminology of HPC and HPC cluster.  Using a cluster often has the following advantages for researchers:  Speed: With many more CPU cores, often with higher performance specs, than a typical laptop or desktop, HPC systems can offer significant speed up.Volume: Many HPC systems have both the processing memory (RAM) and disk storage to handle very large amounts of data. Terabytes of RAM and petabytes of storage are available for research projects.Efficiency: Many HPC systems operate a pool of resources that are drawn on by many users. In most cases when the pool is large and diverse enough the resources on the system are used almost constantly.Cost: Bulk purchasing and government funding mean that the cost to the research community for using these systems in significantly less that it would be otherwise.Convenience: Maybe your calculations just take a long time to run or are otherwise inconvenient to run on your personal computer. There’s no need to tie up your own computer for hours when you can use someone else’s instead.  This is how a large-scale compute system like a cluster can help solve problems like those listed at the start of the lesson.  Thinking ahead How do you think using a large-scale computing system will be different from using your laptop? Please think about some differences you may already know about, and some differences/difficulties you imagine you may run into.  ","version":"Next","tagName":"h2"},{"title":"On Command Line​","type":1,"pageTitle":"Why Use a Cluster?","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/why_use_cluster/#on-command-line","content":" Using HPC systems often involves the use of a shell through a command line interface (CLI) and either specialized software or programming techniques. The shell is a program with the special role of having the job of running other programs rather than doing calculations or similar tasks itself. What the user types goes into the shell, which then figures out what commands to run and orders the computer to execute them. (Note that the shell is called “the shell” because it encloses the operating system in order to hide some of its complexity and make it simpler to interact with.) The most popular Unix shell is Bash, the Bourne Again SHell (so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows.  Interacting with the shell is done via a command line interface (CLI) on most HPC systems. In the earliest days of computers, the only way to interact with early computers was to rewire them. From the 1950s to the 1980s most people used line printers. These devices only allowed input and output of the letters, numbers, and punctuation found on a standard keyboard, so programming languages and software interfaces had to be designed around that constraint and text-based interfaces were the way to do this. A typing-based interface is often called a command-line interface, or CLI, to distinguish it from a graphical user interface, or GUI, which most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the Enter (or Return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off.  Learning to use Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. However, using a command line interface can be extremely powerful, and learning how to use one will allow you to reap the benefits described above.  ","version":"Next","tagName":"h2"},{"title":"The rest of this lesson​","type":1,"pageTitle":"Why Use a Cluster?","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_hpc/why_use_cluster/#the-rest-of-this-lesson","content":" The only way to use these types of resources is by learning to use the command line. This introduction to HPC systems has two parts:  We will learn to use the UNIX command line (also known as Bash).We will use our new Bash skills to connect to and operate a high-performance computing supercomputer.  The skills we learn here have other uses beyond just HPC: Bash and UNIX skills are used everywhere, be it for web development, running software, or operating servers. It’s become so essential that Microsoft now ships it as part of Windows! Knowing how to use Bash and HPC systems will allow you to operate virtually any modern device. With all of this in mind, let’s connect to a cluster and get started!  Key Points High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world.These HPC systems can be used to do work that would either be impossible or much slower or smaller systems.The standard method of interacting with such systems is via a command line interface such as Bash. ","version":"Next","tagName":"h2"},{"title":"Introduction to Using the Shell on Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/intro/","content":"","keywords":"","version":"Next"},{"title":"Why Use a Cluster​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/intro/#why-use-a-cluster","content":" Overview Questions: Why would I be interested in High Performance Computing (HPC) on Greene?What can I expect to learn from this course? Objectives: Be able to describe what an HPC system is.Identify how an HPC system could benefit you.  ","version":"Next","tagName":"h2"},{"title":"Why Use Greene?​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/intro/#why-use-greene","content":" What do you need? How does computing help you do your research? How could more computing help you do more or better research?  Frequently, research problems that use computing can outgrow the desktop or laptop computer where they started:  A statistics student wants to do cross-validate their model. This involves running the model 1000 times — but each run takes an hour. Running on their laptop will take over a month!A genomics researcher has been using small datasets of sequence data, but soon will be receiving a new type of sequencing data that is 10 times as large. It’s already challenging to open the datasets on their computer — analyzing these larger datasets will probably crash it.An engineer is using a fluid dynamics package that has an option to run in parallel. So far, they haven’t used this option on their desktop, but in going from 2D to 3D simulations, simulation time has more than tripled and it might be useful to take advantage of that feature.  In all these cases, what is needed is access to more computers than can be used at the same time. Luckily, large a scale computing system — shared computing resources with lots of computers — is available at NYU and it is named Greene. Greene has more central processing units (CPUs), CPUs that operate at higher speeds, more memory, more storage, and faster connections with other computer systems. This kind of system is frequently referred to as a “cluster”, “supercomputer” or an “high performance computing” (HPC) system. In this lesson, we will usually use the terminology HPC and HPC cluster.  Using a cluster often has the following advantages for researchers:  Speed: With many more CPU cores, often with higher performance specs than a typical laptop or desktop, HPC systems can offer significant speed up.Volume: Many HPC systems have both the processing memory (RAM) and disk storage to handle very large amounts of data. Terabytes of RAM and petabytes of storage are available for research projects.Efficiency: Many HPC systems operate a pool of resources that are drawn on by many users. In most cases when the pool is large and diverse enough the resources on the system are used almost constantly.Cost: Bulk purchasing and government funding mean that the cost to the research community for using these systems in significantly less that it would be otherwise.Convenience: Maybe your calculations just take a long time to run or are otherwise inconvenient to run on your personal computer. There’s no need to tie up your own computer for hours when you can use someone else’s instead.  This is how a large-scale compute system like a cluster can help solve problems like those listed at the start of the lesson.  Thinking ahead How do you think using a large-scale computing system will be different from using your laptop? Please think about some differences/difficulties you may run into.  ","version":"Next","tagName":"h3"},{"title":"On Command Line​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/intro/#on-command-line","content":" Using HPC systems often involves the use of a shell through a command line interface (CLI) and either specialized software or programming techniques. The shell is a program with the special role of having the job of running other programs rather than doing calculations or similar tasks itself. What the user types goes into the shell, which then figures out what commands to run and orders the computer to execute them. (Note that the shell is called “the shell” because it encloses the operating system in order to hide some of its complexity and make it simpler to interact with.) The most popular Unix shell is Bash, the Bourne Again SHell (so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows.  Interacting with the shell is done via a command line interface (CLI) on most HPC systems. In the earliest days of computers, the only way to interact with early computers was to rewire them. From the 1950s to the 1980s most people used line printers. These devices only allowed input and output of the letters, numbers, and punctuation found on a standard keyboard, so programming languages and software interfaces had to be designed around that constraint and text-based interfaces were the way to do this. A typing-based interface is often called a command-line interface, or CLI, to distinguish it from a graphical user interface, or GUI, which most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the Enter (or Return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off.  Learning to use Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. However, using a command line interface can be extremely powerful, and learning how to use one will allow you to reap the benefits described above.  ","version":"Next","tagName":"h3"},{"title":"The rest of this lesson​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/intro/#the-rest-of-this-lesson","content":" The only way to use these types of resources is by learning to use the command line. This introduction to HPC systems has two parts:  We will learn to use the UNIX command line (also known as Bash).We will use our new Bash skills to connect to and operate a high-performance computing supercomputer.  The skills we learn here have other uses beyond just HPC: Bash and UNIX skills are used everywhere, be it for web development, running software, or operating servers. It’s become so essential that Microsoft now ships it as part of Windows! Knowing how to use Bash and HPC systems will allow you to operate virtually any modern device. With all of this in mind, let’s connect to a cluster and get started!  Key Points High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world.These HPC systems can be used to do work that would either be impossible or much slower or smaller systems.The standard method of interacting with such systems is via a command line interface such as Bash. ","version":"Next","tagName":"h3"},{"title":"Connecting to the remote HPC system Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/","content":"","keywords":"","version":"Next"},{"title":"Secure Connections​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#secure-connections","content":" The first step in using a cluster is to establish a connection from your computer to the cluster. When we are sitting at a computer (or standing, or holding it in our hands or on our wrists), we have come to expect a visual display with icons, widgets, and perhaps some windows or applications: a graphical user interface, or GUI. Since computer clusters are remote resources that we connect to over slow or intermittent interfaces (WiFi and VPNs especially), it is more practical to use a command-line interface, or CLI, to send commands as plain-text. If a command returns output, it is printed as plain text as well. The commands we run today will not open a window to show graphical results.  If you have ever opened the Windows Command Prompt or macOS Terminal, you have seen a CLI. This is the CLI on your local machine. The only leap to be made here is to open a CLI on a remote machine, while taking some precautions so that other folks on the network can’t see (or change) the commands you’re running or the results the remote machine sends back. We will use the Secure SHell protocol (or SSH) to open an encrypted network connection between two machines, allowing you to send &amp; receive text and data without having to worry about prying eyes.    SSH clients are usually command-line tools, where you provide the remote machine address as the only required argument. If your username on the remote system differs from what you use locally, you must provide that as well. If your SSH client has a graphical front-end, such as PuTTY or MobaXterm, you will set these arguments before clicking “connect.” From the terminal, you’ll write something like ssh userName@hostname, where the argument is just like an email address: the “@” symbol is used to separate the personal ID from the address of the remote machine.  When logging in to a laptop, tablet, or other personal device, a username, password, or pattern are normally required to prevent unauthorized access. In these situations, the likelihood of somebody else intercepting your password is low, since logging your keystrokes requires a malicious exploit or physical access. For systems like log-1 running an SSH server, anybody on the network can log in, or try to. Since usernames are often public or easy to guess, your password is often the weakest link in the security chain. Many clusters therefore forbid password-based login, requiring instead that you generate and configure a public-private key pair with a much stronger password. Even though Greene does not require the use SSH keys to login, please consider using the instructions below to use them. It will make for quicker and more secure connections with Greene.  ","version":"Next","tagName":"h2"},{"title":"Remote Connections with the NYU VPN & HPC Gateway Server​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#remote-connections-with-the-nyu-vpn--hpc-gateway-server","content":" If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options:  VPN Option: set up your computer to use the NYU VPN. Once you’ve created a VPN connection, you can proceed as if you were connected to the NYU net.Gateway Option: go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect to HPC systems without needing to first connect to the VPN.  ","version":"Next","tagName":"h2"},{"title":"Connect to the NYU VPN​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#connect-to-the-nyu-vpn","content":" To connect to VPN, please see the NYU instructions.  ","version":"Next","tagName":"h2"},{"title":"Log into the Greene Cluster​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#log-into-the-greene-cluster","content":" ","version":"Next","tagName":"h2"},{"title":"Inside the NYU network (non-Windows)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#inside-the-nyu-network-non-windows","content":" From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can log in to the HPC clusters directly. You do not need to log in to the gateway host.  To log in to the HPC cluster (Greene), simply use:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Outside the NYU network (non-Windows)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#outside-the-nyu-network-non-windows","content":" From an off-campus location (outside NYU-NET), logging in to the HPC clusters is a two-step process:  log in to the gateway host, gw.hpc.nyu.edu. From a Mac or Linux workstation, this is a simple terminal command (replace &lt;NetID&gt; with your NetID). Your password is the same password you use for NYU Home:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   log in to the cluster. For Greene, this is done with:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"From Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#from-windows","content":" Windows users will need to use PuTTY, please see the NYU instructions.  ","version":"Next","tagName":"h3"},{"title":"Opening a Terminal​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#opening-a-terminal","content":" Accessing the Greene HPC cluster is primarily done through the Command Line Interface (CLI). A CLI provides a text-based environment that allows users to manage files, run programs, and navigate directories via command input. On macOS, the built-in CLI tool is Terminal, while Windows 10 users can leverage the Windows Subsystem for Linux (WSL) for similar functionality. Additionally, a popular tool for connecting to Linux servers from Windows is PuTTY, a free SSH client.  Connecting to an HPC system is most often done through a tool known as “SSH” (Secure SHell) and usually SSH is run through a terminal. So, to begin using an HPC system we need to begin by opening a terminal. Different operating systems have different terminals, none of which are exactly the same in terms of their features and abilities while working on the operating system. When connected to the remote system the experience between terminals will be identical as each will faithfully present the same experience of using that system.  Here is the process for opening a terminal in each operating system.  ","version":"Next","tagName":"h2"},{"title":"Linux​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#linux","content":" There are many different versions (aka “flavours”) of Linux and how to open a terminal window can change between flavours. Fortunately most Linux users already know how to open a terminal window since it is a common part of the workflow for Linux users. If this is something that you do not know how to do then a quick search on the Internet for “how to open a terminal window in” with your particular Linux flavour appended to the end should quickly give you the directions you need.  To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Mac​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#mac","content":" Macs have had a terminal built in since the first version of OS X, since it is built on a UNIX-like operating system, leveraging many parts from BSD (Berkeley Software Distribution). The terminal can be quickly opened through the use of the Searchlight tool. Hold down the command key and press the spacebar. In the search bar that shows up type “terminal”, choose the terminal app from the list of results (it will look like a tiny, black computer screen) and you will be presented with a terminal window. Alternatively, you can find Terminal under “Utilities” in the Applications menu in the Finder.  To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to Greene:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#windows","content":" While Windows does have a command-line interface known as the “Command Prompt” that has its roots in MS-DOS (Microsoft Disk Operating System) it does not have an SSH tool built into it and so one needs to be installed. There are a variety of programs that can be used for this; a few common ones we describe here, as follows:  Git BASH​  Git BASH gives you a terminal like interface in Windows. You can use this to connect to a remote computer via SSH. It can be downloaded for free from Git for Windows.  Windows Subsystem for Linux​  The Windows Subsystem for Linux also allows you to connect to a remote computer via SSH. Please see the instructions from Microsoft.  MobaXterm​  MobaXterm is a terminal window emulator for Windows and the home edition can be downloaded for free from mobatek.net. If you follow the link you will note that there are two editions of the home version available: Portable and Installer. The portable edition puts all MobaXterm content in a folder on the desktop (or anywhere else you would like it) so that it is easy to add plug-ins or remove the software. The installer edition adds MobaXterm to your Windows installation and menu as any other program you might install. If you are not sure that you will continue to use MobaXterm in the future, the portable edition is likely the best choice for you.  If you use MobaXterm you can use MobaKeyGen to manage your ssh keys. Please see the MoabXterm documentation for details.  Download the version that you would like to use and install it as you would any other software on your Windows installation. Once the software is installed you can run it by either opening the folder installed with the portable edition and double-clicking on the executable file named MobaXterm_Personal_11.1 (your version number may vary) or, if the installer edition was used, finding the executable through either the start menu or the Windows search option.  Once the MobaXterm window is open you should see a large button in the middle of that window with the text “Start Local Terminal”. Click this button and you will have a terminal window at your disposal.  PuTTY​  It is strictly speaking not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely the oldest, most well-known, and widely used software solution to take this approach.  PuTTY is available for free download. Download the version that is correct for your operating system and install it as you would other software on your Windows system. Once installed it will be available through the start menu or similar.  You can use puttygen to create ssh keys if you are using PuTTY. Please see the puttygen page in the PuTTY documentation for details.  Running PuTTY will not initially produce a terminal but instead a window full of connection options. Putting the address of the remote system in the “Host Name (or IP Address)” box and either pressing enter or clicking the “Open” button should begin the connection process.  If this works you will see a terminal window open that prompts you for a username through the “login as:” prompt and then for a password. If both of these are passed correctly then you will be given access to the system and will see a message saying so within the terminal. If you need to escape the authentication process you can hold the Control (Ctrl) key and press the c key to exit and start again.  Note that you may want to paste in your password rather than typing it. Use Ctrl plus a right-click of the mouse to paste content from the clipboard to the PuTTY terminal.  For those logging in with PuTTY it would likely be best to cover the terminal basics already mentioned above before moving on to navigating the remote system.  ","version":"Next","tagName":"h3"},{"title":"Open OnDemand (Web-based Graphical User Interface)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#open-ondemand-web-based-graphical-user-interface","content":" Open OnDemand (OOD) is an open source project funded by the National Science Foundation (NSF). OOD is designed to create easier access for users to interface with HPC systems. Originally developed by Ohio Supercomputer Center (OSC), used by many universities around the world, and now servicing the NYU Greene HPC cluster.  OOD has a variety of convenient tools to manage files, access the command line, manage and monitor jobs, and launch interactive applications, such as Jupyter Notebooks, RStudio sessions, and even full Linux Desktops.  Features Include:  Easy file management - upload and download files, view HTML and pictures without downloadingCommand-line shell access without any SSH client locally installedJob management and monitoringFull Linux desktop experience without X11Interactive Apps such as JupyterHub and RStudio without the need for port forwarding  OOD is accessible to all users with a valid NYU HPC account while on-campus network or through a VPN.  To access OOD visit: https://ood.hpc.nyu.edu (VPN Required)  ","version":"Next","tagName":"h2"},{"title":"Access the Shell​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#access-the-shell","content":" Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required.  ","version":"Next","tagName":"h3"},{"title":"Interactive Applications​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#interactive-applications","content":" GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Connections to Open OnDemand​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#troubleshooting-connections-to-open-ondemand","content":" A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue.  In Chrome, this can be done by navigating to this page in your settings:  chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&amp;search=site+data  The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache.  Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu.  ","version":"Next","tagName":"h3"},{"title":"SSH keys (optional)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#ssh-keys-optional","content":" SSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems. In this section you will create a pair of SSH keys, a private key which you keep on your own computer and a public key which is placed on the remote HPC system that you will log into.  ","version":"Next","tagName":"h2"},{"title":"Creating SSH keys on Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#creating-ssh-keys-on-windows","content":" We mentioned methods for creating SSH keys using some of the Windows SSH options above.  ","version":"Next","tagName":"h3"},{"title":"Creating SSH keys on Linux, Mac and Windows Subsystem for Linux​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#creating-ssh-keys-on-linux-mac-and-windows-subsystem-for-linux","content":" Once you have opened a terminal check for existing SSH keys and filenames since existing SSH keys could be overwritten by the following command if the filename is the same. If you already have a key with the name given after the -f option you will need to change the filename to keep from losing your existing file.  $ ls ~/.ssh/   then generate a new public-private key pair:  $ ssh-keygen -o -a 100 -t rsa -b 4096 -f ~/.ssh/id_Greene_rsa   -o (no default): use the OpenSSH key format, rather than PEM.-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.-t (default is rsa): specify the “type” or cryptographic algorithm.-b (default is 2048): sets the number of bits in the key.-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!  When prompted, enter a strong password that you will remember. Cryptography is only as good as the weakest link, and this will be used to connect to a powerful, precious, computational resource.  Take a look in ~/.ssh (use ls ~/.ssh). You should see the two new files: your private key (~/.ssh/key_Greene_rsa) and the public key (~/.ssh/key_Greene_rsa.pub). If a key is requested by the system administrators, the public key is the one to provide.  danger Private keys are your private identity A private key that is visible to anyone but you should be considered compromised, and must be destroyed. This includes having improper permissions on the directory it (or a copy) is stored in, traversing any network in the clear, attachment on unencrypted email, and even displaying the key (which is ASCII text) in your terminal window. Protect this key as if it unlocks your front door. In many ways, it does.  Further information For more information on SSH security and some of the flags set here, an excellent resource is Secure Secure Shell.  ","version":"Next","tagName":"h3"},{"title":"SSH Agent for Easier Key Handling​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#ssh-agent-for-easier-key-handling","content":" An SSH key is only as strong as the password used to unlock it, but on the other hand, typing out a complex password every time you connect to a machine is tedious and gets old very fast. This is where the SSH Agent comes in.  Using an SSH Agent, you can type your password for the private key once, then have the Agent remember it for some number of hours or until you log off. Unless some nefarious actor has physical access to your machine, this keeps the password safe, and removes the tedium of entering the password multiple times.  Just remember your password, because once it expires in the Agent, you have to type it in again.  SSH Agents on Linux, macOS, and Windows​  Open your terminal application and check if an agent is running:  [user@laptop ~]$ ssh-add -l   If you get an error like this one,  Error Error connecting to agent: No such file or directory  … then you need to launch the agent as follows:  [user@laptop ~]$ eval $(ssh-agent)   What’s in a $(...)? The syntax of this SSH Agent command is unusual, based on what we’ve seen in the UNIX Shell tutorial. This is because the ssh-agent command creates opens a connection that only you have access to, and prints a series of shell commands that can be used to reach it – but does not execute them! [user@laptop ~]$ ssh-agent SSH_AUTH_SOCK=/tmp/ssh-Zvvga2Y8kQZN/agent.131521; export SSH_AUTH_SOCK; SSH_AGENT_PID=131522; export SSH_AGENT_PID; echo Agent pid 131522; The eval command interprets this text output as commands and allows you to access the SSH Agent connection you just created. You could run each line of the ssh-agent output yourself, and achieve the same result. Using eval just makes this easier.  Otherwise, your agent is already running: don’t mess with it.  Add your key to the agent, with session expiration after 8 hours:  [user@laptop ~]$ ssh-add -t 8h ~/.ssh/id_ed25519 Enter passphrase for .ssh/id_ed25519: Identity added: .ssh/id_ed25519 Lifetime set to 86400 seconds   For the duration (8 hours), whenever you use that key, the SSH Agent will provide the key on your behalf without you having to type a single keystroke.  SSH Agent on PuTTY​  If you are using PuTTY on Windows, download and use pageant as the SSH agent. See the PuTTY documentation.  ","version":"Next","tagName":"h3"},{"title":"Modifying your .ssh/config file​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#modifying-your-sshconfig-file","content":" Please add the following lines to your ~/.ssh/config file:  Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes UserKnownHostsFile /dev/null LogLevel ERROR Host hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User &lt;Your NetID&gt; Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User &lt;Your NetID&gt;   You'll need to replace the sections above labelled &lt;Your NetID&gt; with your NetID. You can find more details about this at the Quickstart section of Accessing HPC at NYU  ","version":"Next","tagName":"h2"},{"title":"Logging onto the system​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#logging-onto-the-system","content":" With all of this in mind, let’s connect to our remote HPC system. In this tutorial, we will connect to Greene — an HPC system located at New York University. Although it’s unlikely that every system will be exactly like Greene, it’s a very good example of what you can expect from an HPC installation. To connect to Greene we will use SSH (if you are using PuTTY see details above).  SSH allows us to connect to UNIX computers remotely, and use them as if they were our own. The general syntax of the connection command follows the format:  ssh yourUsername@some.computer.address   Let’s attempt to connect to the HPC system now:  If you'd like to connect without typing your password you'll need to copy your public key file to greene first:  scp ~/.ssh/id_Greene_rsa.pub &lt;NetID&gt;@greene.hpc.nyu.edu:/home/&lt;NetID&gt;   You'll need to log in with your password at least once even if you plan to use SSH keys in the future because we'll need to set up your keys.  If you are on NYU WiFi or VPN you can connect directly with:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   otherwise, you'll need to go through the NYU gateway first:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu ssh &lt;NetID&gt;@greene.hpc.nyu.edu   When you are logged in you will see information about your last login, the host you've connected to, and your storage quota. It should look something like this:  Last login: Fri May 9 09:45:18 2025 from 0.0.0.0 Hostname: log-1 at Mon May 12 10:48:19 EDT 2025 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 23.74GB(47.48%)/4913(16.38%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 35.91GB(0.70%)/19585(1.96%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST NO/YES 2TB/5.0M 0.0TB(0.0%)/2(0%) [NetID@log-1 ~]$   By looking at the information after Hostname: and in the prompt you'll notice that the machine you're currently logged into is not Greene. This is expected. You've just logged into a login node that is connected to Greene. It is from the login nodes that you will submit jobs to Greene.  If you logged in using PuTTY this will not apply because it does not offer a local terminal.  ","version":"Next","tagName":"h2"},{"title":"Setting up your SSH keys (optional)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#setting-up-your-ssh-keys-optional","content":" If you copied your SSH keys to your home directory in an earlier step, we'll guide you through setting them up for use now.  If you do not have a directory name .ssh in your home directory create one now with the command: mkdir ~/.ssh  Remember you can list the hidden (dot) files by running the command: ls -a  add your key to the list of authorized_keys with the command: cat ~/id_Greene_rsa.pub &gt;&gt; ~/.ssh/authorized_keys  That’s all! Disconnect, then try to log back into the remote: if your key and agent have been configured correctly, you should not be prompted for the password.  ","version":"Next","tagName":"h3"},{"title":"Telling the Difference between the Local Terminal and the Remote Terminal​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#telling-the-difference-between-the-local-terminal-and-the-remote-terminal","content":" You can see that the prompt has changed after you log into a remote system. Let's take a closer look at the prompt after login: [NetID@log-1 ~]$ (in this example) tells us that we are logged into the login node log-1 with the identity NetID.  This change presents a small complication that we will need to navigate throughout this workshop. Exactly what is reported before the $ in the terminal when it is connected to the local system and the remote system will typically be different for every user. We still need to indicate which system we are entering commands on though so we will adopt the following convention:  [local]$ when the command is to be entered on a terminal connected to your local computer[NetID@glogin-1 ~]$ when the command is to be entered on a terminal connected to the remote system$ when it really doesn’t matter which system the terminal is connected to  ","version":"Next","tagName":"h3"},{"title":"Being certain which system your terminal is connected to​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#being-certain-which-system-your-terminal-is-connected-to","content":" If you ever need to be certain which system a terminal you are using is connected to then use the following command:  $ hostname   ","version":"Next","tagName":"h3"},{"title":"Keep two terminal windows open​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#keep-two-terminal-windows-open","content":" It is strongly recommended that you have two terminals open, one connected to the local system and one connected to the remote system that you can switch back and forth between. If you only use one terminal window then you will need to reconnect to the remote system using one of the methods above when you see a change from [local]$ to [NetID@login-1 ~]$ and disconnect when you see the reverse. ","version":"Next","tagName":"h3"},{"title":"Wildcards and Pipes","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/","content":"","keywords":"","version":"Next"},{"title":"Redirecting output​","type":1,"pageTitle":"Wildcards and Pipes","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/#redirecting-output","content":" Each of the commands we’ve used so far does only a very small amount of work. However, we can chain these small UNIX commands together to perform otherwise complicated actions!  For our first foray into piping, or redirecting output, we are going to use the &gt; operator to write output to a file. When using &gt;, whatever is on the left of the &gt; is written to the filename you specify on the right of the arrow. The actual syntax looks like command &gt; filename.  Let’s try several basic usages of &gt;. echo simply prints back, or echoes, whatever you type after it.  $ echo &quot;this is a test&quot; this is a test $ echo &quot;this is a test&quot; &gt; test.txt $ ls bash-lesson.tar.gz fastq dmel-all-r6.19.gtf gene_association.fb dmel_unique_protein_isoforms_fb_2016_01.tsv test.txt $ cat test.txt this is a test   Awesome, let’s try that with a more complicated command, like wc -l.  $ wc -l * &gt; word_counts.txt wc: fastq: Is a directory $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 723505 total   Notice how we still got some output to the console even though we “piped” the output to a file? Our expected output still went to the file, but how did the error message get skipped and not go to the file?  This phenomena is an artefact of how UNIX systems are built. There are 3 input/output streams for every UNIX program you will run: stdin, stdout, and stderr.  Let’s dissect these three streams of input/output in the command we just ran: wc -l * &gt; word_counts.txt  stdin is the input to a program. In the command we just ran, stdin is represented by *, which is simply every filename in our current directory.stdout contains the actual, expected output. In this case, &gt; redirected stdout to the file word_counts.txt.stderr typically contains error messages and other information that doesn’t quite fit into the category of “output”. If we insist on redirecting both stdout and stderr to the same file we would use &amp;&gt; instead of &gt;. (We can redirect just stderr using 2&gt;.)  Knowing what we know now, let’s try re-running the command, and send all of the output (including the error message) to the same word_counts.txt files as before.  $ wc -l * &amp;&gt; word_counts.txt   Notice how there was no output to the console that time. Let’s check that the error message went to the file like we specified.  $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv wc: fastq: Is a directory 106290 gene_association.fb 1 test.txt 7 word_counts.txt 723512 total   Success! The wc: fastq: Is a directory error message was written to the file. Also, note how the file was silently overwritten by directing output to the same place as before. Sometimes this is not the behaviour we want. How do we append (add) to a file instead of overwriting it?  Appending to a file is done the same was as redirecting output. However, instead of &gt;, we will use &gt;&gt;.  $ echo &quot;We want to add this sentence to the end of our file&quot; &gt;&gt; word_counts.txt $ cat word_counts.txt 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 471308 Drosophila_melanogaster.BDGP5.77.gtf 0 fastq 1304914 fb_synonym_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 1904642 total We want to add this sentence to the end of our file   ","version":"Next","tagName":"h2"},{"title":"Chaining commands together​","type":1,"pageTitle":"Wildcards and Pipes","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/#chaining-commands-together","content":" We now know how to redirect stdout and stderr to files. We can actually take this a step further and redirect output (stdout) from one command to serve as the input (stdin) for the next. To do this, we use the | (pipe) operator.  grep is an extremely useful command. It finds things for us within files. Basic usage (there are a lot of options for more clever things, see the man page) uses the syntax grep whatToFind fileToSearch. Let’s use grep to find all of the entries pertaining to the Act5C gene in Drosophila melanogaster.  $ grep Act5C dmel-all-r6.19.gtf   The output is nearly unintelligible since there is so much of it. Let’s send the output of that grep command to head so we can just take a peek at the first line. The | operator lets us send output from one command to the next:  $ grep Act5C dmel-all-r6.19.gtf | head -n 1 X\tFlyBase\tgene\t5900861\t5905399\t.\t+\t.\tgene_id &quot;FBgn0000042&quot;; gene_symbol &quot;Act5C&quot;;   Nice work, we sent the output of grep to head. Let’s try counting the number of entries for Act5C with wc -l. We can do the same trick to send grep’s output to wc -l:  $ grep Act5C dmel-all-r6.19.gtf | wc -l 46   note This is just the same as redirecting output to a file, then reading the number of lines from that file.  Writing commands using pipes How many files are there in the “fastq” directory we made earlier? (Use the shell to do this.) [Click for Solution] Solution ls fastq/ | wc -l Output of ls is one line per item, when chaining commands together like this, so counting lines gives the number of files.  Reading from compressed files Let’s compress one of our files using gzip. $ gzip gene_association.fb zcat acts like cat, except that it can read information from .gz (compressed) files. Using zcat, can you write a command to take a look at the top few lines of the gene_association.fb.gz file (without decompressing the file itself)? [Click for Solution] Solution zcat gene_association.fb.gz | head or for Mac: zcat &lt; gene_association.fb.gz | head zcat works a little differently on Macs. You'll need to use &lt; to explicitly input the file for zcat. The head command without any options shows the first 10 lines of a file.  Key Points The * wildcard is used as a placeholder to match any text that follows a pattern.Redirect a command’s output to a file with &gt;.Commands can be chained with | ","version":"Next","tagName":"h2"},{"title":"Moving around and looking at things","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/moving_looking/","content":"","keywords":"","version":"Next"},{"title":"System Architecture​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#system-architecture","content":" File Systems: The NYU HPC clusters have multiple file systems for user’s files. Each file system is configured differently to serve a different purpose.  Space\tEnvironment Variable\tSpace Purpose\tFlushed\tAllocation (per user)/home\t$HOME\tProgram development space; storing small files you want to keep long term, e.g. source code, scripts.\tNO\t20 GB /scratch\t$SCRATCH\tComputational workspace. Best suited to large, infrequent reads and writes.\tYES. Files not accessed for 60 days are deleted.\t5 TB /archive\t$ARCHIVE\tLong-term storage\tNO\t2 TB /vast\t$VAST\tFlash memory for high I/O workflows\tYES. Files not accessed for 60 days are deleted.\t2 TB  Please see HPC Storage for more details.  Right now, all we see is something that looks like this:  [NetID@login-1 ~]$   The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, either from these lessons or from other sources, do not type the prompt, only the commands that follow it.  Type the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are:  $ whoami &lt;NetID&gt;   More specifically, when we type whoami in the shell:  finds a program called whoami,runs that program,displays that program’s output, thendisplays a new prompt to tell us that it’s ready for more commands.  Next, let’s find out where we are by running a command called pwd (which stands for “print working directory”). (“Directory” is another word for “folder”). At any moment, our current working directory (where we are) is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/&lt;NetID&gt;, which is &lt;NetID&gt;'s home directory. Note that the location of your home directory may differ from system to system.  $ pwd /home/&lt;NetID&gt;   So, we know where we are. How do we look and see what’s in our current directory?  $ ls   ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns.  Differences between remote and local system Open a second terminal window on your local computer and run the ls command without logging in remotely. What differences do you see? [click to see the solution] SolutionYou probably see something like this: Output Application Documents Library Music Public Desktop Downloads Movies Pictures In addition you should also note that the preamble before the prompt ($) is different. This is very important for making sure you know what system you are issuing commands on when in the shell.  If nothing shows up when you run ls, it means that nothing’s there. Let’s make a directory for us to play with.  mkdir &lt;new directory name&gt; makes a new directory with that name in your current location. Notice that this command required two pieces of input: the actual name of the command (mkdir) and an argument that specifies the name of the directory you wish to create.  $ mkdir documents   Let’s ls again. What do we see?  Our folder is there, awesome. What if we wanted to go inside it and do stuff there? We will use the cd (change directory) command to move around. Let’s cd into our new documents folder.  $ cd documents $ pwd ~/documents   What is the ~ character? When using the shell, ~ is a shortcut that represents /home/&lt;NetID&gt;.  Now that we know how to use cd, we can go anywhere. That’s a lot of responsibility. What happens if we get “lost” and want to get back to where we started?  To go back to your home directory, the following three commands will work:  $ cd /home/&lt;NetID&gt; $ cd ~ $ cd   A quick note on the structure of a UNIX (Linux/Mac/Android/Solaris/etc) filesystem. Directories and absolute paths (i.e. exact position in the system) are always prefixed with a /. / by itself is the “root” or base directory.  Let’s go there now, look around, and then return to our home directory.  $ cd / $ ls $ cd ~ bin dev initrd local mnt proc root scratch tmp work boot etc lib localscratch nix project run srv usr cvmfs home lib64 media opt ram sbin sys var   The “home” directory is the one where we generally want to keep all of our files. Other folders on a UNIX OS contain system files, and get modified and changed as you install new software or upgrade your OS.  There are several other useful shortcuts you should be aware of.  . represents your current directory.. represents the “parent” directory of your current locationWhile typing nearly anything, you can have bash try to autocomplete what you are typing by pressing the tab key.  Let’s try these out now:  $ cd ./documents $ pwd $ cd .. $ pwd /home/&lt;NetID&gt;/documents /home/&lt;NetID&gt;   Many commands also have multiple behaviours that you can invoke with command line ‘flags.’ What is a flag? It’s generally just your command followed by a - and the name of the flag (sometimes it’s – followed by the name of the flag). You follow the flag(s) with any additional arguments you might need.  We’re going to demonstrate a couple of these “flags” using ls.  Show hidden files with -a. Hidden files are files that begin with ., these files will not appear otherwise, but that doesn’t mean they aren’t there! “Hidden” files are not hidden for security purposes, they are usually just config files and other tempfiles that the user doesn’t necessarily need to see all the time.  $ ls -a . .. .bash_logout .bash_profile .bashrc documents .emacs .mozilla .ssh   Notice how both . and .. are visible as hidden files. Show files, their size in bytes, date last modified, permissions, and other things with -l.  $ ls -l drwxr-xr-x 2 &lt;NetID&gt; tc001 4096 Jan 14 17:31 documents   This is a lot of information to take in at once and we will explain it all later! ls -l is extremely useful, and tells you almost everything you need to know about your files without actually looking at them.  We can also use multiple flags at the same time!  $ ls -l -a [yourUsername@gra-login1 ~]$ ls -la total 36 drwx--S--- 5 &lt;NetID&gt; tc001 4096 Nov 28 09:58 . drwxr-x--- 3 root tc001 4096 Nov 28 09:40 .. -rw-r--r-- 1 &lt;NetID&gt; tc001 18 Dec 6 2016 .bash_logout -rw-r--r-- 1 &lt;NetID&gt; tc001 193 Dec 6 2016 .bash_profile -rw-r--r-- 1 &lt;NetID&gt; tc001 231 Dec 6 2016 .bashrc drwxr-sr-x 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 documents -rw-r--r-- 1 &lt;NetID&gt; tc001 334 Mar 3 2017 .emacs drwxr-xr-x 4 &lt;NetID&gt; tc001 4096 Aug 2 2016 .mozilla drwx--S--- 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 .ssh   Flags generally precede any arguments passed to a UNIX command. ls actually takes an extra argument that specifies a directory to look into. When you use flags and arguments together, the syntax (how it’s supposed to be typed) generally looks something like this:  $ command &lt;flags/options&gt; &lt;arguments&gt;   So using ls -l -a on a different directory than the one we’re in would look something like:  $ ls -l -a ~/documents drwxr-sr-x 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 . drwx--S--- 5 &lt;NetID&gt; tc001 4096 Nov 28 09:58 ..   ","version":"Next","tagName":"h2"},{"title":"Where to go for help?​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#where-to-go-for-help","content":" How did I know about the -l and -a options? Is there a manual we can look at for help when we need help? There is a very helpful manual for most UNIX commands: man (if you’ve ever heard of a “man page” for something, this is what it is).  $ man ls LS(1) User Commands LS(1) NAME ls - list directory contents SYNOPSIS ls [OPTION]... [FILE]... DESCRIPTION List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too.   To navigate through the man pages, you may use the up and down arrow keys to move line-by-line, or try the spacebar and b keys to skip up and down by full page. Quit the man pages by typing q.  Alternatively, most commands you run will have a --help option that displays addition information For instance, with ls:  $ ls --help Usage: ls [OPTION]... [FILE]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too. -a, --all do not ignore entries starting with . -A, --almost-all do not list implied . and .. --author with -l, print the author of each file -b, --escape print C-style escapes for nongraphic characters --block-size=SIZE scale sizes by SIZE before printing them; e.g., '--block-size=M' prints sizes in units of 1,048,576 bytes; see SIZE format below -B, --ignore-backups do not list implied entries ending with ~ # further output omitted for clarity   Unsupported command-line options If you try to use an option that is not supported, ls and other programs will print an error message similar to this: [remote]$ ls -j Error ls: invalid option -- 'j' Try 'ls --help' for more information.  ","version":"Next","tagName":"h2"},{"title":"File System Challenge Questions​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#file-system-challenge-questions","content":" Looking at documentation Looking at the man page for ls or using ls --help, what does the -h (--human-readable) option do? [Click for Solution] Solution When used with the -l option, use unit suffixes: Byte, Kilobyte, Megabyte, Gigabyte, Terabyte and Petabyte in order to reduce the number of digits to four or fewer using base 2 for sizes. This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).  Absolute vs Relative Paths Starting from /Users/amanda/data/, which of the following commands could Amanda use to navigate to her home directory, which is /Users/amanda? cd .cd /cd /home/amandacd ../..cd ~cd homecd ~/data/..cdcd .. [Click for Solution] Solution No: . stands for the current directory.No: / stands for the root directory.No: Amanda’s home directory is /Users/amanda.No: this goes up two levels, i.e. ends in /Users.Yes: ~ stands for the user’s home directory, in this case /Users/amanda.No: this would navigate into a directory home in the current directory if it exists.Yes: unnecessarily complicated, but correct.Yes: shortcut to go back to the user’s home directory.Yes: goes up one level.  Relative Path Resolution Using the filesystem diagram below, if pwd displays /Users/thing, what will ls -F ../backup display? ../backup: No such file or directory2012-12-01 2013-01-08 2013-01-272012-12-01/ 2013-01-08/ 2013-01-27/original/ pnas_final/ pnas_sub/ [Click for Solution] Solution No: there is a directory backup in /Users.No: this is the content of Users/thing/backup, but with .. we asked for one level further up.No: see previous explanation.Yes: ../backup/ refers to /Users/backup/.  ls Reading Comprehension Assuming a directory structure as in the above figure, if pwd displays /Users/backup, and -r tells ls to display things in reverse order, what command will display: Output pnas_sub/ pnas_final/ original/ ls pwdls -r -Fls -r -F /Users/backupEither #2 or #3 above, but not #1. [Click for Solution] Solution No: pwd is not the name of a directory.Yes: ls without directory argument lists files and directories in the current directory.Yes: uses the absolute path explicitly.Correct: see explanations above.  Exploring More ls Arguments What does the command ls do when used with the -l and -h arguments? Some of its output is about properties that we do not cover in this lesson (such as file permissions and ownership), but the rest should be useful nevertheless. [Click for Solution] Solution The -l arguments makes ls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification. The -h argument makes the file size “human readable”, i.e. display something like 5.3K instead of 5369.  Listing Recursively and by Time The command ls -R lists the contents of directories recursively, i.e., lists their sub-directories, sub-sub-directories, and so on in alphabetical order at each level. The command ls -t lists things by time of last change, with most recently changed files or directories first. In what order does ls -R -t display things? Hint: ls -l uses a long listing format to view timestamps. [Click for Solution] Solution The directories are listed alphabetical at each level, the files/directories in each directory are sorted by time of last change.  Key Points Your current directory is referred to as the working directory.To change directories, use cd.To view files, use ls.You can view help for a command with man command or command --help.Hit tab to autocomplete whatever you’re currently typing. ","version":"Next","tagName":"h2"},{"title":"Writing and Reading Files","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/","content":"","keywords":"","version":"Next"},{"title":"Creating and Editing Text Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#creating-and-editing-text-files","content":" When working on an HPC system, we will frequently need to create or edit text files. Text is one of the simplest computer file formats, defined as a simple sequence of text lines.  What if we want to make a file? There are a few ways of doing this, the easiest of which is simply using a text editor. For this lesson, we are going to use nano, since it’s more intuitive than many other terminal text editors.  To create or edit a file, type nano &lt;filename&gt;, on the terminal, where &lt;filename&gt; is the name of the file. If the file does not already exist, it will be created. Let’s make a new file now, type whatever you want in it, and save it.  $ nano draft.txt     Nano defines a number of shortcut keys (prefixed by the Control or Ctrl key) to perform actions such as saving the file or exiting the editor. Here are the shortcut keys for a few common actions:  Ctrl+O — save the file (into a current name or a new name).Ctrl+X — exit the editor. If you have not saved your file upon exiting, nano will ask you if you want to save.Ctrl+K — cut (“kill”) a text line. This command deletes a line and saves it on a clipboard. If repeated multiple times without any interruption (key typing or cursor movement), it will cut a chunk of text lines.Ctrl+U — paste the cut text line (or lines). This command can be repeated to paste the same text elsewhere.  Option\tExplanationCtrl + O\tSave the changes Ctrl + X\tExit nano Ctrl + K\tCut single line Ctrl + U\tPaste the text  Using vim as a text editor From time to time, you may encounter the vim text editor. Although vim isn’t the easiest or most user-friendly of text editors, you’ll be able to find it on any system and it has many more features than nano. vim has several modes, a “command” mode (for doing big operations, like saving and quitting) and an “insert” mode. You can switch to insert mode with the i key, and command mode with Esc. In insert mode, you can type more or less normally. In command mode there are a few commands you should be aware of: :q! — quit, without saving:wq — save and quitdd — cut/delete a liney — paste a line  Do a quick check to confirm our file was created.  $ ls draft.txt   ","version":"Next","tagName":"h2"},{"title":"Reading Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#reading-files","content":" Let’s read the file we just created now. There are a few different ways of doing this, one of which is reading the entire file with cat.  $ cat draft.txt It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;.   By default, cat prints out the content of the given file. Although cat may not seem like an intuitive command with which to read files, it stands for “concatenate”. Giving it multiple file names will print out the contents of the input files in the order specified in the cat’s invocation. For example:  $ cat draft.txt draft.txt It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;. It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;.   Reading Multiple Text Files Create two more files using nano, giving them different names such as chap1.txt and chap2.txt. Then use a single cat command to read and print the contents of draft.txt, chap1.txt, and chap2.txt.  ","version":"Next","tagName":"h2"},{"title":"Creating Directory​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#creating-directory","content":" We’ve successfully created a file. What about a directory? We’ve actually done this before, using mkdir.  $ mkdir files $ mkdir documents $ ls documents files draft.txt   ","version":"Next","tagName":"h2"},{"title":"Moving, Renaming, Copying Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#moving-renaming-copying-files","content":" Moving — We will move draft.txt to the files directory with mv (“move”) command. The same syntax works for both files and directories: mv &lt;file/directory&gt; &lt;new-location&gt;  $ mv draft.txt files $ cd files $ ls draft.txt   Command\tExplanationmv dummy_file.txt test_file.txt\tRenames dummy_file.txt as test_file.txt mv subdir new_subdir\tRenames the directory “subdir” to a new directory “new_subdir”  Renaming — draft.txt isn’t a very descriptive name. How do we go about changing it? It turns out that mv is also used to rename files and directories. Although this may not seem intuitive at first, think of it as moving a file to be stored under a different name. The syntax is quite similar to moving files: mv oldName newName  $ mv draft.txt newname.testfile $ ls newname.testfile   File extensions are arbitrary In the last example, we changed both a file’s name and extension at the same time. On UNIX systems, file extensions (like .txt) are arbitrary. A file is a .txt file only because we say it is. Changing the name or extension of the file will never change a file’s contents, so you are free to rename things as you wish. With that in mind, however, file extensions are a useful tool for keeping track of what type of data it contains. A .txt file typically contains text, for instance.  Copying — What if we want to copy a file, instead of simply renaming or moving it? Use cp command (an abbreviated name for “copy”). This command has two different uses that work in the same way as mv:  Copy to same directory (copied file is renamed): cp file newFilenameCopy to other directory (copied file retains original name): cp file directory  You can also combine these two operations in one command to copy a file to a different directory with a new name: cp file directory/newFilename  Command\tExplanationcp test_file1.txt test_file2.txt\tCopies a duplicate copy of test_file1.txt with the new name test_file2.txt cp -r subdir subdir2\tRecursively copies the directory “subdir” to a new directory “subdir2”. That is, a new directory “subdir2” is created, and each file and directory under “subdir” is replicated in “subdir2”.  Let’s try this out:  $ cp newname.testfile copy.testfile $ ls newname.testfile copy.testfile $ cp newname.testfile .. $ cd .. $ ls files documents newname.testfile   ","version":"Next","tagName":"h2"},{"title":"Removing files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#removing-files","content":" We’ve begun to clutter up our workspace with all of the directories and files we’ve been making. Let’s learn how to get rid of them. One important note before we start… when you delete a file on UNIX systems, they are gone forever. There is no “recycle bin” or “trash”. Once a file is deleted, it is gone, never to return. So be very careful when deleting files.  Files are deleted with rm file [moreFiles]. To delete the newname.testfile in our current directory:  $ ls files documents newname.testfile $ rm newname.testfile $ ls files documents   That was simple enough. Directories are deleted in a similar manner using rmdir if the directory is empty or rm -r (the -r option stands for ‘recursive’) if the directory has contents.  $ ls files documents $ rmdir documents $ rmdir files rmdir: failed to remove `files/': Directory not empty $ ls files $ rm -r files $ ls   Command\tExplanationrm dummy_file.txt\tRemove a file rm -i dummy_file.txt\tIf you use -i you will be prompted for confirmation before each file is deleted. rm -f serious_file.txt\tForcibly removes a file without asking, regardless of its permissions (provided you own the file). rmdir subdir/\tRemoves “subdir” if it is already empty. Otherwise, the command fails. rm -r subdir/\tRecursively deletes the directory “subdir” and everything in it. Use it with care!  What happened? As it turns out, rmdir is unable to remove directories that have stuff in them. To delete a directory and everything inside it, we will use a special variant of rm, rm -rf directory. This is probably the scariest command on UNIX- it will force delete a directory and all of its contents without prompting. ALWAYS double check your typing before using it… if you leave out the arguments, it will attempt to delete everything on your file system that you have permission to delete. So when deleting directories be very, very careful.  What happens when you use rm -rf accidentally Steam is a major online sales platform for PC video games with over 125 million users. Despite this, it hasn’t always had the most stable or error-free code. In January 2015, user kevyin on GitHub reported that Steam’s Linux client had deleted every file on his computer. It turned out that one of the Steam programmers had added the following line: rm -rf &quot;$STEAMROOT/&quot;*. Due to the way that Steam was set up, the variable $STEAMROOT was never initialized, meaning the statement evaluated to rm -rf /*. This coding error in the Linux client meant that Steam deleted every single file on a computer when run in certain scenarios (including connected external hard drives). Moral of the story: be very careful when using rm -rf!  ","version":"Next","tagName":"h2"},{"title":"Looking at files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#looking-at-files","content":" Sometimes it’s not practical to read an entire file with cat. The file might be way too large, take a long time to open, or maybe we want to only look at a certain part of the file. As an example, we are going to look at a large and complex file type used in bioinformatics, a .gtf file. The GTF2 format is commonly used to describe the location of genetic features in a genome.  Let’s grab and unpack a set of demo files for use later. To do this, we’ll use wget.  $ wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz   Problems with wget? wget is a stand-alone application for downloading things over HTTP/HTTPS and FTP/FTPS connections, and it does the job admirably — when it is installed. Some operating systems instead come with cURL, which is the command-line interface to libcurl, a powerful library for programming interactions with remote resources over a wide variety of network protocols. If you have curl but not wget, then try this command instead: $ curl -O https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz For very large downloads, you might consider using Aria2, which has support for downloading the same file from multiple mirrors. You have to install it separately, but if you have it, try this to get it faster than your neighbors: $ aria2c https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Install wget Linux: Debian, Ubuntu, Mint: sudo apt install wgetCentOS, Red Hat: sudo yum install wget or zypper install wgetFedora: sudo dnf install wget macOS: brew install wgetWindows: Download the Wget executable (wget.exe) from a reliable source.Place the wget.exe file in a directory that's included in your system's PATH environment variable (e.g., C:\\Windows\\System32).Open a command prompt and verify the installation by running wget --version Install cURL Linux: curl is packaged for every major distribution. You can install it through the usual means. Debian, Ubuntu, Mint: sudo apt install curlCentOS, Red Hat: sudo yum install curl or zypper install curlFedora: sudo dnf install curl macOS: curl is preinstalled on macOS. If you must have the latest version you can brew install it, but only do so if the stock version has failed you.Windows: curl comes preinstalled for the Windows 10 command line.For earlier Windows systems, you can download the executable directly; run it in place.curl comes preinstalled in Git for Windows and Windows Subsystem for Linux.On Cygwin, run the setup program again and select the curl package to install it. Install Aria2 Linux: every major distribution has an aria2 package. Install it by the usual means. Debian, Ubuntu, Mint: sudo apt install aria2CentOS, Red Hat: sudo yum install aria2 or zypper install aria2Fedora: sudo dnf install aria2 macOS: aria2c is available through homebrew: brew install aria2Windows: you have the following 2 options: download the latest release and run aria2c in place.Use the Windows Subsystem for Linux  You’ll commonly encounter .tar.gz archives while working in UNIX. To extract the files from a .tar.gz file, we run the command tar -xvf filename.tar.gz:  $ tar -xvf bash-lesson.tar.gz dmel-all-r6.19.gtf dmel_unique_protein_isoforms_fb_2016_01.tsv gene_association.fb SRR307023_1.fastq SRR307023_2.fastq SRR307024_1.fastq SRR307024_2.fastq SRR307025_1.fastq SRR307025_2.fastq SRR307026_1.fastq SRR307026_2.fastq SRR307027_1.fastq SRR307027_2.fastq SRR307028_1.fastq SRR307028_2.fastq SRR307029_1.fastq SRR307029_2.fastq SRR307030_1.fastq SRR307030_2.fastq   Unzipping files We just unzipped a .tar.gz file for this example. What if we run into other file formats that we need to unzip? Just use the handy reference below: gunzip extracts the contents of .gz filesunzip extracts the contents of .zip filestar -xvf extracts the contents of .tar.gz, .tgz and .tar.bz2 files  That is a lot of files! One of these files, dmel-all-r6.19.gtf is extremely large, and contains every annotated feature in the Drosophila melanogaster genome. It’s a huge file. What happens if we run cat on it? (Press Ctrl + C to stop it).  So, cat is a really bad option when reading big files… it scrolls through the entire file far too quickly! What are the alternatives? Try all of these out and see which ones you like best!  head file: Print the top 10 lines in a file to the console. You can control the number of lines you see with the -n numberOfLines flag.tail file: Same as head, but prints the last 10 lines in a file to the console.less file: Opens a file and display as much as possible on-screen. You can scroll with Enter or the arrow keys on your keyboard. Press q to close the viewer.  Out of cat, head, tail, and less, which method of reading files is your favourite? Why?  Key Points Use nano to create or edit text files from a terminal.Use cat file1 [file2 ...] to print the contents of one or more files to the terminal.Use mv old dir to move a file or directory old to another directory dir.Use mv old new to rename a file or directory old to a new name.Use cp old new to copy a file under a new name or location.Use cp old dir copies a file old into a directory dir.Use rm old to delete (remove) a file.File extensions are entirely arbitrary on UNIX systems. ","version":"Next","tagName":"h2"},{"title":"DLP Interpretation Guide","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/dlp/dlp/","content":"","keywords":"","version":"Next"},{"title":"Viewing results from the DLP report​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/dlp/dlp/#viewing-results-from-the-dlp-report","content":" This query, the most basic, fetches the first 100 flagged items in the report.  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` LIMIT 100   Each row of the report contains a great deal of metadata on where the potentially sensitive metadata was found, as well as metadata on the DLP scan itself, but here we select only the following four columns:  quote: the span of text that was flagged as sensitive infoinfo_type.name: the type of sensitive infoInfo_type.sensitivity_score.score: the sensitivity level (LOW, MODERATE, or HIGH)likelihood: the confidence with which the DLP tool has flagged the item (POSSIBLE, LIKELY, or VERY_LIKELY)  The results should look something like this. As you can see, the same piece of text may be flagged multiple times with different types, depending on the results of DLP’s auto-detection algorithms.  To see more results, you can adjust the value of the LIMIT clause or remove it entirely. Alternatively, use some of the sample queries below to view targeted subsets of the data.  ","version":"Next","tagName":"h2"},{"title":"Sample Queries: selecting a subset of flagged items​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/dlp/dlp/#sample-queries-selecting-a-subset-of-flagged-items","content":" Select only high-sensitivity items  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; LIMIT 100   Select only items that are high-sensitivity and have a likelihood higher than “possible”  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; AND likelihood != &quot;POSSIBLE&quot; LIMIT 100   Select all items, sorted by type  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` ORDER BY info_type.name   Select all items of type PERSON_NAME, ordered alphabetically  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.name = &quot;PERSON_NAME&quot; ORDER BY quote  ","version":"Next","tagName":"h2"},{"title":"About SRDE, projects and getting started","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/","content":"","keywords":"","version":"Next"},{"title":"What is the SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#what-is-the-srde","content":" NYU Secure Research Data Environment (SRDE) is a centralized secure computing platform designed to support research projects that require storage, sharing and analysis of high risk datasets. The team provides researchers with consultations and resources to comply with security requirements of research grants and Data Use Agreements. SRDE resources intend to meet the security controls outlined in the NIST 800-171 to safeguard Controlled Unclassified Information (CUI).  Technical description Please refer to our technical description here  ","version":"Next","tagName":"h2"},{"title":"Who can have an SRDE project?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#who-can-have-an-srde-project","content":" Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators).  IRB Waiver Requirement A project must be reviewed and approved (or receive a waiver) by the University's Institutional Review Board (IRB) in order to have an SRDE.  ","version":"Next","tagName":"h2"},{"title":"How do I sign up for an SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#how-do-i-sign-up-for-an-srde","content":" Fill out our intake form to provide more information about your project: Secure Research Data Environment intake form. Once we have received your form, the team will review the information and will contact you to schedule the consultation.  ","version":"Next","tagName":"h2"},{"title":"How much will the SRDE cost?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#how-much-will-the-srde-cost","content":" The cost is dependent on the needs of the project, such as size of the data and the type of machine needed for the analysis. Google Cloud has a calculator that will help estimate the costs: https://cloud.google.com/products/calculator  ","version":"Next","tagName":"h2"},{"title":"My data is not high risk, are there other options available?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#my-data-is-not-high-risk-are-there-other-options-available","content":" There are several options available depending on the data risk classification and the needs of the project. There are resources provided by the university such as research project space, cloud computing, etc. You can check out many of these services on the HPC Support Site. If you are unsure on how to proceed, a consultation with the SRDE team will help determine the best path forward.  ","version":"Next","tagName":"h2"},{"title":"What does the SRDE team need from me for the consultation?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/basics/#what-does-the-srde-team-need-from-me-for-the-consultation","content":" To help get things started it would be beneficial to submit an intake form with any related data governance documentation (files can be attached to the form), including, but not limited to, data use agreement, OSP/IRB documents, and project information. ","version":"Next","tagName":"h2"},{"title":"Scripts, variables, and loops","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/","content":"","keywords":"","version":"Next"},{"title":"Writing a Script​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#writing-a-script","content":" So how do we write a shell script, exactly? It turns out we can do this with a text editor. Start editing a file called demo.sh (to recap, we can do this with nano demo.sh). The .sh is the standard file extension for shell scripts that most people use (you may also see .bash used).  Our shell script will have two parts:  On the very first line, add #!/bin/bash. The #! (pronounced “hash-bang”) tells our computer what program to run our script with. In this case, we are telling it to run our script with our command-line shell (what we’ve been doing everything in so far). If we wanted our script to be run with something else, like Perl, we could use #!/usr/bin/perl instead.Now, anywhere below the first line, add echo &quot;Our script worked!&quot;. When our script runs, echo will happily print out &quot;Our script worked!&quot;.  Our file should now look like this:  #!/bin/bash echo &quot;Our script worked!&quot;   Ready to run our program? Let’s try running it:  $ demo.sh bash: demo.sh: command not found...   Strangely enough, Bash can’t find our script. As it turns out, Bash will only look in certain directories for scripts to run. To run anything else, we need to tell Bash exactly where to look. To run a script that we wrote ourselves, we need to specify the full path to the file, followed by the filename. We could do this one of two ways:  with our absolute path /home/yourNetID/demo.shwith the relative path ./demo.sh  $ ./demo.sh bash: ./demo.sh: Permission denied   There’s one last thing we need to do. Before a file can be run, it needs 'permission' to run. We'll get a better understanding of Linux file permissions in the next section that will allow us to finally run our script.  ","version":"Next","tagName":"h2"},{"title":"Permissions​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#permissions","content":" Let’s look at our file’s permissions with ls -l:  $ ls -l -rw-rw-r-- 1 yourNetID users 12534006 Jan 16 18:50 bash-lesson.tar.gz -rw-rw-r-- 1 yourNetID users 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourNetID users 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourNetID users 721242 Jan 25 2016 dmel_unique_protein_is... drwxrwxr-x 2 yourNetID users 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourNetID users 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourNetID users 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourNetID users 245 Jan 16 19:24 word_counts.txt   That’s a huge amount of output: a full listing of everything in the directory. Let’s see if we can understand what each field of a given row represents, working from the left to right.  What each column of the output above means: File/Directory PermissionsReferencesOwnerGroupSize of itemTime last modifiedFilename This column contains a block of subcolumns that define the permissions for a file or directory given in each row. The permissions are shown for three user types to perform three actions each. The user types are: user (u): This refers to your permissions for this file/directory.group (g): This refers to the permissions for people in the same group as this file/directory. You will see the group in the 4th column.other (o): This refers to the permissions for all other users. The actions are: read (r): This refers to the permission to read this file.write (w): This refers to the permission to write to this file.execute (x): This refers to the permission to execute this file. The following table show what each of the subcolumns refer to and their possible values: directory\tuser read\tuser write\tuser execute\tgroup read\tgroup write\tgroup execute\tother read\tother write\tother executed or -\tr or -\tw or -\tx or -\tr or -\tw or -\tx or -\tr or -\tw or -\tx or - If there is a - in the directory column, the row refers to a file. If it contains a d, the row refers to a directory. The following columns behave in a similar manner. If they contain a -, the associated action is not allowed for the associated user type.  ","version":"Next","tagName":"h2"},{"title":"Changing Permissions​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#changing-permissions","content":" As previously mentioned, in Unix a file has three basic permissions, each of which can be set for three types of user. Those three permission also have a numeric value:  Read permission (“r”) - numeric value 4.Write permission (“w”) - numeric value 2.Execute permission (“x”) - numeric value 1.  note When applied to a directory, execute permission refers to whether the directory can be entered with cd.  You'll need to use the chmod command to modify permissions. You grant permissions with chmod who+what file and revoke them with chmod who-what file. (Notice that the first has + and the second -). Here, “who” is some combination of “u”, “g”, and “o”, and “what” is some combination of “r”, “w”, and “x”. Leaving out the who part of the command applies it to all user types.  So, to set execute permission we use:  $ chmod +x demo.sh $ ls -l -rw-rw-r-- 1 yourNetID users 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x 1 yourNetID users 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourNetID users 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourNetID users 721242 Jan 25 2016 dmel_unique_protein_is... drwxrwxr-x 2 yourNetID users 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourNetID users 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourNetID users 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourNetID users 245 Jan 16 19:24 word_counts.txt   ","version":"Next","tagName":"h2"},{"title":"Executing Script​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#executing-script","content":" Now that we have executable permissions for that file, we can run it.  $ ./demo.sh   Our script worked! Fantastic, we’ve written our first program!  ","version":"Next","tagName":"h2"},{"title":"Comments​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#comments","content":" Before we go any further, let’s learn how to take notes inside our program using comments. A comment is indicated by the # character, followed by whatever we want. Comments do not get run. Let’s try out some comments in the console, then add one to our script!  # This won't show anything.   Now let's try adding this to our script with nano. Edit your script to look something like this:  #!/bin/bash # This is a comment... they are nice for making notes! echo &quot;Our script worked!&quot;   When we run our script, the output should be unchanged from before!  ","version":"Next","tagName":"h2"},{"title":"Shell variables​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#shell-variables","content":" One important concept that we’ll need to cover are shell variables. Variables are a great way of saving information under a name you can access later. In programming languages like Python and R, variables can store pretty much anything you can think of. In the shell, they usually just store text. The best way to understand how they work is to see them in action.  To set a variable, simply type in a name containing only letters, numbers, and underscores, followed by an = and whatever you want to put in the variable. Shell variable names are often uppercase by convention (but do not have to be).  $ VAR=&quot;This is our variable&quot;   To use a variable, prefix its name with a $ sign. Note that if we want to simply check what a variable is, we should use echo (or else the shell will try to run the contents of a variable).  $ echo $VAR This is our variable   Let’s try setting a variable in our script and then recalling its value as part of a command. We’re going to make it so our script runs wc -l on whichever file we specify with FILE.  Our script:  #!/bin/bash # set our variable to the name of our GTF file FILE=dmel-all-r6.19.gtf # call wc -l on our file wc -l $FILE   $ ./demo.sh 542048 dmel-all-r6.19.gtf   What if we wanted to do our little wc -l script on other files without having to change $FILE every time we want to use it? There is actually a special shell variable we can use in scripts that allows us to use arguments in our scripts (arguments are extra information that we can pass to our script, like the -l in wc -l).  To use the first argument to a script, use $1 (the second argument is $2, and so on). Let’s change our script to run wc -l on $1 instead of $FILE. Note that we can also pass all of the arguments using $@ (not going to use it in this lesson, but it’s something to be aware of).  Our script:  #!/bin/bash # call wc -l on our first argument wc -l $1   $ ./demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv   Nice! One thing to be aware of when using variables: they are all treated as pure text. How do we save the output of an actual command like ls -l?  First, a demonstration of what doesn’t work:  $ TEST=ls -l -bash: -l: command not found   What does work? We need to surround any command with $(command):  $ TEST=$(ls -l) $ echo $TEST total 90372 -rw-rw-r-- 1 jeff jeff 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x. 1 jeff jeff 40 Jan 1619:41 demo.sh -rw-rw-r-- 1 jeff jeff 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 jeff jeff 721242 Jan 25 2016 dmel_unique_protein_isoforms_fb_2016_01.tsv drwxrwxr-x. 2 jeff jeff 4096 Jan 16 19:16 fastq -rw-r--r-- 1 jeff jeff 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 jeff jeff 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 jeff jeff 245 Jan 16 19:24 word_counts.txt   note Everything got printed on the same line. This is a feature, not a bug, as it allows us to use $(commands) inside lines of script without triggering line breaks (which would end our line of code and execute it prematurely).  ","version":"Next","tagName":"h2"},{"title":"Loops​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-134/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#loops","content":" To end our lesson on scripts, we are going to learn how to write a for-loop to execute a lot of commands at once. This will let us do the same string of commands on every file in a directory (or other stuff of that nature).  for-loops generally have the following syntax:  #!/bin/bash for VAR in first second third do echo $VAR done   When a for-loop gets run, the loop will run once for everything following the word in. In each iteration, the variable $VAR is set to a particular value for that iteration. In this case it will be set to first during the first iteration, second on the second, and so on. During each iteration, the code between do and done is performed.  Let’s run the script we just wrote (I saved mine as loop.sh).  $ chmod +x loop.sh $ ./loop.sh first second third   What if we wanted to loop over a shell variable, such as every file in the current directory? Shell variables work perfectly in for-loops. In this example, we’ll save the result of ls and loop over each file:  #!/bin/bash FILES=$(ls) for VAR in $FILES do echo $VAR done   $ ./loop.sh bash-lesson.tar.gz demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv dmel-all-r6.19.gtf fastq gene_association.fb.gz loop.sh test.txt word_counts.txt   There’s a shortcut to run on all files of a particular type, say all .gz files:  #!/bin/bash for VAR in *.gz do echo $VAR done   bash-lesson.tar.gz gene_association.fb.gz   Writing our own scripts and loops cd to our fastq directory from earlier and write a loop to print off the name and top 4 lines of every fastq file in that directory. Is there a way to only run the loop on fastq files ending in _1.fastq? [Click for Solution] Solution Create the following script in a file called head_all.sh #!/bin/bash for FILE in *.fasatq do echo $FILE head -n 4 $FILE done The for line could be modified to be for FILE in *_1.fastq to achieve the second aim.  Concatenating variables Concatenating (i.e. mashing together) variables is quite easy to do. Add whatever you want to concatenate to the beginning or end of the shell variable after enclosing it in {} characters. $ FILE=stuff.txt $ echo ${FILE}.example stuff.txt.example Can you write a script that prints off the name of every file in a directory with .processed added to it? [Click for Solution] Solution Create the following script in a file called process.sh: #!/bin/bash for FILE in * do echo ${FILE}.processed done note This will also print directories appended with .processed. To truly only get files and not directories, we need to modify this to use the find command to give us only files in the current directory: #!/bin/bash for FILE in $(find . -maxdepth 1 -type f) do echo ${FILE}.processed done but this will have the side effect of listing hidden files too. We can fix this by making a small change to the find command: #!/bin/bash for FILE in $(find . -maxdepth 1 -type f ! -name &quot;.*&quot;) do echo ${FILE}.processed done We've added ! -name &quot;.*&quot; to the find command. It means not (!) a name that starts with .. As you can see, programming is often iterative in more ways than one.   Special permissions What if we want to give different sets of users different permissions. chmod actually accepts special numeric codes instead of stuff like chmod +x, as we mentioned above. Again, the numeric codes are as follows: read = 4, write = 2, execute = 1. For each user we will assign permissions based on the sum of these permissions (must be between 7 and 0). Let’s make an example file and give everyone permission to do everything with it. $ touch example $ ls -l example -rw-r--r-- 1 yourNetID users 0 May 30 14:50 example $ chmod 777 example $ ls -l example -rwxrwxrwx 1 yourNetID users 0 May 30 14:50 example How might we give ourselves permission to do everything with a file, but allow no one else to do anything with it. [Click for Solution] Solution $ chmod 700 example $ ls -l example -rwx------ 1 yourNetID users 0 May 30 14:50 example We want all permissions, so: 4 (read) + 2 (write) + 1 (execute) = 7 for user (first position), no permissions, i.e. 0, for group (second position) and other (third position).  Key Points A shell script is just a list of bash commands in a text file.To make a shell script file executable, run chmod +x script.sh. ","version":"Next","tagName":"h2"},{"title":"Environment and Roles","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/env_roles/","content":"","keywords":"","version":"Next"},{"title":"Who is the Data Steward?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/env_roles/#who-is-the-data-steward","content":" The data steward is an individual who is responsible for the ingress and egress of the data. The data steward is usually an IT administrator of the school the PI is associated with, they should be familiar with data sets and classification and may even cosign the Data Use Agreement (DUA) associated with the project.  Data Steward role If there is no such person, the project PI will need to assign the role to someone who will NOT be analyzing the data in the SRDE since this role does not have access to the research workspace in the SRDE (to enforce separation of duties). The SRDE team will provide role-based training for the data steward.  ","version":"Next","tagName":"h2"},{"title":"Will other project users have access to my files?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/env_roles/#will-other-project-users-have-access-to-my-files","content":" Each user will have access to two drives within the SRDE workspace: home and scratch. The home drive is private and the scratch drive is shared. Any files for collaboration with other project team members on the workspace should be placed or copied over to the scratch drive.  ","version":"Next","tagName":"h2"},{"title":"What kind of software is available on the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/env_roles/#what-kind-of-software-is-available-on-the-srde","content":" Some statistical analysis software packages are preinstalled on the SRDE, such as Stata and MATLAB, other software is added on a case-by-case basis, dependent on the security and compatibility of the software with the SRDE.  ","version":"Next","tagName":"h2"},{"title":"How long will the data stay in the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/env_roles/#how-long-will-the-data-stay-in-the-srde","content":" Project lifecycle will be determined between the PI and the SRDE Team during the intake interview. ","version":"Next","tagName":"h2"},{"title":"Using the SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/using_srde/","content":"","keywords":"","version":"Next"},{"title":"Will I receive training on how to use the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/using_srde/#will-i-receive-training-on-how-to-use-the-srde","content":" Absolutely! Once your SRDE is set up the SRDE team will schedule onboarding sessions for the research workspace users and a separate one for the data steward. In addition, all users will have access to the User Guide and SRDE support team for troubleshooting and guidance.  ","version":"Next","tagName":"h2"},{"title":"Why is the SRDE in a terminal, is there a screen layout?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/using_srde/#why-is-the-srde-in-a-terminal-is-there-a-screen-layout","content":" At this time the SRDE is command-line only. We are working on an updated version with a graphical user interface (GUI) for projects with the need for it.  ","version":"Next","tagName":"h2"},{"title":"How do I export a file?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/using_srde/#how-do-i-export-a-file","content":" All egress is done by the Data Steward. In order to have a file exported, please follow instructions in the SRDE User Guide to place the file in the export folder in the research workspace, then alert your Data Steward that there is a file ready for export.  ","version":"Next","tagName":"h2"},{"title":"Can I upload my own (non-sensitive) files to the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/faq/using_srde/#can-i-upload-my-own-non-sensitive-files-to-the-srde","content":" In order to maintain the integrity of the data management flow into and out of the SRDE all data going into the environment needs to go through the Data Steward. This ensures both that the Data Steward is aware of all external data entering or leaving the environment and can confirm compliance with the DUA, as well as providing a single path for audit logging. To have your files uploaded to the environment, you can provide them to the Data Steward who can then upload them to the ingress bucket for your retrieval. ","version":"Next","tagName":"h2"},{"title":"Secure Research Data Environment (SRDE)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/getting_started/eligibility_accounts/","content":"","keywords":"","version":"Next"},{"title":"SRDE Eligibility​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/getting_started/eligibility_accounts/#srde-eligibility","content":" A project must be reviewed and approved by the University's Institutional Review Board (IRB) (or receive a waiver) in order to have an SRDE. Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators). SRDE project owners and requesters must be in one of the eligible positions to act as a project Primary Investigator (PI), as outlined by the NYU IRB policy. This includes Tenured/Tenure Track Faculty, Continuing Contract faculty, Honorific Research Faculty, Professional Research Personnel, and Emeriti and retired faculty. The SRDE project owner should be the same as the PI of the IRB protocol or waiver.  Users on a research project must have valid and active NYU NetID credentials, including external, non-NYU collaborators, and be listed on the IRB protocol. Non-NYU Researchers/Collaborators need to obtain an affiliate status to obtain access. A full-time NYU faculty member must sponsor a non-NYU collaborator for affiliate status. Please see instructions for affiliate management (NYU NetID login is required to follow the link). A Data Steward must be a faculty member or university employee.  A Data Steward must be designated for each research project using SRDE and will be solely responsible for the transport of data for the project’s SRDE. Only users approved according to the Data Provider’s requirements are permitted to access project workspaces and related resources. These individuals are required to go through our on-boarding process, which includes signing a User Agreement and agreeing to our Terms of Use.  ","version":"Next","tagName":"h2"},{"title":"Requesting an SRDE Project​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/getting_started/eligibility_accounts/#requesting-an-srde-project","content":" The SRDE form contains details about the project, such as if the project requires IRB (Institutional Review Board) approval, technical requirements (such as data storage and software). The form will be submitted to the SRDE team for review.  Link to the Secure Research Data Environment Intake Form.  After you submit the intake form, the SRDE team will review the submitted documents and will respond to schedule the consultation. ","version":"Next","tagName":"h2"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/getting_started/intro/","content":"Start here! Welcome to the Secure Research Data Environment documentation! If you do not have an active project, please proceed to the next section that explains the eligibility criteria and how you may request one. If you are an active user, you can proceed to one of the categories on the left. tip If you are looking to use REDCap, proceed to REDCap","keywords":"","version":"Next"},{"title":"REDCap at NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/redcap/redcap/","content":"REDCap at NYU REDCap is a Research Electronic Data Capture tool, originally developed in Vanderbilt University. The REDCap Consortium, composed of many active institutional partners from overall the world, utilizes and supports the REDCap ecosystem. For more information on using REDCap at NYU, please proceed to https://sites.google.com/nyu.edu/redcap (VPN required).","keywords":"","version":"Next"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/support/support/","content":"Support Please email your questions to: srde-support@nyu.edu","keywords":"","version":"Next"},{"title":"Best Practices","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/best_practices/","content":"","keywords":"","version":"Next"},{"title":"Shared Files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/best_practices/#shared-files","content":" Each user on the research workspace has their own home directory, as well as access to the top-level /shared partition.  ","version":"Next","tagName":"h2"},{"title":"Shared data files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/best_practices/#shared-data-files","content":" It is recommended to keep datasets under the /shared partition, especially if they are large. This is more efficient than each researcher making their own copy from the ingress bucket, and ensures all experiments are consistent with each other.  ","version":"Next","tagName":"h3"},{"title":"Shared code files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/best_practices/#shared-code-files","content":" Code files should also be stored under the /shared partition whenever possible. You can use a local git repo to keep a version history of your codebase, and to avoid conflicts from multiple developers working on the same file at once. To create a repo,  cd /shared/code git init   And then, after adding or modifying files,  git add * git commit -m “log message describing your change”   The git repo, with its full version history, can be exported alongside your results for transparency and reproducibility. ","version":"Next","tagName":"h3"},{"title":"Data Access","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_access/","content":"Data Access After you are connected to the Workspace Host using SSH per the instructions in the previous section, you can access data that has been placed in the workspace ingress bucket by your Data Steward. You can use the gsutil ls and cp commands to copy the data into your home directory using the steps described below. Use the following command to see list of folders in your workspace: gsutil ls As shown above, there are several folders. Data that has been transferred into the workspace is in the Ingress folder. Use the following command to list the objects in the ingress folder, replacing the path with your project’s path: gsutil ls gs://your-workspace-ingress-path List the contents of the folder with the timestamp corresponding to the date the data was transferred into the workspace, and you will see the files that were uploaded: gsutil ls gs://your-workspace-ingress-path/data-timestamp-folder To copy the files into your home directory use gsutil cp command (use period at end to copy to your home directory): gsutil cp gs://your-workspace-ingress-path/data-timestamp-folder/filename . ","keywords":"","version":"Next"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/troubleshooting/","content":"Troubleshooting Coming soon!","keywords":"","version":"Next"},{"title":"Managing Data Transfer","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Data Ingestion process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#data-ingestion-process","content":" Ingesting data into the secure environment is a two-step process; First the Data Steward must upload the data onto the staging GCP Storage Bucket and then “push” the data into the secure Workspace environment.  ","version":"Next","tagName":"h2"},{"title":"Uploading Data to the Staging Area​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#uploading-data-to-the-staging-area","content":" Option1: Using the Web Console Interface​  Log into GCP console, set project to your staging project (i.e. srde-staging-dev), and navigate on the side panel to Cloud Storage -&gt; Buckets:  Navigate to your research workspace’s corresponding Staging Ingress bucket:  Copy data to the Staging Ingress bucket:  Option2: Using the CLI​  Follow the instructions in section 2 to install and configure gcloud on your workstation. Once this is done, run the following command to find your workspace’s bucket:  gsutil ls | fgrep [Workspace Name]   The workspace name will be given to you by the SRDE team after your workspace has been provisioned. The command above should output two buckets– one will be for data ingest (ingress) and the other will be for data egress:  nyu10003@cloudshell:~ (srde-staging-dev-cedd)$ gsutil ls | fgrep example gs://nyu-us-east4-example-staging-egress-9d94/ gs://nyu-us-east4-example-staging-ingress-4bd9/   To ingest data into the SRDE, run the following command to copy individual files into the ingress bucket:  gsutil cp [FILENAME] gs://[INGRESS BUCKET]   So for instance, the following command would copy an individual text file (1661-0.txt) into the example ingress bucket:  gsutil cp 1661-0.txt gs://nyu-us-east4-example-staging-ingress-4bd9/   To copy a folder, you need to add -r after cp:  gsutil cp -r [FOLDER] gs://[INGRESS BUCKET]   We would use the following command to copy a folder named dataset into the example ingress bucket:  gsutil cp -r dataset gs://nyu-us-east4-example-staging-ingress-4bd9/   ","version":"Next","tagName":"h3"},{"title":"Push Data to the Research Workspace Using Airflow​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#push-data-to-the-research-workspace-using-airflow","content":" Once the data is in the Staging Ingress bucket, navigate to Cloud Composer and click on Airflow:  In Airflow you will see the DAG workflows for your project. If you do not see any DAGs, contact srde-support@nyu.edu with subject line “Missing Airflow Permissions”  Once you see the workflows for your project, pick the one named [project-id]_Ingress_1_Staging_to_Workspace, which will bring you to the DAG page. On the DAG page, click on the “play” button at the top right to trigger the DAG:  The DAG may take a few minutes to run. You can see its progress in the status display on the bottom left.  The display shows a list of tasks executed by the DAG. A light green square will appear next to the task when it is running, and turn dark green when it is complete. When all tasks have finished successfully, the DAG is done.  Researchers will now be able to see the data in the ingress bucket in the research project workspace.  Access policy for Data Stewards Data stewards do not have access to the research project workspace.  Instructions for researchers who need to access the ingested data in the research workspace are found in the Data Access section of this document.  ","version":"Next","tagName":"h3"},{"title":"Data Egress Process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#data-egress-process","content":" To transport data out of the SRDE project workspace, research team members copy files to be exported to the 'export' folder in the Researcher Workspace Egress bucket, sample command below:  After the files have been copied to the export folder in the egress bucket within the workspace, researchers will notify the Data Steward that they are ready to export. The Data Steward will first move the files to the Staging Egress folder and scan them using the Data Loss Prevention API, a tool for automatically detecting sensitive data types. Next, they will check the generated report and either pass the inspection or fail it. Passing the inspection moves the data onwards to the Research Data Egress project for external sharing. Failing the inspection blocks the export.  ","version":"Next","tagName":"h2"},{"title":"Push the data from the Research Workspace to Staging​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#push-the-data-from-the-research-workspace-to-staging","content":" First, run Egress DAG #1 to move files to the Staging Egress folder. Follow the same instructions as above to navigate to the Airflow page.  Once on the Airflow page, find the DAG named [project-id]_Egress_1_Workspace_to_Staging_Inspection.  Once on the DAG page, follow the steps to trigger the DAG, as instructed above. This DAG executes several tasks:  An archive copy of the export files is created within the workspace.The export files are moved to the staging environment.A DLP inspection is run to scan the exported files for sensitive data. The DLP scan may take some time to run, so wait for all tasks to be marked as successful (dark green) before proceeding.  ","version":"Next","tagName":"h3"},{"title":"Check the DLP inspection report​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#check-the-dlp-inspection-report","content":" After Stage 1 is successfully completed, the DLP inspection findings are written to BigQuery. To examine results, navigate to BigQuery by going to Google console webpage, typing BigQuery on the search bar, and selecting it from the list.  Once in BigQuery, on the Explorer tab on the left, click on the corresponding project, then on the table that corresponds to the scan that was done. The name will contain the UTC date and time of the scan, using the format dlp_YYYY-MM-DD-HHMMSS. You can verify the report’s creation time under the “Details” tab.  Select “Query &gt; In new tab” to examine the results. The following default query will return a sample of 1000 results:  SELECT * FROM “table_id” LIMIT 1000   For more information on querying the DLP report, see the DLP Interpretation Guide (TODO Add section on DLP Interp. guide!)  Click on Run to run the query and review the results of the scan. After running the query you will see the results on the lower half of the window:  ","version":"Next","tagName":"h3"},{"title":"Pass or fail the inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#pass-or-fail-the-inspection","content":" Once the results are reviewed, the Data Steward approves or denies movement to the external egress bucket. They navigate back to the Airflow page and choose one of the following options:  If DLP scan results are NOT approved, Data Steward fails the data export by running Egress_2_Staging_Fail_inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be fully deleted from staging, and only the archived copy will remain in the workspace.If DLP scan results ARE approved, Data Steward passes the data export by running Egress_3_Staging_Pass_Inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be transferred to the project’s external egress bucket, where the researchers will be able to access and share it. After the final egress DAG completes successfully, the Data Steward should notify the researchers either a) that their data is available in the external egress bucket or b) that their data export was denied and why.  ","version":"Next","tagName":"h3"},{"title":"Moving Files to Export​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#moving-files-to-export","content":" You can use the gsutil cp command to copy data from your home directory to the Egress export folder in the workspace using the following steps. Use the gsutil ls command to see the list of folders in your workspace. Copy your file into the Egress folder, adding /export/yourfilename to the Egress folder path:  gsutil cp data_file.txt gs://egress_bucket_path/export/data_file.txt     ","version":"Next","tagName":"h2"},{"title":"Auto-Inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/data_transfers/#auto-inspection","content":" When files are added to the export folder, they are automatically scanned for sensitive data using the Data Loss Prevention API. This is the same tool that the Data Steward will use to examine your exported data and approve or deny the export, so you should review the results of auto-inspection carefully. Before notifying the Data Steward that an export is ready, make sure that the DLP inspection does not detect sensitive info, or that if it does, you are aware of the items it flags and can explain why they are false alarms.  The DLP scan is automatically triggered by any new file in the export folder. It may take several minutes to run. When it is complete, a summary file will be written back to the “dlp” folder in the egress bucket.  gsutil ls gs://egress_bucket_path/dlp   Within this folder, a folder is created for each exported file, and within that are dated summary reports for each version.  gsutil ls gs://egress_bucket_path/dlp/data_file.txt/   You should see a file of the format dlp_results_YYYY-MM-DD-HHMMSS corresponding to approximately when you added the file to the export folder. Note that the scan takes about a minute to pick up new files, and may behave oddly if you upload several versions very close together.  To see the summary file contents, use the command:  gsutil cat gs://egress_bucket_path/dlp/data_file.txt/dlp_results_YYYY-MM-DD-HHMMSS   If sensitive information is detected, you will see it listed by type and count  If no sensitive information is detected, you will see a clean scan report. Double-check that the “Processed bytes” and “Total estimated bytes” approximately line up with the size of your file–if both values are 0 it is likely that there was an error in the scan. ","version":"Next","tagName":"h2"},{"title":"Connecting to SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/","content":"","keywords":"","version":"Next"},{"title":"Connecting through Google Cloud Console​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#connecting-through-google-cloud-console","content":" Navigate to Google Cloud Console https://console.cloud.google.com/welcome and login with your NetID. Click the Select a project drop-down list at the top left corner of the page. In the Select a project window that appears, search and select the bastion project using the provided project ID (ex. test-dev1-bastion-1234).    Once selected, navigate to the VM Instances page via the Navigation menu (Menu in the top left corner of the page ) &gt; Compute Engine &gt; VM Instances. A running Bastion instance will be visible in the page as shown below:    ssh to the Bastion instance by clicking on the SSH button, a new SSH-in-browser tab will appear with a restricted CLI ( Command line interface ) connected to the instance. We are now inside the Bastion Host.    Now we can ssh to our workspace host by using the workspace internal IP address 10.0.0.2:  ssh 10.0.0.2   This will open the workspace CLI, with access to the workspace host having the computing needs to work on our data.  ","version":"Next","tagName":"h2"},{"title":"Connecting through Google Cloud Shell​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#connecting-through-google-cloud-shell","content":" Navigate to https://shell.cloud.google.com/ while logged in using your NetID.  ","version":"Next","tagName":"h2"},{"title":"Setting project and zone​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#setting-project-and-zone","content":" Note - Ask your SRDE administrator for the appropriate GCP PROJECT_ID and ZONE_NAME. Replace the values in the two commands below and run them  gcloud config set project PROJECT_ID gcloud config set compute/zone ZONE_NAME   ","version":"Next","tagName":"h3"},{"title":"Confirm settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#confirm-settings","content":" Before proceeding, confirm that the project and zone match your GCP project ID and zone:  gcloud config list [compute] region = us-east4 zone = us-east4-a [core] account = netid@nyu.edu disable_usage_reporting = False project = test-dev1-bastion-1234 Your active configuration is: [default]   ","version":"Next","tagName":"h3"},{"title":"Generate SSH keys​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#generate-ssh-keys","content":" Unused keys expire! Google Cloud Shell will delete your files, including generated SSH keys, if they are not accessed for 120 days. If this happens you will need to generate them again.  The simplest way to generate SSH keys is to delegate the key generation to gcloud. In order to trigger key creation, run the following command.  note Ignore the result of this command. It will most likely print errors to the output console.  gcloud compute ssh bastion-vm   You will be prompted to enter an SSH passphrase. This is optional, however it is recommended for additional user security.  The above command should log you into the bastion VM. You will see a prompt like:  -bash-4.4$”   Before proceeding, exit back to your local machine  exit   Then make sure the above step created two keys in your ssh home directory (~/.ssh) as shown below:  ls ~/.ssh     Start the ssh-agent on your local machine  eval `ssh-agent -s`   Add the google_compute_engine key to your ssh session  ssh-add ~/.ssh/google_compute_engine   Connect to the instance with gcloud using the –ssh-flag-”-A” flag  note This command uses the default project and zone set above.  gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap   ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#add-ssh-key-to-session","content":" Run the following command to add the google_compute_engine key to the current session:ssh  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Future logins​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#future-logins","content":" After the initial login, you will not need to regenerate the SSH keys, but you will need the rest of the command sequence from “Start the SSH agent”. On your local machine:  eval `ssh-agent -s` ssh-add ~/.ssh/google_compute_engine gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap --project=PROJECT_ID   And then on the bastion VM:  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Connecting on MacOS/Linux​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#connecting-on-macoslinux","content":" ","version":"Next","tagName":"h2"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#install-gcloud-cli","content":" Follow the official guidelines to install the latest version of gcloud CLI locally on your computer.  note After completing the gcloud installation, verify that the gcloud binary is in your $PATH environment variable.  ","version":"Next","tagName":"h3"},{"title":"Configure local gcloud settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#configure-local-gcloud-settings","content":" Run the following command. It generates a link as shown below  gcloud auth login --no-launch-browser     Copy the link and open your chrome browser in incognito mode to perform user sign in.Username is your NYU NetID email address. For e.g. netid@nyu.edu  You will be redirected to the NYU SSO page and MFA verification through Duo Push. After successfully logging in, you will be asked to allow google SDK to access your account as shown below    Pressing the “Allow” button on this page will present the authorization code. Copy the code and paste it in the terminal. If this step is successful, you should see this text printed to the console. You are now logged in as [netid@nyu.edu].  ","version":"Next","tagName":"h3"},{"title":"Connect to the workspace​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#connect-to-the-workspace","content":" Follow the same instructions for connecting with Google Cloud Shell above, starting from section on setting project and zone above.  ","version":"Next","tagName":"h3"},{"title":"Connecting on Windows 10/11​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#connecting-on-windows-1011","content":" ","version":"Next","tagName":"h2"},{"title":"Start and Configure SSH-Agent Service​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#start-and-configure-ssh-agent-service","content":" Using an elevated PowerShell window (run as admin), execute the following command to install the SSH-Agent service and configure it to start automatically when you log into your machine:  Get-Service ssh-agent | Set-Service -StartupType Automatic -PassThru | Start-Service     ","version":"Next","tagName":"h3"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#install-gcloud-cli-1","content":" Download the [Google Cloud CLI installer] (https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe) and run the installer    Alternatively, run the following command to download and install:  (New-Object Net.WebClient).DownloadFile(&quot;https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe&quot;, &quot;$env:Temp\\GoogleCloudSDKInstaller.exe&quot;) &amp; $env:Temp\\GoogleCloudSDKInstaller.exe   ","version":"Next","tagName":"h3"},{"title":"Install Git​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#install-git","content":" Download the Git Bash setup from the official website: https://git-scm.com/ and run the installer  ","version":"Next","tagName":"h3"},{"title":"Install Putty​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#install-putty","content":" Download and install Putty from this link https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html  Post installation verify that the Putty authentication agent is installed and available  For 64-bit installer, you will find this executable at C:/Program Files/PuTTY/pageant.exe  ","version":"Next","tagName":"h3"},{"title":"Install Python (>version 3.0)​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#install-python-version-30","content":" Install Python from the official website:https://www.python.org/downloads/  Remember to check “Add python to the environment path.” ***add screenshot  Make sure it's installed and available on PATH. On many systems Python comes pre-installed, you can try running the python command to start the Python interpreter to check and see if it is already installed.    On windows you can also try the py command which is a launcher which is more likely to work. If it is installed you will see a response which will include the version number, for example:    ","version":"Next","tagName":"h3"},{"title":"Logging in:​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#logging-in","content":" Authenticate gcloud by starting a new session of command line or powershell. initialize and login to gcloud with your account (you will be redirected to the browser for authentication)  gcloud auth login       Run Git Bash and start the ssh-agent on your local machine  eval `ssh-agent -s`     Add the SSH key to agent by running  pageant.exe     The app runs in the background. you can find it in the tray.  Right click the icon and select &quot;Add Key&quot;. Add the google_compute_engine key with the PPK extension (~/.ssh/google_compute_engine) to your agent:  :::Skip this step in the future Go to the Pageant shortcut icon from the Windows Start Menu or your desktop.  Right click on the icon, and click on Properties. (If Properties is not an option on the menu, click on Open file location, then right click on the Pageant icon, and click on Properties)  :::    From the Shortcut tab, edit the Target field. Leave the path to pageant.exe intact. After that path, add the path to your Google .ppk key file.  Critical The key path should be outside the quotation marks. i  Here’s an example:  &quot;C:\\Program Files\\PuTTY\\pageant.exe&quot; C:\\Users\\Sam\\.ssh\\google_compute_engine.ppk     ","version":"Next","tagName":"h3"},{"title":"SSH into the bastion VM from Git Bash​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#ssh-into-the-bastion-vm-from-git-bash","content":" tip Ask your SRDE administrator for the appropriate GCP project ID.  Replace gcp-project-id with that information in the below command:  export PROJECT_ID=gcp-project-id; gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --zone=us-east4-a --tunnel-through-iap --project=${PROJECT_ID}     When SSHing to bastion in the git bash window, a new terminal in putty appears with the bastion connection  A PuTTY security alert window may pop up to accept the host key, click on Accept  ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-134/docs/srde/user_guide/connecting/#add-ssh-key-to-session-1","content":" Run ssh-add to add the google_compute_engine key to the current session  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2    ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}